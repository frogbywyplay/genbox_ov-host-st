diff -Naur gcc-4.2.1/gcc/basic-block.h gcc-4.2.1-stm/gcc/basic-block.h
--- gcc-4.2.1/gcc/basic-block.h	2006-11-16 15:10:57.000000000 +0000
+++ gcc-4.2.1-stm/gcc/basic-block.h	2007-08-10 13:08:59.000000000 +0100
@@ -862,6 +862,9 @@
 extern struct edge_list *pre_edge_lcm (int, sbitmap *, sbitmap *,
 				       sbitmap *, sbitmap *, sbitmap **,
 				       sbitmap **);
+extern struct edge_list *pre_edge_lcm_avs (int, sbitmap *, sbitmap *,
+					   sbitmap *, sbitmap *, sbitmap *,
+					   sbitmap *, sbitmap **, sbitmap **);
 extern struct edge_list *pre_edge_rev_lcm (int, sbitmap *,
 					   sbitmap *, sbitmap *,
 					   sbitmap *, sbitmap **,
@@ -1170,7 +1173,7 @@
 
 extern bool insns_match_p (rtx, rtx, struct equiv_info *);
 extern int struct_equiv_block_eq (int, struct equiv_info *);
-extern bool struct_equiv_init (int, struct equiv_info *);
+extern bool struct_equiv_init (int, struct equiv_info *, bool);
 extern bool rtx_equiv_p (rtx *, rtx, int, struct equiv_info *);
 
 /* In cfgrtl.c */
diff -Naur gcc-4.2.1/gcc/cfgcleanup.c gcc-4.2.1-stm/gcc/cfgcleanup.c
--- gcc-4.2.1/gcc/cfgcleanup.c	2006-05-28 18:46:46.000000000 +0100
+++ gcc-4.2.1-stm/gcc/cfgcleanup.c	2007-08-10 13:08:59.000000000 +0100
@@ -1,6 +1,7 @@
 /* Control flow optimization code for GNU compiler.
    Copyright (C) 1987, 1988, 1992, 1993, 1994, 1995, 1996, 1997, 1998,
    1999, 2000, 2001, 2002, 2003, 2004, 2005 Free Software Foundation, Inc.
+   Copyright (c) 2006  STMicroelectronics.
 
 This file is part of GCC.
 
@@ -58,11 +59,6 @@
 
 /* Set to true when we are running first pass of try_optimize_cfg loop.  */
 static bool first_pass;
-static bool try_crossjump_to_edge (int, edge, edge);
-static bool try_crossjump_bb (int, basic_block);
-static bool outgoing_edges_match (int, basic_block, basic_block);
-static int flow_find_cross_jump (int, basic_block, basic_block, rtx *, rtx *);
-static bool old_insns_match_p (int, rtx, rtx);
 
 static void merge_blocks_move_predecessor_nojumps (basic_block, basic_block);
 static void merge_blocks_move_successor_nojumps (basic_block, basic_block);
@@ -74,7 +70,6 @@
 static void notice_new_block (basic_block);
 static void update_forwarder_flag (basic_block);
 static int mentions_nonequal_regs (rtx *, void *);
-static void merge_memattrs (rtx, rtx);
 
 /* Set flags for newly created block.  */
 
@@ -848,97 +843,6 @@
 }
 
 
-/* Removes the memory attributes of MEM expression
-   if they are not equal.  */
-
-void
-merge_memattrs (rtx x, rtx y)
-{
-  int i;
-  int j;
-  enum rtx_code code;
-  const char *fmt;
-
-  if (x == y)
-    return;
-  if (x == 0 || y == 0)
-    return;
-
-  code = GET_CODE (x);
-
-  if (code != GET_CODE (y))
-    return;
-
-  if (GET_MODE (x) != GET_MODE (y))
-    return;
-
-  if (code == MEM && MEM_ATTRS (x) != MEM_ATTRS (y))
-    {
-      if (! MEM_ATTRS (x))
-	MEM_ATTRS (y) = 0;
-      else if (! MEM_ATTRS (y))
-	MEM_ATTRS (x) = 0;
-      else
-	{
-	  rtx mem_size;
-
-	  if (MEM_ALIAS_SET (x) != MEM_ALIAS_SET (y))
-	    {
-	      set_mem_alias_set (x, 0);
-	      set_mem_alias_set (y, 0);
-	    }
-
-	  if (! mem_expr_equal_p (MEM_EXPR (x), MEM_EXPR (y)))
-	    {
-	      set_mem_expr (x, 0);
-	      set_mem_expr (y, 0);
-	      set_mem_offset (x, 0);
-	      set_mem_offset (y, 0);
-	    }
-	  else if (MEM_OFFSET (x) != MEM_OFFSET (y))
-	    {
-	      set_mem_offset (x, 0);
-	      set_mem_offset (y, 0);
-	    }
-
-	  if (!MEM_SIZE (x))
-	    mem_size = NULL_RTX;
-	  else if (!MEM_SIZE (y))
-	    mem_size = NULL_RTX;
-	  else
-	    mem_size = GEN_INT (MAX (INTVAL (MEM_SIZE (x)),
-				     INTVAL (MEM_SIZE (y))));
-	  set_mem_size (x, mem_size);
-	  set_mem_size (y, mem_size);
-
-	  set_mem_align (x, MIN (MEM_ALIGN (x), MEM_ALIGN (y)));
-	  set_mem_align (y, MEM_ALIGN (x));
-	}
-    }
-
-  fmt = GET_RTX_FORMAT (code);
-  for (i = GET_RTX_LENGTH (code) - 1; i >= 0; i--)
-    {
-      switch (fmt[i])
-	{
-	case 'E':
-	  /* Two vectors must have the same length.  */
-	  if (XVECLEN (x, i) != XVECLEN (y, i))
-	    return;
-
-	  for (j = 0; j < XVECLEN (x, i); j++)
-	    merge_memattrs (XVECEXP (x, i, j), XVECEXP (y, i, j));
-
-	  break;
-
-	case 'e':
-	  merge_memattrs (XEXP (x, i), XEXP (y, i));
-	}
-    }
-  return;
-}
-
-
 /* Return true if I1 and I2 are equivalent and thus can be crossjumped.  */
 
 static bool
@@ -1046,119 +950,6 @@
 
   return false;
 }
-
-/* Look through the insns at the end of BB1 and BB2 and find the longest
-   sequence that are equivalent.  Store the first insns for that sequence
-   in *F1 and *F2 and return the sequence length.
-
-   To simplify callers of this function, if the blocks match exactly,
-   store the head of the blocks in *F1 and *F2.  */
-
-static int
-flow_find_cross_jump (int mode ATTRIBUTE_UNUSED, basic_block bb1,
-		      basic_block bb2, rtx *f1, rtx *f2)
-{
-  rtx i1, i2, last1, last2, afterlast1, afterlast2;
-  int ninsns = 0;
-
-  /* Skip simple jumps at the end of the blocks.  Complex jumps still
-     need to be compared for equivalence, which we'll do below.  */
-
-  i1 = BB_END (bb1);
-  last1 = afterlast1 = last2 = afterlast2 = NULL_RTX;
-  if (onlyjump_p (i1)
-      || (returnjump_p (i1) && !side_effects_p (PATTERN (i1))))
-    {
-      last1 = i1;
-      i1 = PREV_INSN (i1);
-    }
-
-  i2 = BB_END (bb2);
-  if (onlyjump_p (i2)
-      || (returnjump_p (i2) && !side_effects_p (PATTERN (i2))))
-    {
-      last2 = i2;
-      /* Count everything except for unconditional jump as insn.  */
-      if (!simplejump_p (i2) && !returnjump_p (i2) && last1)
-	ninsns++;
-      i2 = PREV_INSN (i2);
-    }
-
-  while (true)
-    {
-      /* Ignore notes.  */
-      while (!INSN_P (i1) && i1 != BB_HEAD (bb1))
-	i1 = PREV_INSN (i1);
-
-      while (!INSN_P (i2) && i2 != BB_HEAD (bb2))
-	i2 = PREV_INSN (i2);
-
-      if (i1 == BB_HEAD (bb1) || i2 == BB_HEAD (bb2))
-	break;
-
-      if (!old_insns_match_p (mode, i1, i2))
-	break;
-
-      merge_memattrs (i1, i2);
-
-      /* Don't begin a cross-jump with a NOTE insn.  */
-      if (INSN_P (i1))
-	{
-	  /* If the merged insns have different REG_EQUAL notes, then
-	     remove them.  */
-	  rtx equiv1 = find_reg_equal_equiv_note (i1);
-	  rtx equiv2 = find_reg_equal_equiv_note (i2);
-
-	  if (equiv1 && !equiv2)
-	    remove_note (i1, equiv1);
-	  else if (!equiv1 && equiv2)
-	    remove_note (i2, equiv2);
-	  else if (equiv1 && equiv2
-		   && !rtx_equal_p (XEXP (equiv1, 0), XEXP (equiv2, 0)))
-	    {
-	      remove_note (i1, equiv1);
-	      remove_note (i2, equiv2);
-	    }
-
-	  afterlast1 = last1, afterlast2 = last2;
-	  last1 = i1, last2 = i2;
-	  ninsns++;
-	}
-
-      i1 = PREV_INSN (i1);
-      i2 = PREV_INSN (i2);
-    }
-
-#ifdef HAVE_cc0
-  /* Don't allow the insn after a compare to be shared by
-     cross-jumping unless the compare is also shared.  */
-  if (ninsns && reg_mentioned_p (cc0_rtx, last1) && ! sets_cc0_p (last1))
-    last1 = afterlast1, last2 = afterlast2, ninsns--;
-#endif
-
-  /* Include preceding notes and labels in the cross-jump.  One,
-     this may bring us to the head of the blocks as requested above.
-     Two, it keeps line number notes as matched as may be.  */
-  if (ninsns)
-    {
-      while (last1 != BB_HEAD (bb1) && !INSN_P (PREV_INSN (last1)))
-	last1 = PREV_INSN (last1);
-
-      if (last1 != BB_HEAD (bb1) && LABEL_P (PREV_INSN (last1)))
-	last1 = PREV_INSN (last1);
-
-      while (last2 != BB_HEAD (bb2) && !INSN_P (PREV_INSN (last2)))
-	last2 = PREV_INSN (last2);
-
-      if (last2 != BB_HEAD (bb2) && LABEL_P (PREV_INSN (last2)))
-	last2 = PREV_INSN (last2);
-
-      *f1 = last1;
-      *f2 = last2;
-    }
-
-  return ninsns;
-}
 
 /* Return true iff the condbranches at the end of BB1 and BB2 match.  */
 bool
@@ -1218,8 +1009,8 @@
   if (code2 == UNKNOWN)
     return false;
 
-  if (call_init && !struct_equiv_init (STRUCT_EQUIV_START | info->mode, info))
-    gcc_unreachable ();
+  if (call_init)
+    struct_equiv_init (STRUCT_EQUIV_START | info->mode, info, false);
   /* Make the sources of the pc sets unreadable so that when we call
      insns_match_p it won't process them.
      The death_notes_match_p from insns_match_p won't see the local registers
@@ -1288,15 +1079,20 @@
   return match;
 }
 
-/* Return true iff outgoing edges of BB1 and BB2 match, together with
-   the branch instruction.  This means that if we commonize the control
-   flow before end of the basic block, the semantic remains unchanged.
+/* Return true iff outgoing edges of INFO->y_block and INFO->x_block match,
+   together with the branch instruction.  This means that if we commonize the
+   control flow before end of the basic block, the semantic remains unchanged.
+   If we need to compare jumps, we set STRUCT_EQUIV_MATCH_JUMPS in *MODE,
+   and pass *MODE to struct_equiv_init or assign it to INFO->mode, as
+   appropriate.
 
    We may assume that there exists one edge with a common destination.  */
 
 static bool
-outgoing_edges_match (int mode, basic_block bb1, basic_block bb2)
+outgoing_edges_match (int *mode, struct equiv_info *info)
 {
+  basic_block bb1 = info->y_block;
+  basic_block bb2 = info->x_block;
   int nehedges1 = 0, nehedges2 = 0;
   edge fallthru1 = 0, fallthru2 = 0;
   edge e1, e2;
@@ -1312,114 +1108,19 @@
 		& (EDGE_COMPLEX | EDGE_FAKE)) == 0
 	    && (!JUMP_P (BB_END (bb2)) || simplejump_p (BB_END (bb2))));
 
+  *mode |= STRUCT_EQUIV_MATCH_JUMPS;
   /* Match conditional jumps - this may get tricky when fallthru and branch
      edges are crossed.  */
   if (EDGE_COUNT (bb1->succs) == 2
       && any_condjump_p (BB_END (bb1))
       && onlyjump_p (BB_END (bb1)))
     {
-      edge b1, f1, b2, f2;
-      bool reverse, match;
-      rtx set1, set2, cond1, cond2;
-      enum rtx_code code1, code2;
-
       if (EDGE_COUNT (bb2->succs) != 2
 	  || !any_condjump_p (BB_END (bb2))
 	  || !onlyjump_p (BB_END (bb2)))
 	return false;
-
-      b1 = BRANCH_EDGE (bb1);
-      b2 = BRANCH_EDGE (bb2);
-      f1 = FALLTHRU_EDGE (bb1);
-      f2 = FALLTHRU_EDGE (bb2);
-
-      /* Get around possible forwarders on fallthru edges.  Other cases
-	 should be optimized out already.  */
-      if (FORWARDER_BLOCK_P (f1->dest))
-	f1 = single_succ_edge (f1->dest);
-
-      if (FORWARDER_BLOCK_P (f2->dest))
-	f2 = single_succ_edge (f2->dest);
-
-      /* To simplify use of this function, return false if there are
-	 unneeded forwarder blocks.  These will get eliminated later
-	 during cleanup_cfg.  */
-      if (FORWARDER_BLOCK_P (f1->dest)
-	  || FORWARDER_BLOCK_P (f2->dest)
-	  || FORWARDER_BLOCK_P (b1->dest)
-	  || FORWARDER_BLOCK_P (b2->dest))
-	return false;
-
-      if (f1->dest == f2->dest && b1->dest == b2->dest)
-	reverse = false;
-      else if (f1->dest == b2->dest && b1->dest == f2->dest)
-	reverse = true;
-      else
-	return false;
-
-      set1 = pc_set (BB_END (bb1));
-      set2 = pc_set (BB_END (bb2));
-      if ((XEXP (SET_SRC (set1), 1) == pc_rtx)
-	  != (XEXP (SET_SRC (set2), 1) == pc_rtx))
-	reverse = !reverse;
-
-      cond1 = XEXP (SET_SRC (set1), 0);
-      cond2 = XEXP (SET_SRC (set2), 0);
-      code1 = GET_CODE (cond1);
-      if (reverse)
-	code2 = reversed_comparison_code (cond2, BB_END (bb2));
-      else
-	code2 = GET_CODE (cond2);
-
-      if (code2 == UNKNOWN)
-	return false;
-
-      /* Verify codes and operands match.  */
-      match = ((code1 == code2
-		&& rtx_renumbered_equal_p (XEXP (cond1, 0), XEXP (cond2, 0))
-		&& rtx_renumbered_equal_p (XEXP (cond1, 1), XEXP (cond2, 1)))
-	       || (code1 == swap_condition (code2)
-		   && rtx_renumbered_equal_p (XEXP (cond1, 1),
-					      XEXP (cond2, 0))
-		   && rtx_renumbered_equal_p (XEXP (cond1, 0),
-					      XEXP (cond2, 1))));
-
-      /* If we return true, we will join the blocks.  Which means that
-	 we will only have one branch prediction bit to work with.  Thus
-	 we require the existing branches to have probabilities that are
-	 roughly similar.  */
-      if (match
-	  && !optimize_size
-	  && maybe_hot_bb_p (bb1)
-	  && maybe_hot_bb_p (bb2))
-	{
-	  int prob2;
-
-	  if (b1->dest == b2->dest)
-	    prob2 = b2->probability;
-	  else
-	    /* Do not use f2 probability as f2 may be forwarded.  */
-	    prob2 = REG_BR_PROB_BASE - b2->probability;
-
-	  /* Fail if the difference in probabilities is greater than 50%.
-	     This rules out two well-predicted branches with opposite
-	     outcomes.  */
-	  if (abs (b1->probability - prob2) > REG_BR_PROB_BASE / 2)
-	    {
-	      if (dump_file)
-		fprintf (dump_file,
-			 "Outcomes of branch in bb %i and %i differ too much (%i %i)\n",
-			 bb1->index, bb2->index, b1->probability, prob2);
-
-	      return false;
-	    }
-	}
-
-      if (dump_file && match)
-	fprintf (dump_file, "Conditionals in bb %i and %i match.\n",
-		 bb1->index, bb2->index);
-
-      return match;
+      info->mode = *mode;
+      return condjump_equiv_p (info, true);
     }
 
   /* Generic case - we are seeing a computed jump, table jump or trapping
@@ -1467,31 +1168,22 @@
 		      identical = false;
 		}
 
-	      if (identical)
+	      if (identical
+		  && struct_equiv_init (STRUCT_EQUIV_START | *mode, info, true))
 		{
-		  replace_label_data rr;
 		  bool match;
 
-		  /* Temporarily replace references to LABEL1 with LABEL2
+		  /* Indicate that LABEL1 is to be replaced with LABEL2
 		     in BB1->END so that we could compare the instructions.  */
-		  rr.r1 = label1;
-		  rr.r2 = label2;
-		  rr.update_label_nuses = false;
-		  for_each_rtx (&BB_END (bb1), replace_label, &rr);
+		  info->y_label = label1;
+		  info->x_label = label2;
 
-		  match = old_insns_match_p (mode, BB_END (bb1), BB_END (bb2));
+		  match = insns_match_p (BB_END (bb1), BB_END (bb2), info);
 		  if (dump_file && match)
 		    fprintf (dump_file,
 			     "Tablejumps in bb %i and %i match.\n",
 			     bb1->index, bb2->index);
 
-		  /* Set the original label in BB1->END because when deleting
-		     a block whose end is a tablejump, the tablejump referenced
-		     from the instruction is deleted too.  */
-		  rr.r1 = label2;
-		  rr.r2 = label1;
-		  for_each_rtx (&BB_END (bb1), replace_label, &rr);
-
 		  return match;
 		}
 	    }
@@ -1499,16 +1191,20 @@
 	}
     }
 
+  /* Ensure that the edge counts do match.  */
+  if (EDGE_COUNT (bb1->succs) != EDGE_COUNT (bb2->succs))
+    return false;
+
   /* First ensure that the instructions match.  There may be many outgoing
      edges so this test is generally cheaper.  */
-  if (!old_insns_match_p (mode, BB_END (bb1), BB_END (bb2)))
+  /* FIXME: the regset compare might be costly.  We should try to get a cheap
+     and reasonably effective test first.  */
+  if (!struct_equiv_init (STRUCT_EQUIV_START | *mode, info, true)
+      || !insns_match_p (BB_END (bb1), BB_END (bb2), info))
     return false;
 
-  /* Search the outgoing edges, ensure that the counts do match, find possible
-     fallthru and exception handling edges since these needs more
-     validation.  */
-  if (EDGE_COUNT (bb1->succs) != EDGE_COUNT (bb2->succs))
-    return false;
+  /* Search the outgoing edges, find possible fallthru and exception
+     handling edges since these needs more validation.  */
 
   FOR_EACH_EDGE (e1, ei, bb1->succs)
     {
@@ -1599,15 +1295,14 @@
 static bool
 try_crossjump_to_edge (int mode, edge e1, edge e2)
 {
-  int nmatch;
+  int nmatch, i;
   basic_block src1 = e1->src, src2 = e2->src;
   basic_block redirect_to, redirect_from, to_remove;
-  rtx newpos1, newpos2;
   edge s;
   edge_iterator ei;
-
-  newpos1 = newpos2 = NULL_RTX;
-
+  struct equiv_info info;
+  rtx x_active, y_active;
+  
   /* If we have partitioned hot/cold basic blocks, it is a bad idea
      to try this optimization.
 
@@ -1653,18 +1348,36 @@
     return false;
 
   /* Look for the common insn sequence, part the first ...  */
-  if (!outgoing_edges_match (mode, src1, src2))
+  info.x_block = src2;
+  info.y_block = src1;
+  if (!outgoing_edges_match (&mode, &info))
     return false;
 
   /* ... and part the second.  */
-  nmatch = flow_find_cross_jump (mode, src1, src2, &newpos1, &newpos2);
+  info.input_cost = optimize_size ? COSTS_N_INSNS (1) : -1;
+  nmatch = struct_equiv_block_eq (STRUCT_EQUIV_START | mode, &info);
 
   /* Don't proceed with the crossjump unless we found a sufficient number
      of matching instructions or the 'from' block was totally matched
      (such that its predecessors will hopefully be redirected and the
      block removed).  */
-  if ((nmatch < PARAM_VALUE (PARAM_MIN_CROSSJUMP_INSNS))
-      && (newpos1 != BB_HEAD (src1)))
+  if (!nmatch)
+    return false;
+  if ((nmatch -info.cur.input_count < PARAM_VALUE (PARAM_MIN_CROSSJUMP_INSNS))
+      && (info.cur.y_start != BB_HEAD (src1)))
+    return false;
+  while (info.need_rerun)
+    {
+      nmatch = struct_equiv_block_eq (STRUCT_EQUIV_RERUN | mode, &info);
+      if (!nmatch)
+	return false;
+      if ((nmatch -info.cur.input_count < PARAM_VALUE (PARAM_MIN_CROSSJUMP_INSNS))
+	   && (info.cur.y_start != BB_HEAD (src1)))
+	return false;
+    }
+  nmatch = struct_equiv_block_eq (STRUCT_EQUIV_FINAL | mode, &info);
+  if ((nmatch -info.cur.input_count < PARAM_VALUE (PARAM_MIN_CROSSJUMP_INSNS))
+      && (info.cur.y_start != BB_HEAD (src1)))
     return false;
 
   /* Avoid deleting preserve label when redirecting ABNORMAL edges.  */
@@ -1672,6 +1385,35 @@
       && (e1->flags & EDGE_ABNORMAL))
     return false;
 
+  /* Skip possible basic block header.  */
+  x_active = info.cur.x_start;
+  if (LABEL_P (x_active))
+    x_active = NEXT_INSN (x_active);
+  if (NOTE_P (x_active))
+    x_active = NEXT_INSN (x_active);
+
+  y_active = info.cur.y_start;
+  if (LABEL_P (y_active))
+    y_active = NEXT_INSN (y_active);
+  if (NOTE_P (y_active))
+    y_active = NEXT_INSN (y_active);
+
+  /* In order for this code to become active, either we have to be called
+     before reload, or struct_equiv_block_eq needs to add register scavenging
+     code to allocate input_reg after reload.  */
+  if (info.input_reg)
+    {
+      emit_insn_before (gen_move_insn (info.input_reg, info.x_input),
+			x_active);
+      emit_insn_before (gen_move_insn (info.input_reg, info.y_input),
+			y_active);
+    }
+
+  for (i = 0; i < info.cur.local_count; i++)
+    if (info.local_rvalue[i])
+      emit_insn_before (gen_move_insn (info.x_local[i], info.y_local[i]),
+			y_active);
+
   /* Here we know that the insns in the end of SRC1 which are common with SRC2
      will be deleted.
      If we have tablejumps in the end of SRC1 and SRC2
@@ -1706,30 +1448,34 @@
   /* Avoid splitting if possible.  We must always split when SRC2 has
      EH predecessor edges, or we may end up with basic blocks with both
      normal and EH predecessor edges.  */
-  if (newpos2 == BB_HEAD (src2)
+  if (info.cur.x_start == BB_HEAD (src2)
       && !(EDGE_PRED (src2, 0)->flags & EDGE_EH))
     redirect_to = src2;
   else
     {
-      if (newpos2 == BB_HEAD (src2))
+      if (info.cur.x_start == BB_HEAD (src2))
 	{
 	  /* Skip possible basic block header.  */
-	  if (LABEL_P (newpos2))
-	    newpos2 = NEXT_INSN (newpos2);
-	  if (NOTE_P (newpos2))
-	    newpos2 = NEXT_INSN (newpos2);
+	  if (LABEL_P (info.cur.x_start))
+	    info.cur.x_start = NEXT_INSN (info.cur.x_start);
+	  if (NOTE_P (info.cur.x_start))
+	    info.cur.x_start = NEXT_INSN (info.cur.x_start);
 	}
 
       if (dump_file)
 	fprintf (dump_file, "Splitting bb %i before %i insns\n",
 		 src2->index, nmatch);
-      redirect_to = split_block (src2, PREV_INSN (newpos2))->dest;
+      redirect_to = split_block (src2, PREV_INSN (info.cur.x_start))->dest;
     }
 
   if (dump_file)
-    fprintf (dump_file,
-	     "Cross jumping from bb %i to bb %i; %i common insns\n",
-	     src1->index, src2->index, nmatch);
+    {
+      fprintf (dump_file, "Cross jumping from bb %i to bb %i; %i common insns",
+	       src1->index, src2->index, nmatch);
+      if (info.cur.local_count)
+	fprintf (dump_file, ", %i local registers", info.cur.local_count);
+       fprintf (dump_file, "\n");
+    }
 
   redirect_to->count += src1->count;
   redirect_to->frequency += src1->frequency;
@@ -1793,17 +1539,12 @@
 
   /* Edit SRC1 to go to REDIRECT_TO at NEWPOS1.  */
 
-  /* Skip possible basic block header.  */
-  if (LABEL_P (newpos1))
-    newpos1 = NEXT_INSN (newpos1);
-
-  if (NOTE_P (newpos1))
-    newpos1 = NEXT_INSN (newpos1);
-
-  redirect_from = split_block (src1, PREV_INSN (newpos1))->src;
+  redirect_from = split_block (src1, PREV_INSN (y_active))->src;
   to_remove = single_succ (redirect_from);
 
   redirect_edge_and_branch_force (single_succ_edge (redirect_from), redirect_to);
+  COPY_REG_SET (redirect_from->il.rtl->global_live_at_end,
+		redirect_to->il.rtl->global_live_at_start);
   delete_basic_block (to_remove);
 
   update_forwarder_flag (redirect_from);
@@ -1818,7 +1559,7 @@
    any changes made.  */
 
 static bool
-try_crossjump_bb (int mode, basic_block bb)
+try_crossjump_bb (int mode, basic_block bb, bool first_pass)
 {
   edge e, e2, fallthru;
   bool changed;
@@ -1956,21 +1697,20 @@
 static bool
 try_optimize_cfg (int mode)
 {
+  bool first_crossjump_pass = true;
   bool changed_overall = false;
   bool changed;
+  bool changed_since_crossjump = true;
   int iterations = 0;
   basic_block bb, b, next;
 
-  if (mode & CLEANUP_CROSSJUMP)
-    add_noreturn_fake_exit_edges ();
-
   if (mode & (CLEANUP_UPDATE_LIFE | CLEANUP_CROSSJUMP | CLEANUP_THREADING))
     clear_bb_flags ();
 
   FOR_EACH_BB (bb)
     update_forwarder_flag (bb);
 
-  if (! targetm.cannot_modify_jumps_p ())
+  if (!targetm.cannot_modify_jumps_p ())
     {
       first_pass = true;
       /* Attempt to merge blocks as made possible by edge removal.  If
@@ -2125,11 +1865,6 @@
 	      if (try_forward_edges (mode, b))
 		changed_here = true;
 
-	      /* Look for shared code between blocks.  */
-	      if ((mode & CLEANUP_CROSSJUMP)
-		  && try_crossjump_bb (mode, b))
-		changed_here = true;
-
 	      /* Don't get confused by the index shift caused by
 		 deleting blocks.  */
 	      if (!changed_here)
@@ -2138,23 +1873,43 @@
 		changed = true;
 	    }
 
-	  if ((mode & CLEANUP_CROSSJUMP)
-	      && try_crossjump_bb (mode, EXIT_BLOCK_PTR))
-	    changed = true;
-
 #ifdef ENABLE_CHECKING
 	  if (changed)
 	    verify_flow_info ();
 #endif
 
+	  changed_since_crossjump |= changed;
+
+	  if ((mode & CLEANUP_CROSSJUMP)
+	      && changed_since_crossjump
+	      && !changed)
+	    {
+	      update_life_info_in_dirty_blocks (UPDATE_LIFE_GLOBAL_RM_NOTES,
+						(PROP_DEATH_NOTES
+						 | ((mode & CLEANUP_POST_REGSTACK)
+						    ? PROP_POST_REGSTACK : 0)));
+	      add_noreturn_fake_exit_edges ();
+
+	      for (b = ENTRY_BLOCK_PTR->next_bb; b != EXIT_BLOCK_PTR;)
+		{
+		  if (try_crossjump_bb (mode, b, first_crossjump_pass))
+		    changed = 1;
+		  else
+		    b = b->next_bb;
+		}
+	      changed |= try_crossjump_bb (mode, EXIT_BLOCK_PTR,
+					   first_crossjump_pass);
+	      remove_fake_exit_edges ();
+	      changed_since_crossjump = false;
+	      first_crossjump_pass = false;
+	    }
+
 	  changed_overall |= changed;
 	  first_pass = false;
 	}
       while (changed);
-    }
 
-  if (mode & CLEANUP_CROSSJUMP)
-    remove_fake_exit_edges ();
+    }
 
   FOR_ALL_BB (b)
     b->flags &= ~(BB_FORWARDER_BLOCK | BB_NONTHREADABLE_BLOCK);
diff -Naur gcc-4.2.1/gcc/cfgrtl.c gcc-4.2.1-stm/gcc/cfgrtl.c
--- gcc-4.2.1/gcc/cfgrtl.c	2006-11-16 15:10:57.000000000 +0000
+++ gcc-4.2.1-stm/gcc/cfgrtl.c	2007-08-10 13:08:59.000000000 +0100
@@ -1,6 +1,7 @@
 /* Control flow graph manipulation code for GNU compiler.
    Copyright (C) 1987, 1988, 1992, 1993, 1994, 1995, 1996, 1997, 1998,
    1999, 2000, 2001, 2002, 2003, 2004, 2005 Free Software Foundation, Inc.
+   Copyright (c) 2006  STMicroelectronics.
 
 This file is part of GCC.
 
@@ -803,9 +804,10 @@
 	fprintf (dump_file, "Replacing insn %i by jump %i\n",
 		 INSN_UID (insn), INSN_UID (BB_END (src)));
 
-
       delete_insn_chain (kill_from, insn);
 
+      src->flags |= BB_DIRTY;
+
       /* Recognize a tablejump that we are converting to a
 	 simple jump and remove its associated CODE_LABEL
 	 and ADDR_VEC or ADDR_DIFF_VEC.  */
diff -Naur gcc-4.2.1/gcc/ChangeLog.STM gcc-4.2.1-stm/gcc/ChangeLog.STM
--- gcc-4.2.1/gcc/ChangeLog.STM	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.2.1-stm/gcc/ChangeLog.STM	2007-08-10 13:08:59.000000000 +0100
@@ -0,0 +1,582 @@
+2007-07-16  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.h (MOVE_MAX_PIECES): Tuned for TARGET_SH1.
+
+2007-07-12  Carl Shaw  <carl.shaw@st.com>
+
+	* config/sh/linux-unwind.h (sh_fallback_frame_state): Fix R0-R7 location.
+
+2007-06-26  Christian Bruel  <christian.bruel@st.com>
+
+	* gthr-generic.h: Rename *p to *__p.
+
+2007-06-20  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh-protos.h (sh_loads_bankedreg_p): Declare.
+	* config/sh/sh.c (sh_loads_bankedreg_p): New function.
+	(push_regs): Changed saving order or banked registers.
+	(sh_expand_epilogue): Likewise.
+	* config/sh/sh.h (BANKED_REGISTER_P): New macro.
+	(FIRST_BANKED_REG): Likewise.
+	(LAST_BANKED_REG): Likewise.
+	* config/sh/sh.md (banked) New attribute.
+	(in_delay_slot): Check banked attribute.
+
+2007-06-14  Christian Bruel  <christian.bruel@st.com>
+
+	* configure.ac: Unset CONFIG_SITE for canadian cross compilation.
+	* configure: Regenerate.
+
+2007-06-08  Christian Bruel  <christian.bruel@st.com>
+
+	PR target/29953
+	* config/sh/sh.md (doloop_end): New pattern and splitter.
+	* loop-iv.c (simple_rhs_p): Check for hardware registers.
+
+2007-05-23  Christian Bruel  <christian.bruel@st.com>
+
+	* gcc/config/sh/sh.opt (align-small-blocks=): New Optimisation.
+	* gcc/doc/invoke.texi (align-small-blocks=): Likewise.
+	* gcc/config/sh/sh.c (sh_jump_align): Check sh_align_small_blocks.
+	(barrier_align): Check sh_align_small_blocks.
+
+2007-04-30  Christian Bruel  <christian.bruel@st.com>
+
+	* gcc/config/sh/sh-protos.h (sh_jump_align): New Function.
+	* gcc/config/sh/sh.c (sh_jump_align): Likewise.
+	(barrier_align): compute alignment based on TARGET_CACHE32.
+	* gcc/config/sh/sh.h (JUMP_ALIGN): Define.
+
+2007-03-29  Christian Bruel  <christian.bruel@st.com>
+
+	* gcc/config/sh/sh.h (OVERRIDE_OPTIONS): Set assembler_dialect for sh1.
+
+2007-03-28  Christian Bruel  <christian.bruel@st.com>
+	* doc/invoke.texi: Document -m4-300.
+
+2007-02-20  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.md (movsi_i): Fix type attribute.
+	* config/sh/sh.md (movsi_ie): Set memory constraints attribute length.
+	(movsf_ie): Likewise.
+
+2007-03-09  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/sh.c (__nesf2): Renamed.
+	(__nedf2): Likewise.
+	* config/sh/ieee-754-df.S (__nesf2): Likewise.
+	* config/sh/ieee-754-sf.S (__nedf2): Likewise.
+	* config/sh/t-sh: Likewise.
+
+2007-03-04  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/t-mlib-sh4-300: New file.
+	* config.gcc: (with-multilib-list) accept sh4-300.
+
+https://bugzilla.stlinux.com/show_bug.cgi?id=1203
+2007-02-20  Christian Bruel  <christian.bruel@st.com>
+
+	* regmove.c (rel_record_mem): check reg_overlap_mentioned_p.
+
+2007-01-31  Christian Bruel  <christian.bruel@st.com>
+
+	* basic-block.h (pre_edge_lcm_avs): Declare.
+	* config/i386/i386.h (EMIT_MODE_SET): Add FLIP parameter.
+	* doc/tm.texi (EMIT_MODE_SET): Idem.
+	* config/sh/sh.h (EMIT_MODE_SET): Idem. Call emit_fpu_flip.
+        (CONDITIONAL_REGISTER_USAGE): Set global_regs[FPSCR_REG].
+	* config/sh/sh-protos.h	(emit_fpu_flip): Add proto.
+	* config/sh/sh.c (emit_fpu_flip): New function.
+	* config/sh/sh.md (toggle_pr): Defined for TARGET_SH4_300.
+	Defined if TARGET_FPU_SINGLE.
+	fpscr_toggle don't go in delay slot (temporary fix).
+	* lcm.c (pre_edge_lcm_avs): Renamed from pre_edge_lcm.
+	Call clear_aux_for_edges. Fix comments.
+	(pre_edge_lcm): New wrapper function to call pre_edge_lcm_avs.
+	(pre_edge_rev_lcm): Idem.
+	* mode-switching.c (init_modes_infos): New function.
+	(bb_has_complex_pred): New function.
+	(free_modes_infos): Idem.
+	(init_modes_infos): Idem
+	(add_mode_set): Idem.
+	(get_mode): Idem.
+	(commit_mode_sets): Idem.
+	(merge_modes): Idem.
+	(set_flip_status): Idem
+	(test_flip_status): Idem.
+	(optimize_mode_switching): Add support to maintain flip mode information.
+	* testsuite/gcc.target/sh/sh.exp: New file.
+	* testsuite/gcc.target/sh/fpchg1.c: New test.
+	* testsuite/gcc.target/sh/fpchg2.c: Idem.
+
+2007-01-29  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/IEEE-754/m3/adddf3.S: Fix inf mantissa.
+	* config/sh/IEEE-754/m3/addsf3.S: Likewise.
+	* config/sh/IEEE-754/m3/divsf3.S: Intialize xff000000 label.
+	* config/sh/sh.c (expand_sfunc_op): Use FIRST_FP_PARM_REG for
+	parameters.
+	* config/sh/sh.h (TARGET_OSFP): Disable.
+	* config/sh/sh.md (addsf3, subsf3, mulsf3): Use expand_sfunc_binopt
+	only when TARGET_OSFP.
+	(adddf3, subdf3, muldf3): Likewise.
+	(trunkdfsf2): Likewise.
+
+2007-01-22  Christian Bruel  <christian.bruel@st.com>
+
+	* config/sh/t-sh (LIB1ASMFUNCS): Remove _add_sub_sf3, _mulsf3,
+	_hypotf, _muldf3, _add_sub_df3, _divsf3, _divdf3, _fixunssfsi,
+	_fixsfsi, _fixunsdfsi, _fixdfsi, _floatunssisf, _floatsisf,
+	_floatunssidf and _floatsidf.
+	(FPBIT, DPBIT, dp-bit.c, fp-bit.c): Re-instated.
+
+2007-01-12  Andrew Stubbs  <andrew.stubbs@st.com>
+
+	* config/sh/trap-handler.c: Call exit like old one used to.
+
+2006-11-30  J"orn Rennecke  <joern.rennecke@st.com>
+
+	* regmove.c (struct rel_use_chain): New member invalidate_luid.
+	(link_chains): Check invalidate_luid of predecessor.
+	(optimize_related_values_1): Initialize invalidate_luid for chains.
+
+http://gcc.gnu.org/ml/gcc-patches/2006-11/msg01043.html
+2006-11-29  J"orn Rennecke  <joern.rennecke@st.com>
+
+	PR rtl-optimization/28618:
+	* sched-deps.c (sched_analyze_2): When a likely spilled register
+	is used, put in into a scheduling group with the insn that
+	sets it and with all the insns in-between.
+
+2006-09-02  J"orn Rennecke  <joern.rennecke@st.com>
+
+	config/sh/t-sh: ($(T)ic_invalidate_array_4-100.o): Add -I. .
+	($(T)ic_invalidate_array_4-200.o): Likewise.
+	($(T)ic_invalidate_array_4a.o): Likewise.
+
+2006-09-02  J"orn Rennecke  <joern.rennecke@st.com>
+
+	* sh.md (*movsicc_t_false, *movsicc_t_true): Add mode.
+
+2006-11-10  J"orn Rennecke  <joern.rennecke@st.com>
+
+	* sh.c (expand_cbranchdi4): After reload, use appropriate scratch
+	operands.
+
+2006-11-10  J"orn Rennecke  <joern.rennecke@st.com>
+	    Aanchal Khanna   <aanchalk@noida.hcltech.com>
+	    Rakesh Kumar  <rakesh.kumar@noida.hcltech.com>
+
+	PR target/29845
+	* config/sh/sh-protos.h (sh_function_kind): New enumerator
+	SFUNC_FREQUENT.
+	(expand_sfunc_unop, expand_sfunc_binop): Declare.
+	(sh_expand_float_cbranch): Likewise.
+	* config/sh/lib1funcs.asm (ieee-754-sf.S, ieee-754-df.S): #include.
+	* config/sh/t-sh (LIB1ASMFUNCS): Add nesf2, _nedf2, _gtsf2t, _gtdf2t,
+	_gesf2f, _gedf2f, _extendsfdf2, , _truncdfsf2, _add_sub_sf3, _mulsf3,
+	_hypotf, _muldf3, _add_sub_df3, _divsf3, _divdf3, _fixunssfsi,
+	_fixsfsi, _fixunsdfsi, _fixdfsi, _floatunssisf, _floatsisf,
+	_floatunssidf and _floatsidf.
+	(FPBIT, DPBIT, dp-bit.c, fp-bit.c): Removed.
+	* config/sh/ieee-754-df.S, config/sh/ieee-754-sf.S: New files.
+	* config/sh/predicates.md (soft_fp_comparison_operand): New predicate.
+	(soft_fp_comparison_operator): Likewise.
+	* config/sh/sh.c (sh_soft_fp_cmp, expand_sfunc_op): New functions.
+	(expand_sfunc_unop, expand_sfunc_binop): Likewise.
+	(sh_expand_float_cbranch): Likewise.
+	(sh_expand_float_condop, sh_expand_float_scc): Likewise.
+	(from_compare): Add support for software floating point.
+	(function_symbol): Always look up name.  Add SFUNC_FREQUENT case.
+	* config/sh/sh.h (TARGET_SH1_SOFTFP): New macro.
+	(TARGET_SH1_SOFTFP_MODE): Likewise.
+	* config/sh/sh-modes.def (CC_FP_NE, CC_FP_GT, CC_FP_UNLT): New modes.
+	* config/sh/lib1funcs.h (SLC, SLI, SLCMP, DMULU_SAVE): New macros.
+	(DMULUL, DMULUH, DMULU_RESTORE, SHLL4, SHLR4, SHLL6, SHLR6): Likewise.
+	(SHLL12, SHLR12, SHLR19, SHLL23, SHLR24, SHLR21, SHLL21): Likewise.
+	(SHLR11, SHLR22, SHLR23, SHLR20, SHLL20, SHLD_COUNT, SHLRN): Likewise.
+	(SHLLN, DYN_SHIFT): Likewise.
+	(SUPPORT_SH3_OSFP, SUPPORT_SH3E_OSFP): Likewise.
+	(SUPPORT_SH4_NOFPU_OSFP, SUPPORT_SH4_SINGLE_ONLY_OSFP): Likewise.
+	(TARGET_OSFP): Likewise.
+	(OPTIMIZATION_OPTIONS): Always enable TARGET_CBRANCHDI4 and
+	TARGET_EXPAND_CBRANCHDI4.
+	If flag_trapping_math is set, make it 2.
+	(OVERRIDE_OPTIONS): If flag_trapping_math is 2 and non-trapping
+	software floating point is used, clear flag_trapping_math.
+	For SH1, set TARGET_EXPAND_CBRANCHDI4
+	* config/sh/ieee-754-df.S, config/sh/ieee-754-sf.S: New files.
+	* config/sh/IEEE-754/m3/divsf3.S: Likewise.
+	* config/sh/IEEE-754/m3/divdf3.S: Likewise.
+	* config/sh/IEEE-754/m3/floatunssisf.S: Likewise.
+	* config/sh/IEEE-754/m3/floatunssidf.S: Likewise.
+	* config/sh/IEEE-754/m3/fixunsdfsi.S: Likewise.
+	* config/sh/IEEE-754/m3/divdf3-rt.S: Likewise.
+	* config/sh/IEEE-754/m3/addsf3.S: Likewise.
+	* config/sh/IEEE-754/m3/adddf3.S: Likewise.
+	* config/sh/IEEE-754/m3/mulsf3.S: Likewise.
+	* config/sh/IEEE-754/m3/muldf3.S: Likewise.
+	* config/sh/IEEE-754/m3/floatsisf.S: Likewise.
+	* config/sh/IEEE-754/m3/floatsidf.S: Likewise.
+	* config/sh/IEEE-754/m3/fixdfsi.S: Likewise.
+	* config/sh/IEEE-754/divdf3.S: Likewise.
+	* config/sh/IEEE-754/floatunssisf.S: Likewise.
+	* config/sh/IEEE-754/fixunsdfsi.S: Likewise.
+	* config/sh/IEEE-754/adddf3.S: Likewise.
+	* config/sh/IEEE-754/floatsisf.S: Likewise.
+	* config/sh/IEEE-754/muldf3.S: Likewise.
+	* config/sh/IEEE-754/fixdfsi.S: Likewise.
+	* config/sh/IEEE-754/divsf3.S: Likewise.
+	* config/sh/IEEE-754/fixunssfsi.S: Likewise.
+	* config/sh/IEEE-754/floatunssidf.S: Likewise.
+	* config/sh/IEEE-754/addsf3.S: Likewise.
+	* config/sh/IEEE-754/mulsf3.S: Likewise.
+	* config/sh/IEEE-754/floatsidf.S: Likewise.
+	* config/sh/IEEE-754/fixsfsi.S: Likewise.
+	* config/sh/sh.md (SF_NAN_MASK, DF_NAN_MASK, FR4_REG): New constants.
+	(fpcmp_i1, cbranchsf4, cbranchdf4, addsf3_i3, subsf3_i3): New patterns.
+	(mulsf3_i3, cmpnesf_i1, cmpgtsf_i1, cmpunltsf_i1): Likewise.
+	(cmpeqsf_i1_finite, cmplesf_i1_finite, cmpunsf_i1): Likewise.
+	(cmpuneqsf_i1, movcc_fp_ne, movcc_fp_gtmovcc_fp_unlt): Likewise.
+	(cmpltgtsf_t, cmporderedsf_t, cmpltgtsf_t_4): Likewise.
+	(cmporderedsf_t_4, abssc2, adddf3_i3_wrap, adddf3_i3): Likewise.
+	(muldf3_i3_wrap, muldf3_i3, cmpnedf_i1, cmpgtdf_i1): Likewise.
+	(cmpunltdf_i1, cmpeqdf_i1_finite, cmpundf_i1, cmpuneqdf_i1): Likewise.
+	(cmpltgtdf_t, cmpordereddf_t_4, extendsfdf2_i1): Likewise.
+	(extendsfdf2_i2e, extendsfdf2_i2e_r0, truncdfsf2_i2e): Likewise.
+	(extendsfdf2_i1_r0, truncdfsf2_i1): Likewise.
+	(cmpun_sdf, sunle, cmpuneq_sdf, bunle, bunlt): Likewise.
+	(beq, bne, bgt, blt, ble, bge): Only support for SHmedia.
+	(bgtu, bltu, bgeu, bleu): Likewise.
+	(seq, sge, sne): Add support for software floating point.
+	(addsf3, subsf3, mulsf3): Likewise.
+	(adddf3, subdf3, muldf3, extendsfdf2, truncdfsf2): Likewise.
+	(movnegt): Match only one operand.  Changed user.
+
+http://gcc.gnu.org/ml/gcc-patches/2006-11/msg01074.html
+2006-11-15  J"orn Rennecke  <joern.rennecke@st.com>
+
+	PR middle-end/29847
+	* targhooks.c (regs.h): Include.
+	(default_match_adjust): New function, broken out of
+	reload.c:operands_match_p.
+	* targhooks.h (default_match_adjust): Declare.
+	* reload.c (operands_match_p): Use targetm.match_adjust.
+	* target.h (struct gcc_target): New member match_adjust.
+	* target-def.h (TARGET_MATCH_ADJUST): Define.
+	(TARGET_INITIALIZER): Add TARGET_MATCH_ADJUST.
+	* Makefile.in (targhooks.o): Depend on $(REGS_H).
+	* doc/tm.texi (TARGET_MATCH_ADJUST): Document.
+
+2006-09-02  J"orn Rennecke  <joern.rennecke@st.com>
+
+	* config/sh/sh-protos.h (sh_match_adjust): Declare.
+	* config/sh/sh.c (TARGET_MATCH_ADJUST): Define as sh_match_adjust.
+	(sh_match_adjust): New function.
+
+http://gcc.gnu.org/ml/gcc-patches/2006-11/msg01069.html
+2006-11-15  J"orn Rennecke  <joern.rennecke@st.com>
+
+	PR middle-end/29846
+	* optabs.c (can_compare_p): Succeed if a cbranch handler exists,
+	irrespective of the existance of a compare instruction.
+
+2006-11-06  J"orn Rennecke  <joern.rennecke@st.com>
+
+	* sh.opt (minline-ic_invalidate): New option.
+	(musermode): Adjust comment.
+	* sh.c (sh_initialize_trampoline): Emit library call unless
+	is set; if it is set, don't emit library call if we can use icbi
+	instead.
+	* sh.md (ic_invalidate_line, ic_invalidate_line_sh4a): Also use
+	icbi for TARGET_SH4_300.
+	* t-sh (LIB1ASMFUNCS_CACHE): Set.
+
+2006-11-03  Andrew Stubbs  <andrew.stubbs@st.com>
+
+	* config/sh/crt1.asm (_superh_trap_handler): Remove function.
+	* config/sh/trap-handler.c: New file.
+	* config/sh/t-elf (EXTRA_MULTILIB_PARTS): Add trap-handler.o.
+	* config/sh/t-superh (EXTRA_MULTILIB_PARTS): Likewise.
+	* config/sh/t-sh: Add rule for trap-handler.o.
+	* config/sh/elf.h (STARTFILE_SPEC): Add trap-handler.o.
+	* config/sh/superh.h (STARTFILE_SPEC): Likewise.
+
+2006-10-06  Andrew Stubbs  <andrew.stubbs@st.com>
+
+	* config/sh/crt1.asm (vbr_600): Add missing #if.
+
+2006-08-03  J"orn Rennecke  <joern.rennecke@st.com>
+
+	* sh.opt (mfused-madd): New option.
+	* sh.md (mac_media, macsf3): Make conditional on TARGET_FMAC.
+
+2006-08-01  J"orn Rennecke  <joern.rennecke@st.com>
+
+	* sh.md (cbranchdi4): Move earlyclobber modifier to affected
+	alternative.
+	* sh.c (expand_cbranchsi4): New argument probability.  Changed all
+	callers.
+	(expand_cbranchdi4): When emitting more than one branch, distribute
+	probabilities.
+	* sh-protos.h (expand_cbranchsi4): Update prorotype.
+
+2006-07-04  J"orn Rennecke  <joern.rennecke@st.com>
+
+	PR tree-optimization/28144
+	* fold-const.c (fold_convert_const_int_from_real): Fix endianness in
+	real_from_integer calls; adjust high word for lower bound.
+
+2006-07-04  Andrew Stubbs  <andrew.stubbs@st.com>
+
+	Fix INSbl24460:
+	* config/sh/crt1.asm (vbr_start): Move to new section .test.vbr.
+	Remove pointless handler at VBR+0.
+	(vbr_200, vbr_300, vbr_500): Remove pointless handler.
+	(vbr_600): Save and restore mach and macl, fpul and fpscr and fr0 to
+	fr7. Make sure the timer handler is called with the correct FPU
+	precision setting, according to the ABI.
+
+2006-06-23  J"orn Rennecke <joern.rennecke@st.com>
+
+	PR tree-optimization/28144
+	* fold-const.c (fold_convert_const_int_from_real): When converting
+	to a type with a precision less than 32, use INT_MIN / INT_MAX
+	for saturation bounds.
+
+2006-06-14  J"orn Rennecke <joern.rennecke@st.com>
+
+	* config/sh/sh.opt (m2a-single, m2a-single-only): Fix Condition.
+	* config/sh/sh.h (SUPPORT_SH2A_NOFPU): Fix condition.
+	(SUPPORT_SH2A_SINGLE_ONLY, SUPPORT_SH2A_SINGLE_ONLY): Likewise.
+
+2006-06-14  J"orn Rennecke <joern.rennecke@st.com>
+
+	* Makefile.in (version.o): Depend on $(OBJS) except for version.o .
+
+2006-06-13  J"orn Rennecke <joern.rennecke@st.com>
+
+	* t-sh (MULTILIB_MATCHES): Add and m4-100-nofpu,m4-200-nofpu.
+
+2006-06-09  J"orn Rennecke <joern.rennecke@st.com>
+
+	* sh.md (cmpgeusi_t): Change into define_insn_and_split.  Accept
+	zero as second operand.
+
+2006-05-09  J"orn Rennecke <joern.rennecke@st.com>
+
+	* config/sh/sh.c (expand_cbranchdi4): Avoid calling
+	swap_condition (CODE_FOR_nothing).
+
+2006-05-03  Andrew Stubbs  <andrew.stubbs@st.com>
+            J"orn Rennecke <joern.rennecke@st.com>
+
+	* gcc.c (process_command): Try to set gcc_exec_prefix with
+	make_relative_prefix_ignore_links before resolving to
+	make_relative_prefix.
+	If gcc_exec_prefix has been pre-set, use
+	make_relative_prefix_ignore_links to calculate gcc_libexec_prefix.
+
+2006-04-28  J"orn Rennecke <joern.rennecke@st.com>
+
+	* config/sh/divtab-sh4-300.c, config/sh/lib1funcs-4-300.asm:
+	Fixed some bugs related to nagative values, in particular -0
+	and overflow at -0x80000000.
+	* config/sh/divcost-analysis: added sh4-300 figures.
+	* testsuite/gcc.c-torture/execute/arith-rand-ll.c:
+	Also test for bogus rest sign.
+
+2006-04-27  J"orn Rennecke <joern.rennecke@st.com>
+
+	* config/sh/t-sh (MULTILIB_MATCHES): Add -m4-300* / -m4-340 options.
+
+2006-04-26  J"orn Rennecke <joern.rennecke@st.com>
+
+	* sh.c (expand_cbranchdi4): Avoid generating >= 0U tests.
+
+2006-04-26  J"orn Rennecke <joern.rennecke@st.com>
+
+	* config/sh/t-sh (OPT_EXTRA_PARTS): Add libgcc-4-300.a.
+	($(T)div_table-4-300.o, $(T)libgcc-4-300.a): New rules.
+	* config/sh/divtab-sh4-300.c, config/sh/lib1funcs-4-300.asm: New files.
+	* config/sh/embed-elf.h (LIBGCC_SPEC): Use -lgcc-4-300 for -m4-300* /
+	-m4-340.
+
+	* config/sh/sh.opt: Amend -mdiv= option description.
+
+2006-04-24  J"orn Rennecke <joern.rennecke@st.com>
+
+	* sh.md (mulr03, mulr03+1, mulr03+2): New patterns.
+	(mulsi3) Use mulr03 for TARGET_R0R3_TO_REG_MUL.
+	* sh.h (CONDITIONAL_REGISTER_USAGE): If -mr0r3-to-reg-mul is not in
+	effect, set regno_reg_class for R1_REG .. R3_REG to GENERAL_REGS.
+	(OVERRIDE_OPTIONS): Initilize TARGET_R0R3_TO_REG_MUL if it has
+	not been set by an option.
+	(enum reg_class, REG_CLASS_NAMES, REG_CLASS_CONTENTS): Add R0R3_REGS.
+	(REG_CLASS_FROM_CONSTRAINT): Decode R03.
+	* sh.opt (mlate-r0r3-to-reg-mul, mr0r3-to-reg-mul): New options.
+
+	SH4-300 scheduling description & fixes to SH4-[12]00 description:
+	* sh.md: New instruction types: fstore, movi8, fpscr_toggle, gp_mac,
+	mac_mem, mem_mac, dfp_mul, fp_cmp.
+	(insn_class, dfp_comp, any_fp_comp): Update.
+	(push_fpul, movsf_ie, fpu_switch, toggle_sz, toggle_pr): Update type.
+	(cmpgtsf_t, "cmpeqsf_t, cmpgtsf_t_i4, cmpeqsf_t_i4): Likewise.
+	(muldf3_i): Likewise.
+	(movsi_i): Split rI08 alternative into two separate alternatives.
+	Update type.
+	(movsi_ie, movsi_i_lowpart): Likewise.
+	(movqi_i): Split ri alternative into two separate alternatives.
+	Update type.
+	* sh1.md (sh1_load_store, sh1_fp): Update.
+	* sh4.md (sh4_store, sh4_mac_gp, fp_arith, fp_double_arith): Update.
+	(mac_mem, sh4_fpscr_toggle): New insn_reservations.
+	* sh4a.md (sh4a_mov, sh4a_load, sh4a_store, sh4a_fp_arith): Update.
+	(sh4a_fp_double_arith): Likewise.
+	* sh4-300.md: New file.
+	* sh.c (sh_handle_option): Handle m4-300* options.
+	(sh_adjust_cost): Fix latency of auto-increments.
+	Handle SH4-300 differently than other SH4s.  Check for new insn types.
+	* sh.h (OVERRIDE_OPTIONS): Initilize sh_branch_cost if it has not been
+	set by an option.
+	* sh.opt (m4-300, m4-100-nofpu, m4-200-nofpu): New options.
+	(m4-300-nofpu, -m4-340, m4-300-single, m4-300-single-only): Likewise.
+	(mbranch-cost=, mload-latency=): Likewise.
+	* superh.h (STARTFILE_SPEC): Take -m4-340 into account.
+
+	* sh.md (mulsf3): Remove special expansion code.
+	(mulsf3_ie): Now a define_insn_and_split.
+	(macsf3): Allow for TARGET_SH4.
+
+	* config/sh/predicates.md (less_comparison_operator): Remove
+	redundant code.
+
+	* sh.md (cbranchsi4, cbranchdi4, cbranchdi4_i): New patterns.
+	* sh.c (prepare_cbranch_operands, expand_cbranchsi4): New functions.
+	(expand_cbranchdi4): Likewise.
+	(sh_rtx_costs): Give lower cost for certain CONST_INT values and for
+	CONST_DOUBLE if the outer code is COMPARE.
+	* sh.h (OPTIMIZATION_OPTIONS): If not optimizing for size, set
+	TARGET_CBRANCHDI4 and TARGET_EXPAND_CBRANCHDI4.
+	(OVERRIDE_OPTIONS): For TARGET_SHMEDIA, clear TARGET_CBRANCHDI4.
+	(LEGITIMATE_CONSTANT_P): Also allow DImode and VOIDmode CONST_DOUBLEs.
+	Remove redundant fp_{zero,one}_operand checks.
+	* sh.opt (mcbranchdi, mexpand-cbranchdi, mcmpeqdi): New options.
+	* sh-protos.h (prepare_cbranch_operands, expand_cbranchsi4): Declare.
+	(expand_cbranchdi4): Likewise.
+
+2006-04-20  J"orn Rennecke <joern.rennecke@st.com>
+
+	* sh.h (LEGITIMATE_CONSTANT_P): Allow VOIDmode CONST_DOUBLEs.
+
+2006-04-20  J"orn Rennecke <joern.rennecke@st.com>
+
+	* sh.h (LOCAL_ALIGNMENT): Use DATA_ALIGNMENT.
+
+2006-04-11  J"orn Rennecke <joern.rennecke@st.com>
+
+	* gthr-generic.h: Update to match
+	http://gcc.gnu.org/ml/gcc-patches/2006-04/msg00237.html .
+	* gthr-generic.c, gthr-objc-generic.c: Likewise.
+	* Makefile.in configure.ac: Likewise.
+	* configure: Regenerate.
+
+2006-03-27  Andrew Stubbs  <andrew.stubbs@st.com>
+
+	* gcc.c (read_specs): Add Cygwin path support for MinGW.
+	(find_a_file, process_file): Likewise.
+	* opts.c (common_handle_option): Likewise.
+	* prefix.c (update_path): Likewise.
+	* c-opts.c (c_common_handle_option): Likewise.
+	(check_deps_environment_vars): Likewise.
+	* c-incpath.c (add_path): Likewise.
+	* collect2.c (find_a_file, add_prefix): Likewise.
+	* Makefile.in (INSTALL_HEADERS): Disable the broken install-mkheaders.
+
+2006-03-24  J"orn Rennecke <joern.rennecke@st.com>
+
+	* t-sh (IC_EXTRA_PARTS): Replace ic_invalidate_array_* parts with
+	libic_invalidate_array_* parts.  Rename affected rules.
+	* embed-elf.h (LIBGCC_SPEC): Update.
+
+2006-03-21  J"orn Rennecke <joern.rennecke@st.com>
+
+	Apply patch to re-instate struct-equiv code for crossjump
+	matching:
+	http://gcc.gnu.org/ml/gcc-patches/2006-02/msg01488.html
+	Bernd Schmidt  <bernds@redhat.com>
+	* cfgcleanup.c
+	* cfgrtl.c
+	* cfgcleanup.c
+	* basic-block.h
+	* cfgcleanup.c
+	* struct-equiv.c
+
+2006-03-15  J"orn Rennecke <joern.rennecke@st.com>
+
+	* version.c (VERSUFFIX, bug_report_url): Customize for STM.
+
+2006-03-10  J"orn Rennecke <joern.rennecke@st.com>
+
+	http://gcc.gnu.org/ml/gcc-patches/2006-03/msg00602.html
+	* regmove.c (init_add_limits): Check find_matches to verify a
+	three-address-add instruction is genuine.
+
+2006-01-20  J"orn Rennecke <joern.rennecke@st.com>
+
+	http://gcc.gnu.org/ml/gcc-patches/2006-01/msg01387.html
+	* postreload.c (reload_combine): For every new use of REG_SUM, record
+	the use of BASE.
+
+2006-01-17  Antony King <anthony.king@st.com>
+            J"orn Rennecke <joern.rennecke@st.com>
+
+	* configure.ac: Recognize 'generic' value for threads.
+	Check for existance of a *.c and gthr-objc-*.c file for thread support.
+	Substiture in extra_libgcc_srcs and extra_libgcc_static_srcs.
+	* configure: Regenerate.
+	* Makefile.in (LIB2ADD): Add @extra_libgcc_srcs@.
+	(LIB2ADD_ST): Add @extra_libgcc_static_srcs@.
+	* gthr-generic.h: New file.
+	* gthr-generic.c: New file.
+	* gthr-objc-generic.c: New file.
+
+2005-09-15  J"orn Rennecke <joern.rennecke@st.com>
+	    Bernd Schmidt  <bernds@redhat.com>
+
+	PR rtl-optimization/20211
+	http://gcc.gnu.org/ml/gcc-patches/2005-09/msg01176.html
+	* common.opt: Add optimize-related-values entry.
+	* opts.c (decode_options): Set flag_optimize_related_values.
+	* optabs.c (gen_add3_insn): If direct addition is not possible,
+	try to move the constant into the destination register first.
+	* regmove.c (obstack.h, ggc.h, optabs.h): Include.
+	(related, rel_use_chain, rel_mod, rel_use): New structures.
+	(related_baseinfo, update): Likewise.
+	(lookup_related, rel_build_chain): New functions.
+	(recognize_related_for_insn, record_reg_use, create_rel_use): Likewise.
+	(new_reg_use, rel_record_mem, new_base, invalidate_related): Likewise.
+	(find_related, find_related_toplev, chain_starts_earlier): Likewise.
+	(chain_ends_later, mod_before, remove_setting_insns): Likewise.
+	(perform_addition, modify_address): Likewise.
+	(optimize_related_values_1, optimize_related_values_0): Likewise.
+	(optimize_related_values, count_sets, link_chains): Likewise.
+	(init_add_limits): Likewise.
+	(REL_USE_HASH_SIZE, REL_USE_HASH, rel_alloc, rel_new): New macros.
+	(regno_related, rel_base_list, unrelatedly_used): New variables.
+	(related_obstack, add_limits): Likewise.
+	(regmove_optimize): Call optimize_related_values.
+	Include gt-regmove.h.
+	(have_3addr_const_add): New variable.
+	* Makefile.in (gt-regmove.h): New rule.
+	(regmove.o): Depend on $(OPTABS_H) and gt-regmove.h.
+	(GTFILES): Add regmove.c.
+	* doc/invoke.texi: Document -foptimize-related-values.
+
+2005-09-15  J"orn Rennecke <joern.rennecke@st.com>
+
+	* regmove.c (discover_flags_reg): Use the PATTERN of an INSN.
+
+	* regmove.c (fixup_match_1): When moving a death note, check if
+	it needs changing into a REG_UNUSED note.
+
diff -Naur gcc-4.2.1/gcc/c-incpath.c gcc-4.2.1-stm/gcc/c-incpath.c
--- gcc-4.2.1/gcc/c-incpath.c	2006-05-18 23:16:23.000000000 +0100
+++ gcc-4.2.1-stm/gcc/c-incpath.c	2007-08-10 13:08:59.000000000 +0100
@@ -347,8 +347,23 @@
   char* c;
   for (c = path; *c; c++)
     if (*c == '\\') *c = '/';
+
+/* The native stat() on NT4 accepts trailing slashes, but not
+    backslashes.  Mingw's stat on Windows 2000/XP doesn't accept
+    either trailing slashes or backslashes.  (C:/ and C:\ are of
+    course fine.)  It is harmless to remove trailing slashes on
+    systems that do accept them, so we do it unconditionally.  */
+  while (c > path+1 && c[-1] == '/')
+    c--;
+  if (c == path+2 
+      && ISALPHA (path[0]) && path[1] == ':' && path[2] == '/')
+    c++;
+  if (*c == '/')
+    *c = '\0';
 #endif
 
+  CYGPATH_REPLACE (&path);
+
   p = XNEW (cpp_dir);
   p->next = NULL;
   p->name = path;
diff -Naur gcc-4.2.1/gcc/collect2.c gcc-4.2.1-stm/gcc/collect2.c
--- gcc-4.2.1/gcc/collect2.c	2006-05-18 23:16:23.000000000 +0100
+++ gcc-4.2.1-stm/gcc/collect2.c	2007-08-10 13:08:59.000000000 +0100
@@ -5,6 +5,7 @@
    Contributed by Chris Smith (csmith@convex.com).
    Heavily modified by Michael Meissner (meissner@cygnus.com),
    Per Bothner (bothner@cygnus.com), and John Gilmore (gnu@cygnus.com).
+   Copyright (c) 2006  STMicroelectronics.
 
 This file is part of GCC.
 
@@ -570,7 +571,10 @@
 {
   char *temp;
   struct prefix_list *pl;
-  int len = pprefix->max_len + strlen (name) + 1;
+  int len;
+
+  CYGPATH (name);
+  len = pprefix->max_len + strlen (name) + 1;
 
   if (debug)
     fprintf (stderr, "Looking for '%s'\n", name);
@@ -596,6 +600,7 @@
 	  if (debug)
 	    fprintf (stderr, "  - found: absolute path\n");
 
+	  CYGPATH_FREE (name);
 	  return temp;
 	}
 
@@ -606,7 +611,10 @@
 	strcat (temp, HOST_EXECUTABLE_SUFFIX);
 
 	if (access (temp, X_OK) == 0)
-	  return temp;
+	  {
+	    CYGPATH_FREE (name);
+	    return temp;
+	  }
 #endif
 
       if (debug)
@@ -623,7 +631,10 @@
 	if (stat (temp, &st) >= 0
 	    && ! S_ISDIR (st.st_mode)
 	    && access (temp, X_OK) == 0)
-	  return temp;
+	  {
+	    CYGPATH_FREE (name);
+	    return temp;
+	  }
 
 #ifdef HOST_EXECUTABLE_SUFFIX
 	/* Some systems have a suffix for executable files.
@@ -633,7 +644,10 @@
 	if (stat (temp, &st) >= 0
 	    && ! S_ISDIR (st.st_mode)
 	    && access (temp, X_OK) == 0)
-	  return temp;
+	  {
+	    CYGPATH_FREE (name);
+	    return temp;
+	  }
 #endif
       }
 
@@ -641,6 +655,7 @@
     fprintf (stderr, "  - failed: no entries in prefix list\n");
 
   free (temp);
+  CYGPATH (name);
   return 0;
 }
 
@@ -663,6 +678,7 @@
 
   /* Keep track of the longest prefix.  */
 
+  CYGPATH (prefix);
   len = strlen (prefix);
   if (len > pprefix->max_len)
     pprefix->max_len = len;
diff -Naur gcc-4.2.1/gcc/common.opt gcc-4.2.1-stm/gcc/common.opt
--- gcc-4.2.1/gcc/common.opt	2007-03-12 04:40:09.000000000 +0000
+++ gcc-4.2.1-stm/gcc/common.opt	2007-08-10 13:08:59.000000000 +0100
@@ -1,6 +1,7 @@
 ; Options for the language- and target-independent parts of the compiler.
 
 ; Copyright (C) 2003, 2004, 2005, 2006 Free Software Foundation, Inc.
+; Copyright (c) 2006  STMicroelectronics.
 ;
 ; This file is part of GCC.
 ;
@@ -636,6 +637,10 @@
 Common Report Var(flag_regmove)
 Do the full register move optimization pass
 
+foptimize-related-values
+Common Report Var(flag_optimize_related_values)
+Enable additional regmove optimization for base reg + offset expressions
+
 foptimize-sibling-calls
 Common Report Var(flag_optimize_sibling_calls)
 Optimize sibling and tail recursive calls
diff -Naur gcc-4.2.1/gcc/config/i386/i386.h gcc-4.2.1-stm/gcc/config/i386/i386.h
--- gcc-4.2.1/gcc/config/i386/i386.h	2007-06-21 08:06:29.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/i386/i386.h	2007-08-10 13:08:59.000000000 +0100
@@ -2208,7 +2208,7 @@
    is the set of hard registers live at the point where the insn(s)
    are to be inserted.  */
 
-#define EMIT_MODE_SET(ENTITY, MODE, HARD_REGS_LIVE) 			\
+#define EMIT_MODE_SET(ENTITY, MODE, FLIP, HARD_REGS_LIVE)		\
   ((MODE) != I387_CW_ANY && (MODE) != I387_CW_UNINITIALIZED		\
    ? emit_i387_cw_initialization (MODE), 0				\
    : 0)
diff -Naur gcc-4.2.1/gcc/config/sh/crt1.asm gcc-4.2.1-stm/gcc/config/sh/crt1.asm
--- gcc-4.2.1/gcc/config/sh/crt1.asm	2006-03-27 15:11:14.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/crt1.asm	2007-08-10 13:08:59.000000000 +0100
@@ -1,5 +1,6 @@
 /* Copyright (C) 2000, 2001, 2003, 2004, 2005 Free Software Foundation, Inc.
    This file was pretty much copied from newlib.
+   Copyright (c) 2006  STMicroelectronics.
 
 This file is part of GCC.
 
@@ -894,23 +895,10 @@
 	nop
 #ifdef VBR_SETUP
 ! Exception handlers	
-	.balign 256
+	.section .text.vbr, "ax"
 vbr_start:
-	mov.l 2f, r0     ! load the old vbr setting (if any)
-	mov.l @r0, r0
-	cmp/eq #0, r0
-	bf 1f
-	! no previous vbr - jump to own generic handler
-	bra handler
-	nop
-1:	! there was a previous handler - chain them
-	jmp @r0
-	nop
-	.balign 4
-2:
-	.long old_vbr
 
-	.balign 256
+	.org 0x100
 vbr_100:
 	#ifdef PROFILE
 	! Note on register usage.
@@ -1017,50 +1005,7 @@
 2:	
 	.long old_vbr
 
-	.balign 256
-vbr_200:
-	mov.l 2f, r0     ! load the old vbr setting (if any)
-	mov.l @r0, r0
-	cmp/eq #0, r0
-	bf 1f
-	! no previous vbr - jump to own generic handler
-	bra handler
-	nop	
-1:	! there was a previous handler - chain them
-	add #0x7f, r0	 ! 0x7f
-	add #0x7f, r0	 ! 0xfe
-	add #0x7f, r0	 ! 0x17d
-	add #0x7f, r0    ! 0x1fc
-	add #0x4, r0     ! add 0x200 without corrupting another register
-	jmp @r0
-	nop
-	.balign 4
-2:
-	.long old_vbr
-
-	.balign 256
-vbr_300:
-	mov.l 2f, r0     ! load the old vbr setting (if any)
-	mov.l @r0, r0
-	cmp/eq #0, r0
-	bf 1f
-	! no previous vbr - jump to own generic handler
-	bra handler
-	nop	
-1:	! there was a previous handler - chain them
-	rotcr r0
-	rotcr r0
-	add #0x7f, r0	 ! 0x1fc
-	add #0x41, r0	 ! 0x300
-	rotcl r0
-	rotcl r0	 ! Add 0x300 without corrupting another register
-	jmp @r0
-	nop
-	.balign 4
-2:
-	.long old_vbr
-
-	.balign 256	
+	.org 0x400
 vbr_400:	! Should be at vbr+0x400
 	mov.l 2f, r0     ! load the old vbr setting (if any)
 	mov.l @r0, r0
@@ -1103,28 +1048,7 @@
 	jmp @r2
 	nop
 
-	.balign 256
-vbr_500:
-	mov.l 2f, r0     ! load the old vbr setting (if any)
-	mov.l @r0, r0
-	cmp/eq #0, r0
-	! no previous vbr - jump to own generic handler
-	bt handler
-	! there was a previous handler - chain them
-	rotcr r0
-	rotcr r0
-	add #0x7f, r0	 ! 0x1fc
-	add #0x7f, r0	 ! 0x3f8
-	add #0x42, r0	 ! 0x500
-	rotcl r0
-	rotcl r0	 ! Add 0x500 without corrupting another register
-	jmp @r0
-	nop
-	.balign 4
-2:
-	.long old_vbr
-
-	.balign 256
+	.org 0x600
 vbr_600:
 #ifdef PROFILE	
 	! Should be at vbr+0x600
@@ -1140,11 +1064,48 @@
 	mov.l	r6,@-r15
 	mov.l	r7,@-r15
 	sts.l	pr,@-r15
+	sts.l	mach,@-r15
+	sts.l	macl,@-r15
+#if defined(__SH_FPU_ANY__)
+	! Save fpul and fpscr, save fr0-fr7 in 64 bit mode
+	! and set the pervading precision for the timer_handler
+	mov	#0,r0
+	sts.l	fpul,@-r15
+	sts.l	fpscr,@-r15
+	lds	r0,fpscr	! Clear fpscr
+	fmov	fr0,@-r15
+	fmov	fr1,@-r15
+	fmov	fr2,@-r15
+	fmov	fr3,@-r15
+	mov.l	pervading_precision_k,r0
+	fmov	fr4,@-r15
+	fmov	fr5,@-r15
+	mov.l	@r0,r0
+	fmov	fr6,@-r15
+	fmov	fr7,@-r15
+	lds	r0,fpscr
+#endif /* __SH_FPU_ANY__ */
 	! Pass interrupted pc to timer_handler as first parameter (r4).
 	stc    spc, r4
 	mov.l timer_handler_k, r0
 	jsr @r0
 	nop
+#if defined(__SH_FPU_ANY__)
+	mov	#0,r0
+	lds	r0,fpscr	! Clear the fpscr
+	fmov	@r15+,fr7
+	fmov	@r15+,fr6
+	fmov	@r15+,fr5
+	fmov	@r15+,fr4
+	fmov	@r15+,fr3
+	fmov	@r15+,fr2
+	fmov	@r15+,fr1
+	fmov	@r15+,fr0
+	lds.l	@r15+,fpscr
+	lds.l	@r15+,fpul
+#endif /* __SH_FPU_ANY__ */
+	lds.l @r15+,macl
+	lds.l @r15+,mach
 	lds.l @r15+,pr
 	mov.l @r15+,r7
 	mov.l @r15+,r6
@@ -1157,6 +1118,13 @@
 	stc sgr, r15    ! Restore r15, destroyed by this sequence. 
 	rte
 	nop
+#if defined(__SH_FPU_ANY__)
+	.balign 4
+pervading_precision_k:
+#define CONCAT1(A,B) A##B
+#define CONCAT(A,B) CONCAT1(A,B)
+	.long CONCAT(__USER_LABEL_PREFIX__,__fpscr_values)+4
+#endif
 #else
 	mov.l 2f, r0     ! Load the old vbr setting (if any).
 	mov.l @r0, r0
@@ -1205,201 +1173,5 @@
 handler_exit_k:
 	.long _exit
 	.align 2
-! Simulated compile of trap handler.
-	.section	.debug_abbrev,"",@progbits
-.Ldebug_abbrev0:
-	.section	.debug_info,"",@progbits
-.Ldebug_info0:
-	.section	.debug_line,"",@progbits
-.Ldebug_line0:
-	.text
-.Ltext0:
-	.align 5
-	.type	__superh_trap_handler,@function
-__superh_trap_handler:
-.LFB1:
-	mov.l	r14,@-r15
-.LCFI0:
-	add	#-4,r15
-.LCFI1:
-	mov	r15,r14
-.LCFI2:
-	mov.l	r4,@r14
-	lds	r1, pr
-	add	#4,r14
-	mov	r14,r15
-	mov.l	@r15+,r14
-	rts	
-	nop
-.LFE1:
-.Lfe1:
-	.size	__superh_trap_handler,.Lfe1-__superh_trap_handler
-	.section	.debug_frame,"",@progbits
-.Lframe0:
-	.ualong	.LECIE0-.LSCIE0
-.LSCIE0:
-	.ualong	0xffffffff
-	.byte	0x1
-	.string	""
-	.uleb128 0x1
-	.sleb128 -4
-	.byte	0x11
-	.byte	0xc
-	.uleb128 0xf
-	.uleb128 0x0
-	.align 2
-.LECIE0:
-.LSFDE0:
-	.ualong	.LEFDE0-.LASFDE0
-.LASFDE0:
-	.ualong	.Lframe0
-	.ualong	.LFB1
-	.ualong	.LFE1-.LFB1
-	.byte	0x4
-	.ualong	.LCFI0-.LFB1
-	.byte	0xe
-	.uleb128 0x4
-	.byte	0x4
-	.ualong	.LCFI1-.LCFI0
-	.byte	0xe
-	.uleb128 0x8
-	.byte	0x8e
-	.uleb128 0x1
-	.byte	0x4
-	.ualong	.LCFI2-.LCFI1
-	.byte	0xd
-	.uleb128 0xe
-	.align 2
-.LEFDE0:
-	.text
-.Letext0:
-	.section	.debug_info
-	.ualong	0xb3
-	.uaword	0x2
-	.ualong	.Ldebug_abbrev0
-	.byte	0x4
-	.uleb128 0x1
-	.ualong	.Ldebug_line0
-	.ualong	.Letext0
-	.ualong	.Ltext0
-	.string	"trap_handler.c"
-	.string	"xxxxxxxxxxxxxxxxxxxxxxxxxxxx"
-	.string	"GNU C 3.2 20020529 (experimental)"
-	.byte	0x1
-	.uleb128 0x2
-	.ualong	0xa6
-	.byte	0x1
-	.string	"_superh_trap_handler"
-	.byte	0x1
-	.byte	0x2
-	.byte	0x1
-	.ualong	.LFB1
-	.ualong	.LFE1
-	.byte	0x1
-	.byte	0x5e
-	.uleb128 0x3
-	.string	"trap_reason"
-	.byte	0x1
-	.byte	0x1
-	.ualong	0xa6
-	.byte	0x2
-	.byte	0x91
-	.sleb128 0
-	.byte	0x0
-	.uleb128 0x4
-	.string	"unsigned int"
-	.byte	0x4
-	.byte	0x7
-	.byte	0x0
-	.section	.debug_abbrev
-	.uleb128 0x1
-	.uleb128 0x11
-	.byte	0x1
-	.uleb128 0x10
-	.uleb128 0x6
-	.uleb128 0x12
-	.uleb128 0x1
-	.uleb128 0x11
-	.uleb128 0x1
-	.uleb128 0x3
-	.uleb128 0x8
-	.uleb128 0x1b
-	.uleb128 0x8
-	.uleb128 0x25
-	.uleb128 0x8
-	.uleb128 0x13
-	.uleb128 0xb
-	.byte	0x0
-	.byte	0x0
-	.uleb128 0x2
-	.uleb128 0x2e
-	.byte	0x1
-	.uleb128 0x1
-	.uleb128 0x13
-	.uleb128 0x3f
-	.uleb128 0xc
-	.uleb128 0x3
-	.uleb128 0x8
-	.uleb128 0x3a
-	.uleb128 0xb
-	.uleb128 0x3b
-	.uleb128 0xb
-	.uleb128 0x27
-	.uleb128 0xc
-	.uleb128 0x11
-	.uleb128 0x1
-	.uleb128 0x12
-	.uleb128 0x1
-	.uleb128 0x40
-	.uleb128 0xa
-	.byte	0x0
-	.byte	0x0
-	.uleb128 0x3
-	.uleb128 0x5
-	.byte	0x0
-	.uleb128 0x3
-	.uleb128 0x8
-	.uleb128 0x3a
-	.uleb128 0xb
-	.uleb128 0x3b
-	.uleb128 0xb
-	.uleb128 0x49
-	.uleb128 0x13
-	.uleb128 0x2
-	.uleb128 0xa
-	.byte	0x0
-	.byte	0x0
-	.uleb128 0x4
-	.uleb128 0x24
-	.byte	0x0
-	.uleb128 0x3
-	.uleb128 0x8
-	.uleb128 0xb
-	.uleb128 0xb
-	.uleb128 0x3e
-	.uleb128 0xb
-	.byte	0x0
-	.byte	0x0
-	.byte	0x0
-	.section	.debug_pubnames,"",@progbits
-	.ualong	0x27
-	.uaword	0x2
-	.ualong	.Ldebug_info0
-	.ualong	0xb7
-	.ualong	0x67
-	.string	"_superh_trap_handler"
-	.ualong	0x0
-	.section	.debug_aranges,"",@progbits
-	.ualong	0x1c
-	.uaword	0x2
-	.ualong	.Ldebug_info0
-	.byte	0x4
-	.byte	0x0
-	.uaword	0x0
-	.uaword	0x0
-	.ualong	.Ltext0
-	.ualong	.Letext0-.Ltext0
-	.ualong	0x0
-	.ualong	0x0
 #endif /* VBR_SETUP */
 #endif /* ! __SH5__ */
diff -Naur gcc-4.2.1/gcc/config/sh/divcost-analysis gcc-4.2.1-stm/gcc/config/sh/divcost-analysis
--- gcc-4.2.1/gcc/config/sh/divcost-analysis	2006-03-23 21:39:32.000000000 +0000
+++ gcc-4.2.1-stm/gcc/config/sh/divcost-analysis	2007-08-10 13:08:59.000000000 +0100
@@ -38,12 +38,17 @@
 div_le128_neg -> div_by_1_neg: 4
 div_le128_neg -> rts          18
 
-                    absolute divisor range:
+         sh4-200    absolute divisor range:
             1  [2..128]  [129..64K) [64K..|divident|/256] >=64K,>|divident/256|
 udiv       18     22         38            32                   30
 sdiv pos:  20     24         41            35                   32
 sdiv neg:  15     25         42            36                   33
 
+         sh4-300    absolute divisor range:
+                 8 bit      16 bit       24 bit              > 24 bit
+udiv              15         35            28                   25
+sdiv              14         36            34                   31
+
 
 fp-based:
 
@@ -74,3 +79,4 @@
 mov.l @(r0,r1),r0
 jmp @r0
 ; 2 cycles worse than SFUNC_STATIC
+
diff -Naur gcc-4.2.1/gcc/config/sh/divtab-sh4-300.c gcc-4.2.1-stm/gcc/config/sh/divtab-sh4-300.c
--- gcc-4.2.1/gcc/config/sh/divtab-sh4-300.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/divtab-sh4-300.c	2007-08-10 13:08:59.000000000 +0100
@@ -0,0 +1,82 @@
+/* Copyright (C) 2004 Free Software Foundation, Inc.
+   Copyright (C) 2006 STMicroelectronics
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 59 Temple Place - Suite 330,
+Boston, MA 02111-1307, USA.  */
+
+/* Calculate division table for ST40-300 integer division
+   Contributed by Joern Rernnecke
+   joern.rennecke@st.com  */
+
+#include <stdio.h>
+#include <math.h>
+
+int
+main ()
+{
+  int i, j;
+  double q, r, err, max_err = 0, max_s_err = 0;
+
+  puts("/* This table has been generated by divtab-sh4.c.  */");
+  puts ("\t.balign 4");
+  for (i = -128; i < 128; i++)
+    {
+      int n = 0;
+      if (i == 0)
+	{
+	  /* output some dummy number for 1/0.  */
+	  puts ("LOCAL(div_table_clz):\n\t.byte\t0");
+	  continue;
+	}
+      for (j = i < 0 ? -i : i; j < 128; j += j)
+	n++;
+      printf ("\t.byte\t%d\n", n - 7);
+    }
+  puts("\
+/* 1/-128 .. 1/127, normalized.  There is an implicit leading 1 in bit 32,\n\
+   or in bit 33 for powers of two.  */\n\
+	.balign 4");
+  for (i = -128; i < 128; i++)
+    {
+      if (i == 0)
+	{
+	  puts ("LOCAL(div_table_inv):\n\t.long\t0x0");
+	  continue;
+	}
+      j = i < 0 ? -i : i;
+      while (j < 64)
+	j += j;
+      q = 4.*(1<<30)*128/j;
+      r = ceil (q);
+      printf ("\t.long\t0x%X\n", (unsigned) r);
+      err = r - q;
+      if (err > max_err)
+	max_err = err;
+      err = err * j / 128;
+      if (err > max_s_err)
+	max_s_err = err;
+    }
+  printf ("\t/* maximum error: %f scaled: %f*/\n", max_err, max_s_err);
+  exit (0);
+}
diff -Naur gcc-4.2.1/gcc/config/sh/elf.h gcc-4.2.1-stm/gcc/config/sh/elf.h
--- gcc-4.2.1/gcc/config/sh/elf.h	2005-06-25 02:22:41.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/elf.h	2007-08-10 13:08:59.000000000 +0100
@@ -80,7 +80,7 @@
 
 #undef STARTFILE_SPEC
 #define STARTFILE_SPEC \
-  "%{!shared: crt1.o%s} crti.o%s \
+  "%{!shared: crt1.o%s trap-handler.o%s} crti.o%s \
    %{!shared:crtbegin.o%s} %{shared:crtbeginS.o%s}"
 
 #undef ENDFILE_SPEC
diff -Naur gcc-4.2.1/gcc/config/sh/embed-elf.h gcc-4.2.1-stm/gcc/config/sh/embed-elf.h
--- gcc-4.2.1/gcc/config/sh/embed-elf.h	2006-04-12 21:09:41.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/embed-elf.h	2007-08-10 13:08:59.000000000 +0100
@@ -2,6 +2,7 @@
    non-Linux embedded targets.
    Copyright (C) 2002, 2003 Free Software Foundation, Inc.
    Contributed by J"orn Rennecke <joern.rennecke@superh.com>
+   Copyright (c) 2006  STMicroelectronics.
 
 This file is part of GCC.
 
@@ -32,6 +33,7 @@
 #define LIBGCC_SPEC "%{!shared: \
   %{m4-100*:-lic_invalidate_array_4-100} \
   %{m4-200*:-lic_invalidate_array_4-200} \
+  %{m4-300*|-m4-340:-lic_invalidate_array_4a %{!Os: -lgcc-4-300}} \
   %{m4a*:-lic_invalidate_array_4a}} \
   %{Os: -lgcc-Os-4-200} \
   -lgcc \
diff -Naur gcc-4.2.1/gcc/config/sh/IEEE-754/adddf3.S gcc-4.2.1-stm/gcc/config/sh/IEEE-754/adddf3.S
--- gcc-4.2.1/gcc/config/sh/IEEE-754/adddf3.S	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/IEEE-754/adddf3.S	2007-08-10 13:08:59.000000000 +0100
@@ -0,0 +1,799 @@
+/* Copyright (C) 2004, 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+! Routine for adding two double numbers
+
+! Author: Rakesh Kumar
+! SH1 Support by Joern Rennecke
+! Sticky Bit handling : Joern Rennecke
+
+! Arguments: r4-r5, r6-r7
+! Result: r0-r1
+
+! The value in r4-r5 is referred to as op1
+! and that in r6-r7 is referred to as op2
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+
+	.text
+        .align 5
+	.global	GLOBAL (subdf3)
+	FUNC (GLOBAL (subdf3))
+        .global GLOBAL (adddf3)
+	FUNC (GLOBAL (adddf3))
+
+GLOBAL (subdf3):
+#ifdef __LITTLE_ENDIAN__
+	mov	r4,r1
+	mov	r6,r2
+
+	mov	r5,r4
+	mov	r7,r6
+
+	mov	r1,r5
+	mov	r2,r7
+#endif
+	mov.l	.L_sign,r2
+	bra	.L_adddf3_1
+	xor	r2,r6
+
+GLOBAL (adddf3):
+#ifdef	__LITTLE_ENDIAN__
+	mov	r4,r1
+	mov	r6,r2
+
+	mov	r5,r4
+	mov	r7,r6
+
+	mov	r1,r5
+	mov	r2,r7
+#endif
+	
+.L_adddf3_1:
+	mov.l	r8,@-r15
+	mov	r4,r1
+
+	mov.l 	.L_inf,r2
+	mov	r6,r3
+
+	mov.l	r9,@-r15
+	and	r2,r1		!Exponent of op1 in r1
+
+	mov.l	r10,@-r15
+	and	r2,r3		!Exponent of op2 in r3
+
+	! Check for Nan or Infinity
+	mov.l	.L_sign,r9
+	cmp/eq	r2,r1
+
+	mov	r9,r10
+	bt	.L_thread_inv_exp_op1
+
+	mov	r9,r0
+	cmp/eq	r2,r3
+! op1 has a valid exponent. We need not check it again.
+! Return op2 straight away.
+	and	r4,r9		!r9 has sign bit for op1
+	bt	.L_ret_op2
+
+	! Check for -ve zero
+	cmp/eq	r4,r0
+	and	r6,r10		!r10 has sign bit for op2
+
+	bt	.L_op1_nzero
+
+	cmp/eq	r6,r0
+	bt	.L_op2_nzero
+
+! Check for zero
+.L_non_zero:
+	tst	r4,r4
+	bt	.L_op1_zero
+
+	! op1 is not zero, check op2 for zero
+	tst	r6,r6
+	bt	.L_op2_zero
+
+! r1 and r3 has masked out exponents, r9 and r10 has signs
+.L_add:
+	mov.l	.L_high_mant,r8
+	mov	#-20,r2
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+	shld	r2,r1		! r1 now has exponent for op1 in its lower bits
+#else
+	SHLR20 (r1)
+#endif
+	and	r8,r6	! Higher bits of mantissa of op2
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+	shld	r2,r3		! r3 has exponent for op2 in its lower bits
+#else
+	SHLR20 (r3)
+#endif
+	and	r8,r4	! Higher bits of mantissa of op1
+
+	mov.l	.L_21bit,r8
+
+	tst	r1,r1
+	bt	.L_norm_op1
+
+	! Set the 21st bit.
+	or	r8,r4
+	tst	r3,r3
+
+	bt	.L_norm_op2
+	or	r8,r6
+
+! Check for negative mantissas. Make them positive by negation
+! r9 and r10 have signs of op1 and op2 respectively
+.L_neg_mant:
+	tst	r9,r9
+	bf	.L_neg_op1
+
+	tst	r10,r10
+	bf	.L_neg_op2
+
+.L_add_1:
+	cmp/ge	r1,r3
+
+	mov	r1,r0
+	bt	.L_op2_exp_greater
+
+	sub	r3,r0
+	! If exponent difference is greater than 54, the resultant exponent
+	! won't be changed. Return op1 straight away.
+	mov	#54,r2
+	cmp/gt	r2,r0
+
+	bt	.L_pack_op1
+
+	mov	r1,r3
+	clrt
+
+	cmp/eq	#0,r0
+	bt	.L_add_mant
+
+	! Shift left the first operand and apply rest of shifts to second operand.
+	mov	#0,r2
+	shll	r5
+
+	rotcl	r4
+
+	add	#-1,r3
+	dt	r0
+
+	bt	.L_add_mant
+	dt	r0
+
+	bt	LOCAL(got_guard)
+	dt	r0
+
+	bt	LOCAL(got_sticky)
+
+! Shift the mantissa part of op2 so that both exponents are equal
+.L_shfrac_op2:
+	shar	r6
+	or	r7,r2	! sticky bit
+
+	rotcr	r7
+	dt	r0
+
+	bf	.L_shfrac_op2
+
+	shlr	r2
+
+	subc	r2,r2	! spread sticky bit across r2
+LOCAL(got_sticky):
+	shar	r6
+
+	rotcr	r7
+
+	rotcr	r2
+LOCAL(got_guard):
+	shar	r6
+
+	rotcr	r7
+
+	rotcr	r2
+
+
+! Add the psotive mantissas and check for overflow by checking the
+! MSB of the resultant. In case of overflow, negate the result.
+.L_add_mant:
+	clrt
+	addc	r7,r5
+
+	mov	#0,r10	! Assume resultant to be positive
+	addc	r6,r4
+
+	cmp/pz	r4
+
+	bt	.L_mant_ptv
+	negc	r2,r2
+
+	negc	r5,r5
+
+	mov.l	.L_sign,r10 ! The assumption was wrong, result is negative
+	negc	r4,r4
+
+! 23rd bit in the high part of mantissa could be set.
+! In this case, right shift the mantissa.
+.L_mant_ptv:
+	mov.l	.L_23bit,r0
+
+	tst	r4,r0
+	bt	.L_mant_ptv_0
+
+	shlr	r4
+	rotcr	r5
+
+	add	#1,r3
+	bra	.L_mant_ptv_1
+	rotcr	r2
+
+.L_mant_ptv_0:
+	mov.l	.L_22bit,r0
+	tst	r4,r0
+
+	bt	.L_norm_mant
+
+.L_mant_ptv_1:
+	! 22 bit of resultant mantissa is set. Shift right the mantissa
+	! and add 1 to exponent
+	add	#1,r3
+	shlr	r4
+	rotcr	r5
+	! The mantissa is already normalized. We don't need to
+	! spend any effort. Branch to epilogue. 
+	bra	.L_epil
+	rotcr	r2
+
+! Normalize operands
+.L_norm_op1:
+	shll	r5
+
+	rotcl	r4
+	add	#-1,r1
+
+	tst	r4,r8
+	bt	.L_norm_op1
+
+	tst	r3,r3
+	SL(bf,	.L_neg_mant,
+	 add	#1,r1)
+
+.L_norm_op2:
+	shll	r7
+
+	rotcl	r6
+	add	#-1,r3
+
+	tst	r6,r8
+	bt	.L_norm_op2
+
+	bra	.L_neg_mant
+	add	#1,r3
+
+! Negate the mantissa of op1
+.L_neg_op1:
+	clrt
+	negc	r5,r5
+
+	negc	r4,r4
+	tst	r10,r10
+
+	bt	.L_add_1
+
+! Negate the mantissa of op2
+.L_neg_op2:
+	clrt
+	negc	r7,r7
+
+	bra	.L_add_1
+	negc	r6,r6
+
+! Thread the jump to .L_inv_exp_op1
+.L_thread_inv_exp_op1:
+	bra	.L_inv_exp_op1
+	nop
+
+.L_ret_op2:
+	mov.l	@r15+,r10
+#ifdef	__LITTLE_ENDIAN__
+	mov	r6,r1
+#else
+	mov	r6,r0
+#endif
+
+	mov.l	@r15+,r9
+#ifdef	__LITTLE_ENDIAN__
+	mov	r7,r0
+#else
+	mov	r7,r1
+#endif
+
+	rts
+	mov.l	@r15+,r8
+
+.L_op1_nzero:
+	tst	r5,r5
+	bt	.L_ret_op2
+
+	! op1 is not zero. Check op2 for negative zero
+	cmp/eq	r6,r0
+	bf	.L_non_zero	! both op1 and op2 are not -0
+
+.L_op2_nzero:
+	tst	r7,r7
+	bf	.L_non_zero
+
+	mov.l	@r15+,r10
+#ifdef	__LITTLE_ENDIAN__
+	mov	r4,r1
+#else
+	mov	r4,r0	! op2 is -0, return op1
+#endif
+
+	mov.l	@r15+,r9
+#ifdef	__LITTLE_ENDIAN__
+	mov	r5,r0
+#else
+	mov	r5,r1
+#endif
+
+	rts
+	mov.l	@r15+,r8
+
+! High bit of op1 is known to be zero.
+! Check low bit. r2 contains 0x00000000
+.L_op1_zero:
+	tst	r5,r5
+	bt	.L_ret_op2
+
+	! op1 is not zero. Check high bit of op2
+	tst	r6,r6
+	bf	.L_add	! both op1 and op2 are not zero
+
+! op1 is not zero. High bit of op2 is known to be zero.
+! Check low bit of op2. r2 contains 0x00000000
+.L_op2_zero:
+	tst	r7,r7
+	bf	.L_add
+
+	mov.l	@r15+,r10
+#ifdef	__LITTLE_ENDIAN__
+	mov	r4,r1
+#else
+	mov	r4,r0	! op2 is zero, return op1
+#endif
+
+	mov.l	@r15+,r9
+#ifdef	__LITTLE_ENDIAN__
+	mov	r5,r0
+#else
+	mov	r5,r1
+#endif
+
+	rts
+	mov.l	@r15+,r8
+
+! exp (op1) is smaller or equal to exp (op2)
+! The logic of same operations is present in .L_add. Kindly refer it for
+! comments
+.L_op2_exp_greater:
+	mov	r3,r0
+	sub	r1,r0
+
+	mov	#54,r2
+	cmp/gt	r2,r0
+
+	bt	.L_pack_op2
+
+	cmp/eq	#0,r0
+	bt	.L_add_mant
+
+	mov	#0,r2
+	shll	r7
+	rotcl	r6
+	add	#-1,r0
+	add	#-1,r3
+
+	cmp/eq	#0,r0
+	bt	.L_add_mant
+.L_shfrac_op1:	
+        add     #-1,r0
+        shar    r4
+
+	rotcr	r5
+	rotcr	r2
+
+        cmp/eq  #0,r0
+        bf      .L_shfrac_op1
+
+	bra	.L_add_mant
+	nop
+
+! Return the value in op1
+.L_ret_op1:
+        mov.l   @r15+,r10
+#ifdef	__LITTLE_ENDIAN__
+	mov	r4,r1
+#else
+        mov     r4,r0
+#endif
+
+        mov.l   @r15+,r9
+#ifdef	__LITTLE_ENDIAN__
+	mov	r5,r0
+#else
+        mov     r5,r1
+#endif
+
+        rts
+        mov.l   @r15+,r8
+
+! r1 has exp, r9 has sign, r4 and r5 mantissa
+.L_pack_op1:
+	mov.l	.L_high_mant,r7
+	mov	r4,r0
+
+	tst	r9,r9
+	bt	.L_pack_op1_1
+
+	clrt
+	negc	r5,r5
+	negc	r0,r0
+
+.L_pack_op1_1:
+	and	r7,r0
+	mov	r1,r3
+
+	mov	#20,r2
+	mov	r5,r1
+
+	mov.l	@r15+,r10
+	or	r9,r0
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+	shld	r2,r3
+#else
+	SHLL20 (r3)
+#endif
+	mov.l	@r15+,r9
+
+	or	r3,r0
+#ifdef	__LITTLE_ENDIAN__
+	mov	r0,r2
+	mov	r1,r0
+	mov	r2,r1
+#endif
+	rts
+	mov.l	@r15+,r8
+
+!r2 has exp, r10 has sign, r6 and r7 mantissa
+.L_pack_op2:
+	mov.l	.L_high_mant,r9
+	mov	r6,r0
+
+	tst	r10,r10
+	bt	.L_pack_op2_1
+
+	clrt
+	negc	r7,r7
+	negc	r0,r0
+
+.L_pack_op2_1:
+	and	r9,r0
+	mov	r7,r1
+
+	mov	#20,r2
+	or	r10,r0
+
+	mov.l	@r15+,r10
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+	shld	r2,r3
+#else
+	SHLL20 (r3)
+#endif
+
+	mov.l	@r15+,r9
+
+	or	r3,r0
+#ifdef	__LITTLE_ENDIAN__
+	mov	r0,r2
+	mov	r1,r0
+	mov	r2,r1
+#endif
+	rts
+	mov.l	@r15+,r8
+
+! Normalize the mantissa by setting its 21 bit in high part
+.L_norm_mant:
+	mov.l	.L_21bit,r0
+
+	tst	r4,r0
+	bf	.L_epil
+
+	tst	r4,r4
+	bf	.L_shift_till_1
+
+	tst	r5,r5
+	bf	.L_shift_till_1
+
+	! Mantissa is zero, return 0
+	mov.l	@r15+,r10
+	mov	#0,r0
+
+	mov.l	@r15+,r9
+	mov.l	@r15+,r8
+
+	rts
+	mov	#0,r1
+
+! A loop for making the 21st bit 1 in high part of resultant mantissa
+! It is already ensured that 1 bit is present in the mantissa
+.L_shift_till_1:
+	clrt
+	shll	r5
+
+	rotcl	r4
+	add	#-1,r3
+
+	tst	r4,r0
+	bt	.L_shift_till_1
+
+! Return the result. Mantissa is in r4-r5. Exponent is in r3
+! Sign bit in r10
+.L_epil:
+	cmp/pl	r3
+
+	bf	.L_denorm
+	mov.l	LOCAL(x7fffffff),r0
+
+	mov	r5,r1
+	shlr	r1
+
+	mov	#0,r1
+	addc	r0,r2
+
+! Check extra MSB here
+	mov.l	.L_22bit,r9
+	addc	r1,r5	! round to even
+
+	addc	r1,r4
+	tst	r9,r4
+
+	bf	.L_epil_1
+
+.L_epil_0:
+	mov.l	.L_21bit,r1
+
+	not	r1,r1
+	and	r1,r4
+
+	mov	r4,r0
+	or	r10,r0
+
+	mov.l	@r15+,r10
+	mov	#20,r2
+
+	mov.l	@r15+,r9
+	mov	r5,r1
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+	shld	r2,r3
+#else
+	SHLL20 (r3)
+#endif
+	or	r3,r0
+
+#ifdef	__LITTLE_ENDIAN__
+	mov	r0,r2
+	mov	r1,r0
+	mov	r2,r1
+#endif
+	rts
+	mov.l	@r15+,r8
+
+.L_epil_1:
+	shlr	r4
+	add	#1,r3
+	bra	.L_epil_0
+	rotcr	r5
+
+.L_denorm:
+	add	#-1,r3
+.L_denorm_1:
+	tst	r3,r3
+	bt	.L_denorm_2
+
+	shlr	r4
+	rotcr	r5
+
+	movt	r1
+	bra	.L_denorm_1
+	add	#1,r3
+
+.L_denorm_2:
+	clrt
+	mov	#0,r2
+	addc	r1,r5
+
+	addc	r2,r4
+	mov	r4,r0
+
+	or	r10,r0
+	mov.l	@r15+,r10
+
+	mov	r5,r1
+
+	mov.l	@r15+,r9
+#ifdef	__LITTLE_ENDIAN__
+	mov	r0,r2
+	mov	r1,r0
+	mov	r2,r1
+#endif
+	rts
+	mov.l	@r15+,r8
+
+! op1 is known to be positive infinity, and op2 is Inf. The sign
+! of op2 is not known. Return the appropriate value
+.L_op1_pinf_op2_inf:
+	mov.l	.L_sign,r0
+	tst	r6,r0
+
+	bt	.L_ret_op2_1
+
+	! op2 is negative infinity. Inf - Inf is being performed
+	mov.l	.L_inf,r0
+	mov.l	@r15+,r10
+	mov.l	@r15+,r9
+#ifdef	__LITTLE_ENDIAN__
+	mov	r0,r1
+#endif
+	mov.l	@r15+,r8
+
+	rts
+#ifdef	__LITTLE_ENDIAN__
+	mov	#1,r0
+#else
+	mov	#1,r1	! Any value here will return Nan
+#endif
+	
+.L_ret_op1_1:
+        mov.l   @r15+,r10
+#ifdef	__LITTLE_ENDIAN__
+	mov	r4,r1
+#else
+        mov     r4,r0
+#endif
+
+        mov.l   @r15+,r9
+#ifdef	__LITTLE_ENDIAN__
+	mov	r5,r0
+#else
+        mov     r5,r1
+#endif
+
+        rts
+        mov.l   @r15+,r8
+
+.L_ret_op2_1:
+	mov.l	@r15+,r10
+#ifdef	__LITTLE_ENDIAN__
+	mov	r6,r1
+#else
+	mov	r6,r0
+#endif
+
+	mov.l	@r15+,r9
+#ifdef	__LITTLE_ENDIAN__
+	mov	r7,r0
+#else
+	mov	r7,r1
+#endif
+
+	rts
+	mov.l	@r15+,r8
+
+! op1 is negative infinity. Check op2 for infinity or Nan
+.L_op1_ninf:
+	cmp/eq	r2,r3
+	bf	.L_ret_op1_1	! op2 is neither Nan nor Inf
+
+	mov.l	@r15+,r9
+	div0s	r4,r6		! different signs -> NaN
+	mov	r4,DBLRH
+	or	r6,DBLRH
+	mov.l	@r15+,r8
+	SL(bf, 0f,
+	 mov	r5,DBLRL)
+	mov	#-1,DBLRH	! return NaN.
+0:	rts
+	or	r7,DBLRL
+
+!r1 contains exponent for op1, r3 contains exponent for op2
+!r2 has .L_inf (+ve Inf)
+!op1 has invalid exponent. Either it contains Nan or Inf
+.L_inv_exp_op1:
+	! Check if a is Nan
+	cmp/pl	r5
+	bt	.L_ret_op1_1
+
+	mov.l	.L_high_mant,r0
+	and	r4,r0
+
+	cmp/pl	r0
+	bt	.L_ret_op1_1
+
+	! op1 is not Nan. It is infinity. Check the sign of it.
+	! If op2 is Nan, return op2
+	cmp/pz	r4
+
+	bf	.L_op1_ninf
+
+	! op2 is +ve infinity here
+	cmp/eq	r2,r3
+	bf	.L_ret_op1_1	! op2 is neither Nan nor Inf
+
+	! r2 is free now
+	mov.l	.L_high_mant,r0
+	tst	r6,r0		! op2 also has invalid exponent
+
+	bf	.L_ret_op2_1	! op2 is Infinity, and op1 is +Infinity
+
+	tst	r7,r7
+	bt	.L_op1_pinf_op2_inf	! op2 is Infinity, and op1 is +Infinity
+	!op2 is not infinity, It is Nan
+	bf	.L_ret_op2_1
+
+	.align 2	
+.L_high_mant:
+	.long 0x000FFFFF
+
+.L_21bits:
+	.long 0x001FFFFF
+
+.L_22bit:
+	.long 0x00200000
+
+.L_23bit:
+	.long 0x00400000
+
+.L_21bit:
+	.long 0x00100000
+
+.L_sign:
+	.long 0x80000000
+
+.L_inf:
+	.long 0x7ff00000
+
+LOCAL(x7fffffff): .long 0x7fffffff
+
+ENDFUNC (GLOBAL (subdf3))
+ENDFUNC (GLOBAL (adddf3))
diff -Naur gcc-4.2.1/gcc/config/sh/IEEE-754/addsf3.S gcc-4.2.1-stm/gcc/config/sh/IEEE-754/addsf3.S
--- gcc-4.2.1/gcc/config/sh/IEEE-754/addsf3.S	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/IEEE-754/addsf3.S	2007-08-10 13:08:59.000000000 +0100
@@ -0,0 +1,535 @@
+/* Copyright (C) 2004, 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+! Add floating point numbers in r4, r5.
+
+! Author: Rakesh Kumar
+
+! Arguments are in r4, r5 and result in r0
+
+! Entry points: ___subsf3, ___addsf3
+
+! r4 and r5 are referred as op1 and op2
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+
+	.text
+	.align 5
+        .global GLOBAL (subsf3)
+	.global	GLOBAL (addsf3)
+	FUNC (GLOBAL (subsf3))
+	FUNC (GLOBAL (addsf3))
+
+GLOBAL (subsf3):
+        mov.l   .L_sign_bit,r1
+        xor     r1,r5
+
+GLOBAL (addsf3):
+	mov.l	r8,@-r15
+	mov	r4,r3
+
+	mov.l	.L_pinf,r2
+	mov	#0,r8
+
+	and	r2,r3 ! op1's exponent.
+	mov	r5,r6
+
+	! Check NaN or Infinity
+	and	r2,r6 ! op2's exponent.
+	cmp/eq	r2,r3
+
+	! go if op1 is NaN or INF. 
+	mov.l	.L_sign_bit,r0
+	SL(bt,	.L_inv_op1,
+	 mov	#-23,r1)
+	
+	! Go if op2 is NaN/INF.
+	cmp/eq	r2,r6
+	mov	r0,r7
+	bt	.L_ret_op2
+
+#if defined (__sh1__) || defined (__sh2__) || defined (__SH2E__)
+	SHLR23 (r3)
+#else
+	shld	r1,r3
+#endif
+#if defined (__sh1__) || defined (__sh2__) || defined (__SH2E__)
+	SHLR23 (r6)
+#else
+	shld	r1,r6
+#endif
+
+	! Check for negative zero
+	cmp/eq	r0,r5
+
+	mov	r5,r1
+	SL(bt,	.L_ret_op1,
+	 and	r7,r1)
+
+	cmp/eq	r0,r4
+	bt	.L_ret_op2
+
+	! if op1 is zero return op2
+	tst	r4,r4
+	bt	.L_ret_op2
+
+	! Equal numbers with opposite sign
+	mov	r4,r2
+	xor	r5,r2
+
+	cmp/eq	r0,r2
+	bt	.L_ret_zero
+
+	! if op2 is zero return op1
+	mov.l	.L_mask_fra,r2
+	tst	r5,r5
+
+	! Extract the mantissa
+	mov	r4,r0
+	SL(bt,	.L_ret_op1,
+	 and	r2,r5)
+
+	and	r2,r4
+
+	mov.l	.L_imp_bit,r2
+	and	r7,r0	! sign bit of op1
+
+	! Check for denormals
+	tst	r3,r3
+	bt	.L_norm_op1
+
+	! Attach the implicit bit
+	or	r2,r4
+	tst	r6,r6
+
+	bt	.L_norm_op2
+
+	or	r2,r5
+	tst	r0,r0
+
+	! operands are +ve or -ve??
+	bt	.L_ptv_op1
+
+	neg	r4,r4
+
+.L_ptv_op1:
+	tst	r1,r1
+	bt	.L_ptv_op2
+
+	neg	r5,r5
+
+! Test exponents for equality
+.L_ptv_op2:
+	cmp/eq	r3,r6
+	bt	.L_exp_eq
+
+! Make exponents of two arguments equal
+.L_exp_ne:
+	! r0, r1 contain sign bits.
+	! r4, r5 contain mantissas.
+	! r3, r6 contain exponents.
+	! r2, r7 scratch.
+
+	! Calculate result exponent.
+	mov	r6,r2
+	sub	r3,r2	! e2 - e1
+
+	cmp/pl	r2
+	mov	#23,r7
+
+	! e2 - e1 is -ve
+	bf	.L_exp_ne_1
+
+	mov	r6,r3 ! Result exp.
+	cmp/gt	r7,r2 ! e2-e1 > 23
+
+	mov	#1,r7
+	bt	.L_pack_op2_0
+
+	! Align the mantissa
+.L_loop_ne:
+	shar	r4
+
+	rotcr	r8
+	cmp/eq	r7,r2
+
+	add	#-1,r2
+	bf	.L_loop_ne
+
+	bt	.L_exp_eq
+
+! Exponent difference is too high.
+! Return op2 after placing pieces in proper place
+.L_pack_op2_0:
+	! If op1 is -ve
+	tst	r1,r1
+	bt	.L_pack_op2
+
+	neg	r5,r5
+
+! r6 has exponent
+! r5 has mantissa, r1 has sign
+.L_pack_op2:
+	mov.l	.L_nimp_bit,r2
+	mov	#23,r3
+
+	mov	r1,r0
+	
+	and	r2,r5
+	mov.l	@r15+,r8
+
+	or	r5,r0
+
+#if defined (__sh1__) || defined (__sh2__) || defined (__SH2E__)
+	SHLL23 (r6)
+#else
+	shld	r3,r6
+#endif
+        rts
+	or	r6,r0
+
+! return op1. It is NAN or INF or op2 is zero.
+.L_ret_op1:
+	mov	r4,r0
+
+	rts
+	mov.l	@r15+,r8
+
+! return zero
+.L_ret_zero:
+	mov	#0,r0
+
+	rts
+	mov.l	@r15+,r8
+
+! return op2. It is NaN or INF or op1 is zero.
+.L_ret_op2:
+	mov	r5,r0
+
+	rts
+	mov.l	@r15+,r8
+
+! op2 is denormal. Normalize it.
+.L_norm_op2:
+	shll	r5
+	add	#-1,r6
+
+	tst	r2,r5
+	bt	.L_norm_op2
+
+	! Check sign
+	tst	r1,r1
+	bt	.L_norm_op2_2
+
+	neg	r5,r5
+
+.L_norm_op2_2:
+	add	#1,r6
+	cmp/eq	r3,r6
+
+	bf	.L_exp_ne
+	bt	.L_exp_eq
+
+! Normalize op1
+.L_norm_op1:
+	shll	r4
+	add	#-1,r3
+
+	tst	r2,r4
+	bt	.L_norm_op1
+
+	! Check sign
+	tst	r0,r0
+	bt	.L_norm_op1_1
+
+	neg	r4,r4
+
+.L_norm_op1_1:
+	! Adjust biasing
+	add	#1,r3
+
+	! Check op2 for denormalized value
+	tst	r6,r6
+	bt	.L_norm_op2
+
+	mov.l	.L_imp_bit,r2
+
+	tst	r1,r1	! Check sign
+	or	r2,r5	! Attach 24th bit
+
+	bt	.L_norm_op1_2
+
+	neg	r5,r5
+
+.L_norm_op1_2:
+	cmp/eq	r3,r6
+
+	bt	.L_exp_eq
+	bf	.L_exp_ne
+
+! op1 is NaN or Inf
+.L_inv_op1:
+	! Return op1 if it is NAN. 
+	! r2 is infinity
+	cmp/gt	r2,r4
+	bt	.L_ret_op1
+
+	! op1 is +/- INF
+	! If op2 is same return now.
+	cmp/eq	r4,r5
+	bt	.L_ret_op1
+
+	! return op2 if it is NAN
+	cmp/gt	r2,r5
+	bt	.L_ret_op2
+
+	! Check if op2 is inf
+	cmp/eq	r2,r6
+	bf	.L_ret_op1
+	
+	! Both op1 and op2 are infinities 
+	!of opp signs, or there is -NAN. Return a NAN.
+	mov.l	@r15+,r8
+	rts
+	mov	#-1,r0
+
+! Make unequal exponents equal.
+.L_exp_ne_1:
+	mov	#-25,r7
+	cmp/gt	r2,r7 ! -23 > e2 - e1
+
+	add	#1,r2
+	bf	.L_exp_ne_2
+
+	tst	r0,r0
+	bt	.L_pack_op1
+
+.L_pack_op1_0:
+	bra	.L_pack_op1
+	neg	r4,r4
+
+! Accumulate the shifted bits in r8
+.L_exp_ne_2:
+	! Shift with rounding
+	shar	r5
+	rotcr	r8
+
+	tst	r2,r2
+
+	add	#1,r2
+	bf	.L_exp_ne_2
+
+! Exponents of op1 and op2 are equal (or made so)
+! The mantissas are in r4-r5 and remaining bits in r8
+.L_exp_eq:
+	add	r5,r4 ! Add fractions.
+	mov.l	.L_sign_bit,r2
+
+	! Check for negative result
+	mov	#0,r0
+	tst	r2,r4
+
+	mov.l	.L_255,r5
+	bt	.L_post_add
+
+	negc	r8,r8
+	negc	r4,r4
+	or	r2,r0
+
+.L_post_add:
+	! Check for extra MSB
+	mov.l	.L_chk_25,r2
+
+	tst	r2,r4
+	bt	.L_imp_check
+
+	shar 	r4
+	rotcr	r8
+
+	add	#1,r3
+	cmp/ge	r5,r3
+
+	! Return Inf if exp > 254
+	bt	.L_ret_inf
+
+! Check for implicit (24th) bit in result
+.L_imp_check:
+        mov.l	.L_imp_bit,r2
+	tst	r2,r4
+
+	bf	.L_pack_op1
+
+! Result needs left shift
+.L_lft_shft:
+	shll	r8
+	rotcl	r4
+
+	add	#-1,r3
+	tst	r2,r4
+
+	bt	.L_lft_shft
+	
+! Pack the result after rounding
+.L_pack_op1:
+	! See if denormalized result is possible 
+	mov.l	.L_chk_25,r5
+	cmp/pl	r3
+
+	bf	.L_denorm_res
+
+	! Are there any bits shifted previously?
+	tst	r8,r8
+	bt	.L_pack_1
+
+	! Round
+	shll	r8
+	movt	r6
+
+	add	r6,r4
+
+	! If we are halfway between two numbers,
+	! round towards LSB = 0
+	tst	r8,r8
+
+	bf	.L_pack_1
+
+	shlr	r4
+	shll	r4
+
+.L_pack_1:
+	! Adjust extra MSB generated after rounding
+	tst	r4,r5
+	mov.l	.L_255,r2
+
+	bt	.L_pack_2
+	shar	r4
+
+	add	#1,r3 
+	cmp/ge	r2,r3	! Check for exp overflow
+
+	bt	.L_ret_inf
+	
+! Pack it finally
+.L_pack_2:
+	! Do not store implicit bit
+	mov.l	.L_nimp_bit,r2
+	mov	#23,r1
+
+	and	r2,r4
+
+#if defined (__sh1__) || defined (__sh2__) || defined (__SH2E__)
+	SHLL23 (r3)
+#else
+	shld	r1,r3
+#endif
+	mov.l	@r15+,r8
+
+	or	r4,r0
+        rts
+	or	r3,r0
+
+! Return infinity
+.L_ret_inf:
+	mov.l	.L_pinf,r2
+
+	mov.l	@r15+,r8
+	rts
+	or	r2,r0
+
+! Result must be denormalized
+.L_denorm_res:
+	mov	#0,r2
+	
+! Denormalizing loop with rounding
+.L_den_1:
+	shar	r4
+	movt	r6
+
+	tst	r3,r3
+	bt	.L_den_2
+
+	! Increment the exponent
+	add	#1,r3
+
+	tst	r6,r6
+	bt	.L_den_0
+
+	! Count number of ON bits shifted
+	add	#1,r2
+
+.L_den_0:
+	bra	.L_den_1
+	nop
+
+! Apply rounding
+.L_den_2:
+	cmp/eq	r6,r1
+	bf	.L_den_3
+
+	add	r6,r4
+	mov	#1,r1
+
+	! If halfway between two numbers,
+	! round towards LSB = 0
+	cmp/eq	r2,r1
+	bf	.L_den_3
+
+	shar	r4
+	shll	r4
+
+.L_den_3:
+
+	mov.l	@r15+,r8
+	rts
+	or	r4,r0
+	
+	.align 2
+.L_imp_bit:
+        .long   0x00800000
+
+.L_nimp_bit:
+	.long	0xFF7FFFFF
+
+.L_mask_fra:
+        .long   0x007FFFFF
+
+.L_pinf:
+        .long   0x7F800000
+
+.L_sign_bit:
+	.long	0x80000000
+
+.L_bit_25:
+	.long	0x01000000
+
+.L_chk_25:
+        .long   0x7F000000
+
+.L_255:
+	.long	0x000000FF
+
+ENDFUNC (GLOBAL (addsf3))
+ENDFUNC (GLOBAL (subsf3))
diff -Naur gcc-4.2.1/gcc/config/sh/IEEE-754/divdf3.S gcc-4.2.1-stm/gcc/config/sh/IEEE-754/divdf3.S
--- gcc-4.2.1/gcc/config/sh/IEEE-754/divdf3.S	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/IEEE-754/divdf3.S	2007-08-10 13:08:59.000000000 +0100
@@ -0,0 +1,598 @@
+/* Copyright (C) 2004, 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+!division of two double precision floating point numbers
+!Author:Aanchal Khanna
+!
+!Entry:
+!r4,r5:dividend
+!
+!r6,r7:divisor
+!
+!Exit:
+!r0,r1:quotient
+
+!Notes: dividend is passed in regs r4 and r5 and divisor is passed in regs 
+!r6 and r7, quotient is returned in regs r0 and r1. dividend is referred as op1
+!and divisor as op2.
+!
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+
+	.text
+	.align	5
+	.global	GLOBAL (divdf3)
+	FUNC (GLOBAL (divdf3))
+
+GLOBAL (divdf3):
+
+#ifdef  __LITTLE_ENDIAN__
+	mov	r4,r1
+	mov	r5,r4
+	mov	r1,r5
+
+        mov     r6,r1
+        mov     r7,r6
+        mov     r1,r7
+#endif
+	mov	r4,r2
+	mov.l	.L_inf,r1
+
+	and	r1,r2
+	mov.l   r8,@-r15
+
+	cmp/eq	r1,r2
+	mov     r6,r8
+
+	bt	.L_a_inv
+	and	r1,r8
+
+	cmp/eq	r1,r8
+	mov.l	.L_high_mant,r3
+
+	bf	.L_chk_zero
+	and	r6,r3
+
+	mov.l   .L_mask_sign,r8	
+	cmp/pl	r7
+
+	mov	r8,r0
+	bt	.L_ret_b	!op2=NaN,return op2
+
+	and	r4,r8
+	cmp/pl	r3
+
+	and	r6,r0
+	bt	.L_ret_b	!op2=NaN,return op2
+
+	xor     r8,r0           !op1=normal no,op2=Inf, return Zero
+	mov     #0,r1
+	
+#ifdef __LITTLE_ENDIAN__
+	mov	r0,r2
+	mov	r1,r0
+	mov	r2,r1
+#endif
+	rts
+	mov.l   @r15+,r8
+
+.L_ret_b:
+	mov	r7,r1
+	mov     r6,r0
+
+#ifdef __LITTLE_ENDIAN__
+        mov     r0,r2
+        mov     r1,r0
+        mov     r2,r1
+#endif
+
+	rts
+	mov.l   @r15+,r8
+
+.L_a_inv:
+	!chk if op1 is Inf or NaN
+	mov.l   .L_high_mant,r2
+	cmp/pl  r5
+
+	and	r4,r2
+	bt	.L_ret_a
+
+	and	r1,r8		!r1 contains infinity
+	cmp/pl	r2
+
+	bt	.L_ret_a
+	cmp/eq	r1,r8
+
+	mov	r1,DBLRH
+	add	DBLRH,DBLRH
+	bf	0f
+	mov	#-1,DBLRH	! Inf/Inf, return NaN.
+0:	div0s	r4,r6
+	mov.l   @r15+,r8	
+	rts
+	rotcr	DBLRH
+
+.L_ret_a:
+	!return op1
+	mov	r5,r1
+	mov	r4,r0
+
+#ifdef __LITTLE_ENDIAN__
+        mov     r0,r2
+        mov     r1,r0
+        mov     r2,r1
+#endif
+	rts
+        mov.l   @r15+,r8
+
+.L_chk_zero:
+	!chk if op1=0
+	mov.l   .L_mask_sign,r0
+        mov     r4,r3
+
+        and     r0,r3
+        shll    r4
+
+        and     r6,r0
+        shlr    r4
+
+        xor     r3,r0
+        shll    r6
+
+	shlr	r6
+	tst	r4,r4
+
+
+	bf      .L_op1_not_zero	
+	tst	r5,r5
+	
+        bf      .L_op1_not_zero
+	tst	r7,r7
+
+	mov.l   @r15+,r8
+	bf	.L_ret_zero
+
+	tst	r6,r6
+	bf	.L_ret_zero
+
+	rts
+	mov     #-1,DBLRH       !op1=op2=0, return NaN
+	
+.L_ret_zero:
+	!return zero
+	mov	r0,r1
+	rts
+#ifdef __LITTLE__ENDIAN
+	mov	#0,r0
+#else
+	mov	#0,r1		!op1=0,op2=normal no,return zero
+#endif
+
+.L_norm_b:
+	!normalize op2
+        shll    r7
+        mov.l   .L_imp_bit,r3
+
+        rotcl   r6
+        tst     r3,r6
+
+        add     #-1,r8
+        bt      .L_norm_b
+
+        bra     .L_divide
+        add     #1,r8
+
+.L_op1_not_zero:
+	!op1!=0, chk if op2=0
+	tst	r7,r7	
+	mov	r1,r3
+	
+	mov	#0,r1
+	bf	.L_normal_nos
+
+	tst	r6,r6
+	bf      .L_normal_nos
+
+	mov.l   @r15+,r8
+	or	r3,r0
+
+#ifdef __LITTLE_ENDIAN__
+        mov     r0,r2
+        mov     r1,r0
+        mov     r2,r1
+#endif
+
+	rts
+	nop
+
+.L_normal_nos:
+	!op1 and op2 are normal nos
+	tst	r2,r2
+	mov	#-20,r1
+
+! The subsequent branch is for the upper compare
+! Shifting will not alter the result, for the
+! macro is declared with care.
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+	shld    r1,r2
+#else
+	SHLR20 (r2)
+#endif
+	bt	.L_norm_a	!normalize dividend
+	
+.L_chk_b:
+	mov.l	r9,@-r15
+	tst	r8,r8
+
+        mov.l   .L_high_mant,r9
+
+! The subsequent branch is for the upper compare
+! Shifting will not alter the result, for the
+! macro is declared with care.
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r1,r8
+#else
+        SHLR20 (r8)
+#endif
+				! T set -> normalize divisor
+	SL(bt,	.L_norm_b,
+	 and	r9,r4)
+
+.L_divide:
+	mov.l   .L_2047,r1
+	sub	r8,r2
+
+	mov.l	.L_1023,r8
+	and	r9,r6
+
+	!resultant exponent
+	add	r8,r2
+	!chk the exponent for overflow
+	cmp/ge	r1,r2
+	
+	mov.l	.L_imp_bit,r1
+	bt	.L_overflow
+	
+	mov	#0,r8
+	or	r1,r4
+	
+	or      r1,r6	
+	mov	#-24,r3
+
+	!chk if the divisor is 1(mantissa only)
+	cmp/eq	r8,r7
+	bf	.L_div2
+
+	cmp/eq	 r6,r1
+	bt	.L_den_one
+
+.L_div2:
+	!divide the mantissas
+	shll8	r4
+	mov	r5,r9
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r3,r9
+#else
+        SHLR24 (r9)
+#endif
+	shll8	r6
+
+	or	r9,r4
+	shll8   r5
+
+	mov	r7,r9
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r3,r9
+#else
+        SHLR24 (r9)
+#endif
+	mov	r8,r3
+	shll8	r7
+
+	or	r9,r6	
+	cmp/gt	r4,r6
+
+	mov	r3,r9
+	bt	.L_shift
+
+	cmp/eq	r4,r6
+	bf	.L_loop
+
+	cmp/gt	r5,r7
+	bf	.L_loop
+
+.L_shift:
+	add	#-1,r2
+	shll	r5
+	rotcl	r4
+
+.L_loop:
+	!actual division loop
+	cmp/gt	r6,r4
+	bt	.L_subtract
+
+	cmp/eq	r6,r4
+	bf	.L_skip
+
+	cmp/ge	r7,r5
+	bf	.L_skip
+
+.L_subtract:
+	clrt
+	subc	r7,r5
+	
+	or	r1,r8
+	subc	r6,r4
+
+.L_skip:
+	shlr	r1
+	shll	r5
+
+	rotcl	r4
+	cmp/eq	r1,r3
+
+	bf	.L_loop
+	mov.l	.L_imp_bit,r1
+
+	!chk if the divison was for the higher word of the quotient
+	tst	r1,r9
+	bf	.L_chk_exp
+
+	mov	r8,r9
+	mov.l   .L_mask_sign,r1
+
+	!divide for the lower word of the quotient
+	bra	.L_loop
+	mov	r3,r8
+
+.L_chk_exp:
+	!chk if the result needs to be denormalized
+	cmp/gt	r2,r3
+	bf	.L_round
+	mov     #-53,r7
+
+.L_underflow:
+	!denormalize the result
+	add	#1,r2
+	cmp/gt	r2,r7
+
+	or      r4,r5           !remainder
+	add	#-2,r2
+
+	mov	#32,r4
+	bt      .L_return_zero
+
+	add	r2,r4
+	cmp/ge	r3,r4
+
+	mov	r2,r7
+	mov	r3,r1
+
+	mov     #-54,r2
+	bt	.L_denorm
+	mov	#-32,r7
+
+.L_denorm:
+	shlr	r8
+	rotcr	r1
+
+	shll	r8
+	add     #1,r7
+
+	shlr	r9
+	rotcr	r8
+
+	cmp/eq	r3,r7
+	bf	.L_denorm
+
+	mov	r4,r7
+	cmp/eq	r2,r4
+
+	bt	.L_break
+	mov     r3,r6
+
+	cmp/gt	r7,r3
+	bf	.L_break
+
+	mov	r2,r4
+	mov	r1,r6
+
+	mov	r3,r1
+	bt	.L_denorm
+
+.L_break:
+	mov     #0,r2
+
+	cmp/gt	r1,r2
+
+	addc	r2,r8
+	mov.l   .L_comp_1,r4
+
+	addc	r3,r9		
+	or	r9,r0
+
+	cmp/eq	r5,r3
+	bf	.L_return	
+
+	cmp/eq	r3,r6
+	mov.l	.L_mask_sign,r7
+
+	bf	.L_return
+	cmp/eq	r7,r1
+
+	bf	.L_return
+	and	r4,r8
+
+.L_return:
+	mov.l	@r15+,r9
+	mov     r8,r1
+
+#ifdef __LITTLE_ENDIAN__
+        mov     r0,r2
+        mov     r1,r0
+        mov     r2,r1
+#endif
+	rts
+	mov.l   @r15+,r8
+
+.L_norm_a:
+        !normalize op1
+        shll    r5
+        mov.l   .L_imp_bit,r3
+
+        rotcl   r4
+        tst     r3,r4
+
+        add     #-1,r2
+        bt      .L_norm_a
+
+        bra     .L_chk_b
+        add     #1,r2
+
+.L_overflow:
+	!overflow, return inf
+	mov.l   .L_inf,r2
+#ifdef __LITTLE_ENDIAN__
+	or	r2,r1	
+	mov	#0,r0
+#else
+	or	r2,r0
+	mov	#0,r1
+#endif
+        mov.l   @r15+,r9
+        rts
+        mov.l   @r15+,r8
+
+.L_den_one:
+	!denominator=1, result=numerator
+        mov     r4,r9
+        mov   	#-53,r7
+
+	cmp/ge	r2,r8
+	mov	r8,r4
+
+	mov	r5,r8
+	mov	r4,r3
+
+	!chk the exponent for underflow
+	SL(bt,	.L_underflow,
+	 mov     r4,r5)
+
+	mov.l	.L_high_mant,r7
+        bra     .L_pack
+	mov     #20,r6
+
+.L_return_zero:
+	!return zero
+	mov	r3,r1
+	mov.l	@r15+,r9
+
+	rts
+	mov.l   @r15+,r8
+
+.L_round:
+	!apply rounding
+	cmp/eq	r4,r6
+	bt	.L_lower
+
+	clrt
+	subc    r6,r4
+
+	bra     .L_rounding
+	mov	r4,r6
+	
+.L_lower:
+	clrt
+	subc	r7,r5
+	mov	r5,r6
+	
+.L_rounding:
+	!apply rounding
+	mov.l   .L_invert,r1
+	mov	r3,r4
+
+	movt	r3
+	clrt
+	
+	not	r3,r3
+	and	r1,r3	
+
+	addc	r3,r8
+	mov.l   .L_high_mant,r7
+
+	addc	r4,r9
+	cmp/eq	r4,r6
+
+	mov.l   .L_comp_1,r3
+	SL (bf,	.L_pack,
+	 mov     #20,r6)
+	and	r3,r8
+
+.L_pack:
+	!pack the result, r2=exponent,r0=sign,r8=lower mantissa, r9=higher mantissa
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+	shld    r6,r2
+#else
+        SHLL20 (r2)
+#endif
+	and	r7,r9
+
+	or	r2,r0
+	mov	r8,r1
+
+	or      r9,r0
+	mov.l	@r15+,r9
+
+#ifdef __LITTLE_ENDIAN__
+        mov     r0,r2
+        mov     r1,r0
+        mov     r2,r1
+#endif
+	rts
+	mov.l	@r15+,r8
+
+	.align	2
+
+.L_mask_sign:
+	.long	0x80000000
+.L_high_mant:
+	.long	0x000fffff
+.L_inf:
+	.long	0x7ff00000
+.L_1023:
+	.long	1023
+.L_2047:
+	.long	2047
+.L_imp_bit:
+	.long	0x00100000	
+.L_comp_1:
+	.long	0xfffffffe
+.L_invert:
+	.long	0x00000001
+
+ENDFUNC (GLOBAL (divdf3))
diff -Naur gcc-4.2.1/gcc/config/sh/IEEE-754/divsf3.S gcc-4.2.1-stm/gcc/config/sh/IEEE-754/divsf3.S
--- gcc-4.2.1/gcc/config/sh/IEEE-754/divsf3.S	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/IEEE-754/divsf3.S	2007-08-10 13:08:59.000000000 +0100
@@ -0,0 +1,404 @@
+/* Copyright (C) 2004, 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+!divides two single precision floating point 
+
+! Author: Aanchal Khanna
+
+! Arguments: Dividend is in r4, divisor in r5
+! Result: r0
+
+! r4 and r5 are referred as op1 and op2 resp.
+
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+
+	.text
+	.align	5
+	.global	GLOBAL (divsf3)
+	FUNC (GLOBAL (divsf3))
+
+GLOBAL (divsf3):
+	mov.l	.L_mask_sign,r1
+	mov	r4,r3
+
+	xor	r5,r3
+	shll	r4
+
+	shlr	r4
+	mov.l	.L_inf,r2
+
+	and	r3,r1		!r1=resultant sign
+	mov	r4,r6
+
+	shll	r5
+	mov	#0,r0		
+
+	shlr	r5
+	and	r2,r6
+
+	cmp/eq	r2,r6
+	mov	r5,r7
+
+	and     r2,r7
+	bt	.L_op1_inv
+
+	cmp/eq	r2,r7
+	mov	#-23,r3
+
+	bt	.L_op2_inv
+#if defined (__sh1__) || defined (__sh2__) || defined (__SH2E__)
+	SHLR23 (r6)
+	SHLR23 (r7)
+#else
+	shld	r3,r6
+	shld	r3,r7
+#endif
+
+	cmp/eq	r0,r4
+
+	bt	.L_op1_zero		!dividend=0
+	cmp/eq	r0,r6
+
+	mov.l   .L_imp_bit,r3
+	bt	.L_norm_op1		!normalize dividend
+.L_chk_op2:
+	cmp/eq	r0,r5
+	bt	.L_op2_zero		!divisor=0
+
+	cmp/eq	r0,r7
+	bt	.L_norm_op2		!normalize divisor
+
+.L_div1:
+	sub	r7,r6
+	add	#127,r6			!r6=resultant exponent
+
+	mov     r3,r7
+	mov.l	.L_mask_mant,r3
+
+	and	r3,r4
+	!chk exponent for overflow
+        mov.l   .L_255,r2
+
+	and     r3,r5
+	or	r7,r4
+
+	cmp/ge  r2,r6
+	or	r7,r5
+
+	bt	.L_return_inf
+	mov	r0,r2
+
+	cmp/eq  r4,r5
+	bf      .L_den_one
+
+	cmp/ge	r6,r0
+	!numerator=denominator, quotient=1, remainder=0
+	mov	r7,r2			
+
+	mov     r0,r4
+	!chk exponent for underflow
+	bt	.L_underflow
+        bra     .L_pack
+        nop
+
+.L_den_one:
+	!denominator=1, result=numerator
+
+	cmp/eq  r7,r5
+        bf      .L_divide
+
+	!chk exponent for underflow
+	cmp/ge  r6,r0
+        mov    r4,r2           
+
+        SL(bt,    .L_underflow,
+	 mov	r0,r4)
+	bra     .L_pack
+	nop
+
+.L_divide:
+	!dividing the mantissas r4<-dividend, r5<-divisor
+
+	cmp/hi	r4,r5
+	bf	.L_loop
+
+	shll	r4		! if mantissa(op1)< mantissa(op2)
+	add     #-1,r6		! shift left the numerator and decrease the exponent.
+
+.L_loop:
+	!division loop
+
+	cmp/ge	r5,r4
+	bf	.L_skip
+
+	or	r7,r2
+	sub	r5,r4
+
+.L_skip:
+	shlr	r7
+	shll	r4
+
+	cmp/eq	r0,r7
+	bf	.L_loop
+
+	!chk the exponent for underflow
+	cmp/ge  r6,r0
+	bt      .L_underflow
+	
+	!apply rounding
+	cmp/gt	r5,r4
+	bt	.L_round1
+
+	cmp/eq	r4,r5
+	bt	.L_round2
+
+.L_pack:
+	!pack the result, r1=sign, r2=quotient, r6=exponent
+
+	mov    #23,r4
+	and     r3,r2
+
+#if defined (__sh1__) || defined (__sh2__) || defined (__SH2E__)
+	SHLL23 (r6)
+#else
+	shld	r4,r6
+#endif
+	or	r2,r1
+
+	or	r6,r1
+	mov	r1,r0	
+	
+	rts
+	nop
+
+.L_round1:
+	!Apply proper rounding
+
+        bra     .L_pack
+        add     #1,r2
+
+.L_round2:
+	!Apply proper rounding
+
+        mov.l   .L_comp_1,r5
+        bra     .L_pack
+        and     r5,r2
+
+.L_op1_inv:
+	!chk if op1 is Inf or NaN
+
+	mov.l	.L_mask_mant,r3
+	mov	r4,r6
+
+	and	r3,r6
+	cmp/hi	r0,r6
+
+	bt	.L_ret_op1
+	cmp/eq	r2,r7
+
+	SL(bf,	.L_ret_op1,
+	 mov	r1,r0)
+
+	rts
+	mov	#-1,r0	! 0/0, return NaN
+	
+.L_op2_inv:
+	!chk if op2 is Inf or NaN
+
+	mov.l	.L_mask_mant,r3
+	mov	r5,r7
+	
+	and	r3,r7
+	cmp/hi	r0,r7
+
+	bt	.L_ret_op2
+	mov	r1,r0
+	
+	rts
+	nop
+
+.L_op1_zero:
+	!op1 is zero. If op2 is zero, return NaN, else return zero
+
+	cmp/eq	r0,r5
+
+	bf	.L_ret_op1	
+
+	rts
+	mov	#-1,r0
+
+.L_op2_zero:
+	!B is zero,return Inf
+
+	rts
+	or	r2,r0
+
+.L_return_inf:
+	mov.l	.L_inf,r0
+	
+	rts
+	or	r1,r0
+
+.L_norm_op1:
+	!normalize dividend
+
+	shll	r4
+	tst	r2,r4
+	
+	add     #-1,r6
+	bt	.L_norm_op1
+
+	bra	.L_chk_op2
+	add	#1,r6
+
+.L_norm_op2:
+	!normalize divisor
+
+	shll	r5
+	tst	r2,r5
+	
+	add	#-1,r7
+	bt	.L_norm_op2
+
+	bra	.L_div1
+	add	#1,r7
+
+.L_underflow:
+	!denormalize the result
+
+	add	#1,r6
+	mov	#-24,r7
+
+	cmp/gt	r6,r7
+	mov	r2,r5
+
+	bt	.L_return_zero
+	add     #-1,r6
+
+	mov	#32,r3
+	neg	r6,r7
+
+	add	#1,r7
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+	shld	r6,r2
+#else
+	cmp/ge	r0,r6
+	bf	.L_mov_right
+
+.L_mov_left:
+	cmp/eq	r0,r6
+	bt	.L_out
+
+	shll	r2
+	bra	.L_mov_left
+	add	#-1,r6
+
+.L_mov_right:
+	cmp/eq	r0,r6
+	bt	.L_out
+
+	add	#1,r6
+	bra	.L_mov_right
+	shlr	r2
+	
+.L_out:
+#endif
+	sub	r7,r3
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+	shld	r3,r5
+#else
+	cmp/ge	r0,r3
+	bf	.L_mov_right_1
+
+.L_mov_left_1:
+	shll	r5
+	add	#-1,r3
+
+	cmp/eq	r0,r3
+	bf	.L_mov_left_1
+
+	bt	.L_out_1
+
+.L_mov_right_1:
+	cmp/eq	r0,r3
+	bt	.L_out_1
+
+	add	#1,r3
+	bra	.L_mov_right_1
+	shlr	r5
+
+.L_out_1:
+#endif
+	shlr	r2
+	addc	r0,r2
+
+	cmp/eq	r4,r0		!r4 contains the remainder
+	mov      r2,r0
+
+	mov.l	.L_mask_sign,r7
+	bf	.L_return
+
+	mov.l   .L_comp_1,r2
+	cmp/eq	r7,r5
+
+	bf	.L_return
+	and	r2,r0
+
+.L_return:
+	rts
+	or     r1,r0
+	
+.L_ret_op1:
+	rts
+	or	r4,r0
+
+.L_ret_op2:
+	rts
+	or	r5,r0
+
+.L_return_zero:
+	rts
+	or	r1,r0
+
+
+
+	.align	2
+.L_inf:
+	.long	0x7f800000
+.L_mask_sign:
+	.long	0x80000000
+.L_mask_mant:
+	.long	0x007fffff
+.L_imp_bit:
+	.long	0x00800000
+.L_comp_1:
+	.long	0xfffffffe
+.L_255:
+	.long	255
+
+ENDFUNC (GLOBAL (divsf3))
diff -Naur gcc-4.2.1/gcc/config/sh/IEEE-754/fixdfsi.S gcc-4.2.1-stm/gcc/config/sh/IEEE-754/fixdfsi.S
--- gcc-4.2.1/gcc/config/sh/IEEE-754/fixdfsi.S	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/IEEE-754/fixdfsi.S	2007-08-10 13:08:59.000000000 +0100
@@ -0,0 +1,200 @@
+/* Copyright (C) 2004 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+!conversion of double precision floating point number to signed integer
+!Author:Aanchal Khanna
+!
+!Entry:
+!r4,r5:operand
+!
+!Exit:
+!r0:result
+!
+!Note:argument is passed in regs r4 and r5, the result is returned in
+!reg r0.
+!
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+
+	.text
+	.align 	5
+	.global GLOBAL (fixdfsi)
+	FUNC (GLOBAL (fixdfsi))
+
+GLOBAL (fixdfsi):
+
+#ifdef  __LITTLE_ENDIAN__
+        mov     r4,r1
+        mov     r5,r4
+        mov     r1,r5
+
+#endif
+	mov.l	.L_p_inf,r2
+	mov     #-20,r1
+	
+	mov	r2,r7
+	mov.l   .L_1023,r3
+
+	and	r4,r2
+	shll    r4
+        
+	movt    r6		! r6 contains the sign bit
+	
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r1,r2		! r2 contains the exponent
+#else
+        SHLR20 (r2)
+#endif
+	 shlr    r4
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r1,r7
+#else
+        SHLR20 (r7)
+#endif
+	cmp/hi	r2,r3		! if exp < 1023,return 0
+	mov.l	.L_mask_high_mant,r1
+
+	SL(bt,	.L_epil,
+	 mov	#0,r0)
+	and	r4,r1		! r1 contains high mantissa
+
+	cmp/eq	r2,r7		! chk if exp is invalid
+	mov.l	.L_1053,r7
+
+	bt	.L_inv_exp
+	mov	#11,r0
+	
+	cmp/hi	r7,r2		! If exp > 1053,return maxint
+	sub     r2,r7
+
+	mov.l	.L_21bit,r2
+	SL(bt,	.L_ret_max,
+	 add	#1,r7)		! r7 contains the number of shifts
+
+	or	r2,r1
+	mov	r7,r3
+	shll8   r1
+
+	neg     r7,r7
+	shll2	r1
+
+        shll	r1
+	cmp/hi	r3,r0
+
+	!chk if the result can be made only from higher mantissa
+	SL(bt,	.L_lower_mantissa,
+	 mov	#21,r0)
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r7,r1
+#else
+.L_loop:
+        tst	r7,r7
+        bt      .L_break1
+        add     #1,r7
+        bra     .L_loop
+        shlr    r1
+
+.L_break1:
+#endif
+	tst	r6,r6
+	SL(bt,	.L_epil,
+	 mov	r1,r0)
+
+	rts
+	neg	r0,r0
+
+.L_lower_mantissa:
+	!result is made from lower mantissa also
+	neg	r0,r0
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r0,r5
+#else
+        SHLR21 (r5)
+#endif
+
+	or	r5,r1		!pack lower and higher mantissas
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r7,r1
+#else
+.L_sh_loop:
+	tst	r7,r7
+	bt	.L_break
+	add	#1,r7
+	bra	.L_sh_loop
+	shlr	r1
+
+.L_break:
+#endif
+	mov	r1,r0
+	bra	.L_chk_sign
+	nop
+
+.L_epil:
+	rts
+	nop
+
+.L_inv_exp:
+	cmp/hi	r0,r5
+	bt	.L_epil
+
+	cmp/hi	r0,r1		!compare high mantissa,r1
+	bt	.L_epil
+
+.L_ret_max:
+	mov.l   .L_maxint,r0
+	tst	r6,r6
+	bt	.L_epil
+
+	rts
+	add	#1,r0
+
+.L_chk_sign:
+	tst	r6,r6		!sign bit is set, number is -ve
+	bt	.L_epil
+	
+	rts
+	neg	r0,r0
+
+	.align	2
+
+.L_maxint:
+	.long	0x7fffffff
+.L_p_inf:
+	.long	0x7ff00000
+.L_mask_high_mant:
+	.long	0x000fffff
+.L_1023:
+	.long	0x000003ff
+.L_1053:
+	.long	1053
+.L_21bit:
+	.long	0x00100000
+
+ENDFUNC (GLOBAL (fixdfsi))
diff -Naur gcc-4.2.1/gcc/config/sh/IEEE-754/fixsfsi.S gcc-4.2.1-stm/gcc/config/sh/IEEE-754/fixsfsi.S
--- gcc-4.2.1/gcc/config/sh/IEEE-754/fixsfsi.S	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/IEEE-754/fixsfsi.S	2007-08-10 13:08:59.000000000 +0100
@@ -0,0 +1,165 @@
+/* Copyright (C) 2004 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+! Conversion routine for float to integer
+
+! Author: Rakesh Kumar
+
+! Arguments: r4 (in floating point format)
+! Return: r0
+
+! r4 is referred as op1
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+
+	.text
+	.align 5
+	.global	GLOBAL (fixsfsi)
+	FUNC (GLOBAL (fixsfsi))
+
+GLOBAL (fixsfsi):
+	mov.l	.L_mask_sign,r7
+	mov	r4,r2
+
+	! Check for NaN
+	mov.l	.L_inf,r1
+	and	r7,r2
+
+	cmp/gt	r1,r2
+	mov	#127,r5
+
+	mov	r4,r3
+	SL(bt,	.L_epil,
+	 mov	#0,r0)
+
+	shll	r2
+	mov.l	.L_frac,r6
+
+	shlr16	r2
+	and	r6,r3	! r3 has fraction
+
+	shlr8	r2	! r2 has exponent
+	mov.l	.L_24bit,r1
+
+	! If exponent is less than 127, return 0
+	cmp/gt	r2,r5
+	or	r1,r3	! Set the implicit bit
+
+	mov.l	.L_157,r1
+	SL1(bt,	.L_epil,
+	 shll8	r3)
+
+	! If exponent is greater than 157,
+	! return the maximum/minumum integer
+	! value deducing from sign
+	cmp/gt	r1,r2
+	sub	r2,r1
+
+	mov.l	.L_sign,r2
+	SL(bt,	.L_ret_max,
+	 add	#1,r1)
+
+	and	r4,r2	! Sign in r2
+	neg	r1,r1
+
+	! Shift mantissa by exponent difference from 157
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+	shld	r1,r3
+#else
+        cmp/gt  r0,r1
+        bt      .L_mov_left
+
+.L_mov_right:
+        cmp/eq  r1,r0
+        bt      .L_ret
+
+        add     #1,r1
+        bra     .L_mov_right
+
+        shlr    r3
+
+.L_mov_left:
+        add     #-1,r1
+
+        shll    r3
+        cmp/eq  r1,r0
+
+        bf      .L_mov_left
+.L_ret:
+#endif
+	! If op1 is negative, negate the result
+	cmp/eq	r0,r2
+	SL(bf,	.L_negate,
+	 mov	r3,r0)
+
+! r0 has the appropriate value
+.L_epil:
+	rts
+	nop
+
+! Return the max/min integer value
+.L_ret_max:
+	and	r4,r2	! Sign in r2
+	mov.l	.L_max,r3
+
+	mov.l	.L_sign,r1
+	cmp/eq	r0,r2
+
+	mov	r3,r0
+	bt	.L_epil
+
+	! Negative number, return min int
+	rts
+	mov	r1,r0
+
+! Negate the result
+.L_negate:
+	rts
+	neg	r0,r0
+
+	.align 2
+.L_inf:
+	.long 0x7F800000
+
+.L_157:
+	.long 157
+
+.L_max:
+	.long 0x7FFFFFFF
+
+.L_frac:
+	.long 0x007FFFFF
+
+.L_sign:
+	.long 0x80000000
+
+.L_24bit:
+	.long 0x00800000
+
+.L_mask_sign:
+	.long 0x7FFFFFFF
+
+ENDFUNC (GLOBAL (fixsfsi))
diff -Naur gcc-4.2.1/gcc/config/sh/IEEE-754/fixunsdfsi.S gcc-4.2.1-stm/gcc/config/sh/IEEE-754/fixunsdfsi.S
--- gcc-4.2.1/gcc/config/sh/IEEE-754/fixunsdfsi.S	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/IEEE-754/fixunsdfsi.S	2007-08-10 13:08:59.000000000 +0100
@@ -0,0 +1,181 @@
+/* Copyright (C) 2004 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+!conversion of double precision floating point number to unsigned integer
+!Author:Aanchal Khanna
+!
+!Entry:
+!r4,r5:operand
+!
+!Exit:
+!r0:result
+!
+!Note:argument is passed in regs r4 and r5, the result is returned in
+!reg r0.
+!
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+
+	.text
+	.align 5
+	.global GLOBAL (fixunsdfsi)
+	FUNC (GLOBAL (fixunsdfsi))
+
+GLOBAL (fixunsdfsi):
+
+#ifdef  __LITTLE_ENDIAN__
+        mov     r4,r1
+        mov     r5,r4
+        mov     r1,r5
+#endif
+	mov.l	.L_p_inf,r2
+	mov     #-20,r1
+	
+	mov	r2,r7
+	mov.l   .L_1023,r3
+
+	and	r4,r2
+	shll    r4
+
+        movt    r6		! r6 contains the sign bit
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r1,r2           ! r2 contains the exponent
+#else
+        SHLR20 (r2)
+#endif
+	shlr    r4
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r1,r7
+#else
+        SHLR20 (r7)
+#endif
+	tst	r6,r6	
+	SL(bf,	.L_epil,
+	 mov	#0,r0)
+
+	cmp/hi	r2,r3		! if exp < 1023,return 0
+	mov.l	.L_high_mant,r1
+
+	SL(bt,	.L_epil,
+	 and	r4,r1)		! r1 contains high mantissa
+
+	cmp/eq	r2,r7		! chk if exp is invalid
+	mov.l	.L_1054,r7
+
+	bt	.L_inv_exp
+	mov	#11,r0
+	
+	cmp/hi	r7,r2		! If exp > 1054,return maxint
+	sub     r2,r7		!r7 contains the number of shifts
+
+	mov.l	.L_21bit,r2
+	bt	.L_ret_max
+
+	or	r2,r1
+	mov	r7,r3
+
+	shll8   r1
+	neg     r7,r7
+
+	shll2	r1
+
+        shll	r1
+	cmp/hi	r3,r0
+
+	SL(bt,	.L_lower_mant,
+	 mov	#21,r0)
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r7,r1
+#else
+.L_sh_loop:
+        tst	r7,r7
+        bt      .L_break
+        add     #1,r7
+        bra     .L_sh_loop
+        shlr    r1
+
+.L_break:
+#endif
+	rts
+	mov     r1,r0
+
+.L_lower_mant:
+	neg	r0,r0
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r0,r5
+#else
+        SHLR21 (r5)
+#endif
+	or	r5,r1		!pack lower and higher mantissas
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r7,r1
+#else
+.L_loop:
+        tst	r7,r7
+        bt      .L_break1
+        add     #1,r7
+        bra     .L_loop
+        shlr    r1
+
+.L_break1:
+#endif
+	mov	r1,r0
+.L_epil:
+	rts
+	nop
+
+.L_inv_exp:
+	cmp/hi	r0,r5
+	bt	.L_epil
+
+	cmp/hi	r0,r1		!compare high mantissa,r1
+	bt	.L_epil
+
+.L_ret_max:
+	mov.l   .L_maxint,r0
+
+	rts
+	nop
+
+	.align	2
+
+.L_maxint:
+	.long	0xffffffff
+.L_p_inf:
+	.long	0x7ff00000
+.L_high_mant:
+	.long	0x000fffff
+.L_1023:
+	.long	0x000003ff
+.L_1054:
+	.long	1054
+.L_21bit:
+	.long	0x00100000
+
+ENDFUNC (GLOBAL (fixunsdfsi))
diff -Naur gcc-4.2.1/gcc/config/sh/IEEE-754/fixunssfsi.S gcc-4.2.1-stm/gcc/config/sh/IEEE-754/fixunssfsi.S
--- gcc-4.2.1/gcc/config/sh/IEEE-754/fixunssfsi.S	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/IEEE-754/fixunssfsi.S	2007-08-10 13:08:59.000000000 +0100
@@ -0,0 +1,155 @@
+/* Copyright (C) 2004 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+! Conversion from floating point to unsigned integer
+
+! Author: Rakesh Kumar
+
+! Argument: r4 (in floating point format)
+! Result: r0
+
+! For negative floating point numbers, it returns zero
+
+! The argument is referred as op1
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+
+	.text
+	.align 5
+	.global	GLOBAL (fixunssfsi)
+	FUNC (GLOBAL (fixunssfsi))
+
+GLOBAL (fixunssfsi):
+	mov.l	.L_sign,r0
+	mov	r4,r2
+
+	! Check for NaN
+	mov.l	.L_inf,r1
+	and	r4,r0
+
+	mov.l	.L_mask_sign,r7
+	mov	#127,r5
+
+	! Remove sign bit
+	cmp/eq	#0,r0
+	and	r7,r2
+
+	! If number is negative, return 0
+	! LIBGCC deviates from standard in this regard.
+	mov	r4,r3
+	SL(bf,	.L_epil,
+	 mov	#0,r0)
+
+	mov.l	.L_frac,r6
+	cmp/gt	r1,r2
+
+	shll	r2
+	SL1(bt,	.L_epil,
+	 shlr16	r2)
+
+	shlr8	r2	! r2 has exponent
+	mov.l	.L_24bit,r1
+
+	and	r6,r3	! r3 has fraction
+	cmp/gt	r2,r5
+
+	! If exponent is less than 127, return 0
+	or	r1,r3
+	bt	.L_epil
+
+	! Process only if exponent is less than 158
+	mov.l	.L_158,r1
+	shll8	r3
+
+	cmp/gt	r1,r2
+	sub	r2,r1
+
+	neg	r1,r1
+	bt	.L_ret_max
+
+! Shift the mantissa with exponent difference from 158
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+	shld	r1,r3
+#else
+	cmp/gt	r0,r1
+	bt	.L_mov_left
+
+.L_mov_right:
+	cmp/eq	r1,r0
+	bt	.L_ret
+
+	add	#1,r1
+	bra	.L_mov_right
+	shlr	r3
+
+.L_mov_left:
+	add	#-1,r1
+	
+	shll	r3
+	cmp/eq	r1,r0
+
+	bf	.L_mov_left
+
+.L_ret:	
+#endif
+	rts
+	mov	r3,r0
+
+! r0 already has appropriate value
+.L_epil:
+	rts
+	nop
+
+! Return the maximum unsigned integer value
+.L_ret_max:
+	mov.l	.L_max,r3
+
+	rts
+	mov	r3,r0
+
+	.align 2
+.L_inf:
+	.long 0x7F800000
+
+.L_158:
+	.long 158
+
+.L_max:
+	.long 0xFFFFFFFF
+
+.L_frac:
+	.long 0x007FFFFF
+
+.L_sign:
+	.long 0x80000000
+
+.L_24bit:
+	.long 0x00800000
+
+.L_mask_sign:
+	.long 0x7FFFFFFF
+
+ENDFUNC (GLOBAL (fixunssfsi))
diff -Naur gcc-4.2.1/gcc/config/sh/IEEE-754/floatsidf.S gcc-4.2.1-stm/gcc/config/sh/IEEE-754/floatsidf.S
--- gcc-4.2.1/gcc/config/sh/IEEE-754/floatsidf.S	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/IEEE-754/floatsidf.S	2007-08-10 13:08:59.000000000 +0100
@@ -0,0 +1,151 @@
+/* Copyright (C) 2004 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+!conversion of signed integer to double precision floating point number
+!Author:Rakesh Kumar
+!
+!Entry:
+!r4:operand 
+!
+!Exit:
+!r0,r1:result
+!
+!Note:argument is passed in reg r4 and the result is returned in 
+!regs r0 and r1.
+!
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+
+        .text
+        .align 5
+        .global GLOBAL (floatsidf)
+	FUNC (GLOBAL (floatsidf))
+
+GLOBAL (floatsidf):
+        mov.l   .L_sign,r0
+        mov     #0,r1
+
+	mov	r0,r2
+	tst	r4,r4 ! check r4 for zero
+
+	! Extract the sign
+	mov	r2,r3
+	SL(bt,	.L_ret_zero,
+	 and	r4,r0)
+
+	cmp/eq	r1,r0
+	not	r3,r3
+
+	mov	r1,r7
+	SL(bt,	.L_loop,
+	 and	r4,r3)
+
+	! Treat -2147483648 as special case
+	cmp/eq	r1,r3
+	neg	r4,r4
+
+	bt	.L_ret_min	
+
+.L_loop:
+	shll	r4	
+	mov	r4,r5
+
+	and	r2,r5
+	cmp/eq	r1,r5
+	
+	add	#1,r7
+	bt	.L_loop
+
+	mov.l	.L_initial_exp,r6
+	not	r2,r2
+	
+	and	r2,r4
+	mov	#21,r3
+
+	sub	r7,r6
+	mov	r4,r1
+
+	mov	#20,r7
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r3,r1
+#else
+        SHLL21 (r1)
+#endif
+	mov	#-11,r2
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r7,r6	! Exponent in proper place
+#else
+        SHLL20 (r6)
+#endif
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r2,r4
+#else
+        SHLR11 (r4)
+#endif
+	or	r6,r0
+
+#ifdef __LITTLE_ENDIAN__
+        mov     r0,r2
+        mov     r1,r0
+        mov     r2,r1
+#endif
+	rts
+#ifdef __LITTLE_ENDIAN__
+	or	r4,r1
+#else
+	or	r4,r0
+#endif
+	
+.L_ret_zero:
+	rts
+	mov	#0,r0
+
+.L_ret_min:
+	mov.l	.L_min,r0
+	
+#ifdef __LITTLE_ENDIAN__
+        mov     r0,r2
+        mov     r1,r0
+        mov     r2,r1
+#endif
+	rts
+	nop
+
+	.align 2
+
+.L_initial_exp:
+	.long 0x0000041E
+
+.L_sign:
+	.long 0x80000000
+
+.L_min:
+	.long 0xC1E00000
+
+ENDFUNC (GLOBAL (floatsidf))
diff -Naur gcc-4.2.1/gcc/config/sh/IEEE-754/floatsisf.S gcc-4.2.1-stm/gcc/config/sh/IEEE-754/floatsisf.S
--- gcc-4.2.1/gcc/config/sh/IEEE-754/floatsisf.S	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/IEEE-754/floatsisf.S	2007-08-10 13:08:59.000000000 +0100
@@ -0,0 +1,200 @@
+/* Copyright (C) 2004 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+! Conversion of integer to floating point
+
+! Author: Rakesh Kumar
+
+! Argument: r4
+! Result: r0
+
+! r4 is referred as op1
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+
+        .text
+        .align 5
+        .global GLOBAL (floatsisf)
+        FUNC (GLOBAL (floatsisf))
+
+GLOBAL (floatsisf):
+	mov.l	.L_sign,r2
+	mov	#23,r6
+
+	! Check for zero
+	tst	r4,r4
+	mov.l	.L_24_bits,r7
+
+	! Extract sign
+	and	r4,r2
+	bt	.L_ret
+
+	! Negative ???
+	mov.l	.L_imp_bit,r5
+	cmp/pl	r4
+
+	not	r7,r3
+	bf	.L_neg
+
+	! Decide the direction for shifting
+	cmp/gt	r7,r4
+	mov	r4,r0
+
+	and	r5,r0
+	bt	.L_shr_0
+
+	! Number may already be in normalized form
+	cmp/eq	#0,r0
+	bf	.L_pack
+
+! Shift the bits to the left. Adjust the exponent
+.L_shl:
+	shll	r4
+	mov	r4,r0
+
+	and	r5,r0
+	cmp/eq	#0,r0
+
+	SL(bt,	.L_shl,
+	 add	#-1,r6)
+
+! Pack the value in floating point format.
+! r6 has unbiased exponent, r4 has mantissa, r2 has sign
+.L_pack:
+	mov	#23,r3
+	not	r5,r5
+
+	mov	r2,r0
+	add	#127,r6
+
+	and	r5,r4
+#if defined (__sh1__) || defined (__sh2__) || defined (__SH2E__)
+	SHLL23 (r6)
+#else
+	shld	r3,r6
+#endif
+
+	or	r6,r0
+	rts
+	or	r4,r0
+
+! Negate the number
+.L_neg:
+	! Take care for -2147483648.
+	mov	r4,r0
+	shll	r0
+	
+	cmp/eq	#0,r0
+	SL(bt,	.L_ret_min,
+	 neg	r4,r4)
+
+        cmp/gt  r7,r4
+        bt	.L_shr_0
+
+	mov	r4,r0
+	and	r5,r0
+
+	cmp/eq	#0,r0
+	bf	.L_pack
+	bt	.L_shl
+	
+.L_shr_0:
+	mov	#0,r1
+
+! Shift right the number with rounding
+.L_shr:
+	shlr	r4
+	movt	r7
+
+	tst	r7,r7
+
+	! Count number of ON bits shifted
+	bt	.L_shr_1
+	add	#1,r1
+
+.L_shr_1:
+	mov	r4,r0
+	add	#1,r6
+
+	and	r3,r0
+	cmp/eq	#0,r0
+
+	! Add MSB of shifted bits
+	bf	.L_shr
+	add	r7,r4
+
+	tst	r7,r7
+	bt	.L_pack
+
+.L_pack1:
+	mov	#1,r0
+	cmp/eq	r1,r0
+
+	bt	.L_rnd
+	mov	r4,r0
+
+	! Rounding may have misplaced MSB. Adjust.
+	and	r3,r0
+	cmp/eq	#0,r0
+
+	bf	.L_shr
+	bt	.L_pack
+
+! If only MSB of shifted bits is ON, we are halfway
+! between two numbers. Round towards even LSB of
+! resultant mantissa.
+.L_rnd:
+	shlr	r4
+	bra	.L_pack
+	shll	r4
+
+.L_ret:
+	rts
+	mov	r4,r0
+
+! Return value for -2147483648
+.L_ret_min:
+	mov.l	.L_min_val,r0
+	rts
+	nop
+
+	.align 2
+.L_sign:
+	.long 0x80000000
+
+.L_imp_bit:
+	.long 0x00800000
+
+.L_24_bits:
+	.long 0x00FFFFFF
+
+.L_nsign:
+	.long 0x7FFFFFFF
+
+.L_min_val:
+	.long 0xCF000000
+
+ENDFUNC (GLOBAL (floatsisf))
diff -Naur gcc-4.2.1/gcc/config/sh/IEEE-754/floatunssidf.S gcc-4.2.1-stm/gcc/config/sh/IEEE-754/floatunssidf.S
--- gcc-4.2.1/gcc/config/sh/IEEE-754/floatunssidf.S	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/IEEE-754/floatunssidf.S	2007-08-10 13:08:59.000000000 +0100
@@ -0,0 +1,76 @@
+/* Copyright (C) 2004, 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+!conversion of unsigned integer to double precision floating point number
+!Author:Rakesh Kumar
+!Rewritten for SH1 support: Joern Rennecke
+!
+!Entry:
+!r4:operand
+!
+!Exit:
+!r0,r1:result
+!
+!Note:argument is passed in reg r4 and the result is returned in
+!regs r0 and r1.
+!
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+
+        .text
+        .align 5
+        .global GLOBAL (floatunsidf)
+	FUNC (GLOBAL (floatunsidf))
+
+GLOBAL (floatunsidf):
+	mov.w	LOCAL(x41f0),DBLRH	! bias + 32
+	tst	r4,r4			! check for zero
+	bt	.L_ret_zero
+.L_loop:
+	shll	r4	
+	SL(bf,	.L_loop,
+	 add	#-16,DBLRH)
+
+	mov	r4,DBLRL
+
+        SHLL20 (DBLRL)
+
+        shll16	DBLRH ! put exponent in proper place
+
+        SHLR12 (r4)
+
+	rts
+	or	r4,DBLRH
+	
+.L_ret_zero:
+	mov	#0,r1
+	rts
+	mov	#0,r0
+
+LOCAL(x41f0):	.word	0x41f0
+	.align 2
+
+ENDFUNC (GLOBAL (floatunsidf))
diff -Naur gcc-4.2.1/gcc/config/sh/IEEE-754/floatunssisf.S gcc-4.2.1-stm/gcc/config/sh/IEEE-754/floatunssisf.S
--- gcc-4.2.1/gcc/config/sh/IEEE-754/floatunssisf.S	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/IEEE-754/floatunssisf.S	2007-08-10 13:08:59.000000000 +0100
@@ -0,0 +1,137 @@
+/* Copyright (C) 2004 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+! Conversion of unsigned integer to floating point
+
+! Author: Rakesh Kumar
+
+! Argument: r4
+! Result: r0
+
+! r4 is referred as op1
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+
+        .text
+        .align 5
+        .global GLOBAL (floatunsisf)
+	FUNC (GLOBAL (floatunsisf))
+
+GLOBAL (floatunsisf):
+	tst	r4,r4
+	mov	#23,r6
+
+	mov.l	.L_set_24_bits,r7
+	SL(bt,	.L_return,
+	 not	r7,r3)
+
+	! Decide the direction for shifting
+	mov.l	.L_set_24_bit,r5
+	cmp/hi	r7,r4
+
+	not	r5,r2
+	SL(bt,	.L_shift_right,
+	 mov	#0,r7)
+
+	tst	r5,r4
+	
+	mov	#0,r0
+	bf	.L_pack_sf
+
+! Shift the bits to the left. Adjust the exponent
+.L_shift_left:
+	shll	r4
+	tst	r5,r4
+
+	add	#-1,r6
+	bt	.L_shift_left
+
+! Pack the value in floating point format.
+! r6 has unbiased exponent, r4 has mantissa
+.L_pack_sf:
+	mov	#23,r3
+	add	#127,r6
+
+	! Align the exponent
+	and	r2,r4
+#if defined (__sh1__) || defined (__sh2__) || defined (__SH2E__)
+        SHLL23 (r6)
+#else
+	shld	r3,r6
+#endif
+
+	or	r6,r0
+	rts
+	or	r4,r0
+
+! Shift right the number with rounding
+.L_shift_right:
+	shlr	r4
+	rotcr	r7
+
+	tst	r4,r3
+	add	#1,r6
+
+	bf	.L_shift_right
+	
+	tst	r7,r7
+	bt	.L_sh_rt_1
+
+	shll	r7
+	movt	r1
+
+	add	r1,r4
+
+	tst	r7,r7
+	bf	.L_sh_rt_1
+
+	! Halfway between two numbers.
+	! Round towards LSB = 0
+	shlr	r4
+	shll	r4
+
+.L_sh_rt_1:
+	mov	r4,r0
+
+	! Rounding may have misplaced MSB. Adjust.
+	and	r3,r0
+	cmp/eq	#0,r0
+
+	bf	.L_shift_right
+	bt	.L_pack_sf
+
+.L_return:
+	rts
+	mov	r4,r0
+
+	.align 2
+.L_set_24_bit:
+	.long 0x00800000
+
+.L_set_24_bits:
+	.long 0x00FFFFFF
+
+ENDFUNC (GLOBAL (floatunsisf))
diff -Naur gcc-4.2.1/gcc/config/sh/IEEE-754/m3/adddf3.S gcc-4.2.1-stm/gcc/config/sh/IEEE-754/m3/adddf3.S
--- gcc-4.2.1/gcc/config/sh/IEEE-754/m3/adddf3.S	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/IEEE-754/m3/adddf3.S	2007-08-10 13:08:59.000000000 +0100
@@ -0,0 +1,588 @@
+/* Copyright (C) 2004, 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+! adddf3 for the Renesas SH / STMicroelectronics ST40 CPUs.
+! Contributed by Joern Rennecke
+! joern.rennecke@st.com
+!
+! This code is optimized for SH4-200 without FPU, but can also be used for SH3.
+! Numbers with same sign are added in typically 37 cycles, worst case is
+! 43 cycles, unless there is an overflow, in which case the addition can
+! take up to takes 47 cycles.
+! Normal numbers with different sign are added in 56 (57 for PIC) cycles
+! or less on SH4.
+! If one of the inputs is a denormal, the worst case is 59 (60 for PIC)
+! cycles. (Two denormal inputs are faster than normal inputs, and
+! denormal outputs don't slow down computation).
+! Subtraction takes two cycles to negate the second input and then drops
+! through to addition.
+
+/* If the input exponents of a difference of two normalized numbers
+   differ by more than one, the output does not need to be adjusted
+   by more than one bit position.  Hence, it makes sense to ensure that
+   the shifts by 0 & 1 are handled quickly to reduce average and worst
+   case times.  */
+FUNC(GLOBAL(adddf3))
+FUNC(GLOBAL(subdf3))
+	.global	GLOBAL(adddf3)
+	.global	GLOBAL(subdf3)
+LOCAL(denorm_arg1):
+	bt	LOCAL(inf_nan_arg0)
+	tst	r0,r2
+	bt/s	LOCAL(denorm_both)
+	shlr	r1
+	mov.l	LOCAL(x00100000),r3
+	bra	LOCAL(denorm_arg1_done)
+	 sub	r2,r3
+
+! Handle denorm addition here because otherwise the ordinary addition would
+! have to check for denormal results.
+! Denormal subtraction could also be done faster, but the denorm subtraction
+! path here is still one cycles faster than the one for normalized input
+! numbers, and 16 instructions shorter than the fastest version.
+! Here we also generate +0.0 + +0.0 -> +0.0 ; -0.0 + -0.0 -> -0.0
+LOCAL(denorm_both):
+	div0s	DBL0H,DBL1H
+	mov.l	LOCAL(x800fffff),r9
+	bt/s	LOCAL(denorm_sub)
+	and	r1,DBL1H
+	and	r9,DBL0H
+	mov.l	@r15+,r9
+	mov	DBL0L,DBLRL
+	mov	DBL0H,DBLRH
+	addc	DBL1L,DBLRL
+	mov.l	@r15+,r8
+	rts
+	 addc	DBL1H,DBLRH
+
+! N.B., since subtraction also generates +0.0 for subtraction of numbers
+! with identical fractions, this also covers the +0.0 + -0.0 -> +0.0 /
+! -0.0 + +0.0 -> +0.0 cases.
+LOCAL(denorm_sub):
+	mov	DBL0H,r8	! tentative result sign
+	and	r1,DBL0H
+	bra	LOCAL(sub_same_exp)
+	 addc	r1,r2	! exponent++, clear T
+
+LOCAL(inf_nan_arg0):
+	mov	DBL0L,DBLRL
+	bra	LOCAL(pop_r8_r9)
+	 mov	DBL0H,DBLRH
+
+LOCAL(ret_arg0):
+	mov.l	LOCAL(x800fffff),DBLRH
+	mov	DBL0L,DBLRL
+	mov	r2,r3
+LOCAL(ret_arg):
+	mov.l	@r15+,r9
+	and	r8,DBLRH
+	mov.l	@r15+,r8
+	rts
+	or	r3,DBLRH
+
+	.balign	4
+GLOBAL(subdf3):
+	cmp/pz	DBL1H
+	add	DBL1H,DBL1H
+	rotcr	DBL1H
+	nop
+
+GLOBAL(adddf3):
+	mov.l	LOCAL(x7ff00000),r0
+	mov	DBL0H,r2
+	mov.l	LOCAL(x001fffff),r1
+	mov	DBL1H,r3
+	mov.l	r8,@-r15
+	and	r0,r2
+	mov.l	r9,@-r15
+	and	r0,r3
+	cmp/hi	r2,r3
+	or	r0,DBL0H
+	or	r0,DBL1H
+	bt	LOCAL(arg1_gt)
+	tst	r0,r3
+	mov	#-20,r9
+	bt/s	LOCAL(denorm_arg1)
+	cmp/hs	r0,r2
+	bt	LOCAL(inf_nan_arg0)
+	sub	r2,r3
+LOCAL(denorm_arg1_done):	! r2 is tentative result exponent
+	shad	r9,r3
+	mov.w	LOCAL(m32),r9
+	mov	DBL0H,r8	! tentative result sign
+	and	r1,DBL0H	! arg0 fraction
+	mov	DBL1H,r0	! the 'other' sign
+	and	r1,DBL1H	! arg1 fraction
+	cmp/ge	r9,r3
+	mov	DBL1H,r1
+	bf/s	LOCAL(large_shift_arg1)
+	 shld	r3,DBL1H
+LOCAL(small_shift_arg1):
+	mov	DBL1L,r9
+	shld	r3,DBL1L
+	tst	r3,r3
+	add	#32,r3
+	bt/s	LOCAL(same_exp)
+	 div0s	r8,r0	! compare signs
+	shld	r3,r1
+
+	or	r1,DBL1L
+	bf/s	LOCAL(add)
+	shld	r3,r9
+	clrt
+	negc	r9,r9
+	mov.l	LOCAL(x001f0000),r3
+LOCAL(sub_high):
+	mov	DBL0L,DBLRL
+	subc	DBL1L,DBLRL
+	mov	DBL0H,DBLRH
+	bra	LOCAL(subtract_done)
+	 subc	DBL1H,DBLRH
+
+LOCAL(large_shift_arg1):
+	mov.w	LOCAL(d0),r9
+	add	#64,r3
+	cmp/pl	r3
+	shld	r3,r1
+	bf	LOCAL(ret_arg0)
+	cmp/hi	r9,DBL1L
+	mov	DBL1H,DBL1L
+	mov	r9,DBL1H
+	addc	r1,r9
+
+	div0s	r8,r0	! compare signs
+
+	bf	LOCAL(add)
+	clrt
+	mov.l	LOCAL(x001f0000),r3
+	bra	LOCAL(sub_high)
+	 negc	r9,r9
+
+LOCAL(add_clr_r9):
+	mov	#0,r9
+LOCAL(add):
+	mov.l	LOCAL(x00200000),r3
+	addc	DBL1L,DBL0L
+	addc	DBL1H,DBL0H
+	mov.l	LOCAL(x80000000),r1
+	tst	r3,DBL0H
+	mov.l	LOCAL(x7fffffff),r3
+	mov	DBL0L,r0
+	bt/s	LOCAL(no_carry)
+	and	r1,r8
+	tst	r9,r9
+	bf	LOCAL(add_one)
+	tst	#2,r0
+LOCAL(add_one):
+	subc	r9,r9
+	sett
+	mov	r0,DBLRL
+	addc	r9,DBLRL
+	mov	DBL0H,DBLRH
+	addc	r9,DBLRH
+	shlr	DBLRH
+	mov.l	LOCAL(x7ff00000),r3
+	add	r2,DBLRH
+	mov.l	@r15+,r9
+	rotcr	DBLRL
+	cmp/hi	r3,DBLRH
+LOCAL(add_done):
+	bt	LOCAL(inf)
+LOCAL(or_sign):
+	or	r8,DBLRH
+	rts
+	 mov.l	@r15+,r8
+
+LOCAL(inf):
+	mov	#0,DBLRL
+	bra	LOCAL(or_sign)
+	mov	r3,DBLRH
+
+LOCAL(pos_difference_0):
+	tst	r3,DBL0H
+	mov	DBL0L,DBLRL
+	mov.l	LOCAL(x80000000),DBL0L
+	mov	DBL0H,DBLRH
+	mov.l	LOCAL(x00100000),DBL0H
+	bt/s	LOCAL(long_norm)
+	and	DBL0L,r8
+	bra	LOCAL(norm_loop)
+	 not	DBL0L,r3
+
+LOCAL(same_exp):
+	bf	LOCAL(add_clr_r9)
+	clrt
+LOCAL(sub_same_exp):
+	subc	DBL1L,DBL0L
+	mov.l	LOCAL(x001f0000),r3
+	subc	DBL1H,DBL0H
+	mov.w	LOCAL(d0),r9
+	bf	LOCAL(pos_difference_0)
+	clrt
+	negc	DBL0L,DBLRL
+	mov.l	LOCAL(x80000000),DBL0L
+	negc	DBL0H,DBLRH
+	mov.l	LOCAL(x00100000),DBL0H
+	tst	r3,DBLRH
+	not	r8,r8
+	bt/s	LOCAL(long_norm)
+	and	DBL0L,r8
+	bra	LOCAL(norm_loop)
+	 not	DBL0L,r3
+
+LOCAL(large_shift_arg0):
+	add	#64,r2
+
+	mov	#0,r9
+	cmp/pl	r2
+	shld	r2,r1
+	bf	LOCAL(ret_arg1_exp_r3)
+	cmp/hi	r9,DBL0L
+	mov	DBL0H,DBL0L
+	mov	r9,DBL0H
+	addc	r1,r9
+	div0s	r8,r0	! compare signs
+	mov	r3,r2	! tentative result exponent
+	bf	LOCAL(add)
+	clrt
+	negc	r9,r9
+	bra	LOCAL(subtract_arg0_arg1_done)
+	 mov	DBL1L,DBLRL
+
+LOCAL(arg1_gt):
+	tst	r0,r2
+	mov	#-20,r9
+	bt/s	LOCAL(denorm_arg0)
+	cmp/hs	r0,r3
+	bt	LOCAL(inf_nan_arg1)
+	sub	r3,r2
+LOCAL(denorm_arg0_done):
+	shad	r9,r2
+	mov.w	LOCAL(m32),r9
+	mov	DBL1H,r8	! tentative result sign
+	and	r1,DBL1H
+	mov	DBL0H,r0	! the 'other' sign
+	and	r1,DBL0H
+	cmp/ge	r9,r2
+	mov	DBL0H,r1
+	shld	r2,DBL0H
+	bf	LOCAL(large_shift_arg0)
+	mov	DBL0L,r9
+	shld	r2,DBL0L
+	add	#32,r2
+	mov.l	r3,@-r15
+	shld	r2,r1
+	mov	r2,r3
+	div0s	r8,r0		! compare signs
+	mov.l	@r15+,r2	! tentative result exponent
+	shld	r3,r9
+	bf/s	LOCAL(add)
+	or	r1,DBL0L
+	clrt
+	negc	r9,r9
+	mov	DBL1L,DBLRL
+LOCAL(subtract_arg0_arg1_done):
+	subc	DBL0L,DBLRL
+	mov	DBL1H,DBLRH
+	mov.l	LOCAL(x001f0000),r3
+	subc	DBL0H,DBLRH
+/* Since the exponents were different, the difference is positive.  */
+/* Fall through */
+LOCAL(subtract_done):
+/* First check if a shift by a few bits is sufficient.  This not only
+   speeds up this case, but also alleviates the need for considering
+   lower bits from r9 or rounding in the other code.
+   Moreover, by handling the upper 1+4 bits of the fraction here, long_norm
+   can assume that DBLRH fits into 20 (20 < 16) bit.  */
+	tst	r3,DBLRH
+	mov.l	LOCAL(x80000000),r3
+	mov.l	LOCAL(x00100000),DBL0H
+	bt/s	LOCAL(long_norm)
+	and	r3,r8
+	mov.l	LOCAL(x7fffffff),r3
+LOCAL(norm_loop):	! Well, this used to be a loop...
+	tst	DBL0H,DBLRH
+	sub	DBL0H,r2
+	bf	LOCAL(norm_round)
+	shll	r9
+	rotcl	DBLRL
+
+	rotcl	DBLRH
+
+	tst	DBL0H,DBLRH
+	sub	DBL0H,r2
+	bf	LOCAL(norm_round)
+	shll	DBLRL
+	rotcl	DBLRH
+	mov.l	@r15+,r9
+	cmp/gt	r2,DBL0H
+	sub	DBL0H,r2
+LOCAL(norm_loop_1):
+	bt	LOCAL(denorm0_n)
+	tst	DBL0H,DBLRH
+	bf	LOCAL(norm_pack)
+	shll	DBLRL
+	rotcl	DBLRH	! clears T
+	bra	LOCAL(norm_loop_1)
+	 subc	DBL0H,r2
+
+LOCAL(no_carry):
+	shlr	r0
+	mov.l	LOCAL(x000fffff),DBLRH
+	addc	r3,r9
+	mov.w	LOCAL(d0),DBL1H
+	mov	DBL0L,DBLRL
+	and	DBL0H,DBLRH	! mask out implicit 1
+	mov.l	LOCAL(x7ff00000),r3
+	addc	DBL1H,DBLRL
+	addc	r2,DBLRH
+	mov.l	@r15+,r9
+	add	DBL1H,DBLRH	! fraction overflow -> exp increase
+	bra	LOCAL(add_done)
+	 cmp/hi	r3,DBLRH
+
+LOCAL(denorm_arg0):
+	bt	LOCAL(inf_nan_arg1)
+	mov.l	LOCAL(x00100000),r2
+	shlr	r1
+	bra	LOCAL(denorm_arg0_done)
+	 sub	r3,r2
+
+LOCAL(inf_nan_arg1):
+	mov	DBL1L,DBLRL
+	bra	LOCAL(pop_r8_r9)
+	 mov	DBL1H,DBLRH
+
+LOCAL(ret_arg1_exp_r3):
+	mov.l	LOCAL(x800fffff),DBLRH
+	bra	LOCAL(ret_arg)
+	 mov	DBL1L,DBLRL
+
+#ifdef __pic__
+	.balign 8
+#endif
+LOCAL(m32):
+	.word	-32
+LOCAL(d0):
+	.word	0
+#ifndef __pic__
+	.balign 8
+#endif
+! Because we had several bits of cancellations, we know that r9 contains
+! only one bit.
+! We'll normalize by shifting words so that DBLRH:DBLRL contains
+! the fraction with 0 < DBLRH <= 0x1fffff, then we shift DBLRH:DBLRL
+! up by 21 minus the number of non-zero bits in DBLRH.
+LOCAL(long_norm):
+	tst	DBLRH,DBLRH
+	mov.w	LOCAL(xff),DBL0L
+	mov	#21,r3
+	bf	LOCAL(long_norm_highset)
+	mov.l	LOCAL(x02100000),DBL1L	! shift 32, implicit 1
+	tst	DBLRL,DBLRL
+	extu.w	DBLRL,DBL0H
+	bt	LOCAL(zero_or_ulp)
+	mov	DBLRL,DBLRH
+	cmp/hi	DBL0H,DBLRL
+	bf	0f
+	mov.l	LOCAL(x01100000),DBL1L	! shift 16, implicit 1
+	clrt
+	shlr16  DBLRH
+	xtrct	DBLRL,r9
+	mov     DBLRH,DBL0H
+LOCAL(long_norm_ulp_done):
+0:	mov	r9,DBLRL	! DBLRH:DBLRL == fraction; DBL0H == DBLRH
+	subc	DBL1L,r2
+	bt	LOCAL(denorm1_b)
+#ifdef __pic__
+	mov.l	LOCAL(c__clz_tab),DBL1H
+LOCAL(long_norm_lookup):
+	mov	r0,r9
+	mova	LOCAL(c__clz_tab),r0
+	add	DBL1H,r0
+#else
+	mov	r0,r9
+LOCAL(long_norm_lookup):
+	mov.l	LOCAL(c__clz_tab),r0
+#endif /* __pic__ */
+	cmp/hi	DBL0L,DBL0H
+	bf	0f
+	shlr8	DBL0H
+0:	mov.b	@(r0,DBL0H),r0
+	bf	0f
+	add	#-8,r3
+0:	mov.w	LOCAL(d20),DBL0L
+	mov	#-20,DBL0H
+	clrt
+	sub	r0,r3
+	mov	r9,r0
+	mov	r3,DBL1H
+	shld	DBL0L,DBL1H
+	subc	DBL1H,r2
+	!
+	bf	LOCAL(no_denorm)
+	shad	DBL0H,r2
+	bra	LOCAL(denorm1_done)
+	add	r2,r3
+	
+LOCAL(norm_round):
+	cmp/pz	r2
+	mov	#0,DBL1H
+	bf	LOCAL(denorm0_1)
+	or	r8,r2
+	mov	DBLRL,DBL1L
+	shlr	DBL1L
+	addc	r3,r9
+	mov.l	@r15+,r9
+	addc	DBL1H,DBLRL	! round to even
+	mov.l	@r15+,r8
+	rts
+	 addc	r2,DBLRH
+
+LOCAL(norm_pack):
+	add	r8,DBLRH
+	mov.l	@r15+,r8
+	rts
+	add	r2,DBLRH
+
+LOCAL(denorm0_1):
+	mov.l	@r15+,r9
+	mov	r8,DBL0L
+	mov.l	@r15+,r8
+LOCAL(denorm0_shift):
+	shlr	DBLRH
+	rotcr	DBLRL
+
+	rts
+	add	DBL0L,DBLRH
+
+LOCAL(denorm0_n):
+	mov	r8,DBL0L
+	addc	DBL0H,r2
+	mov.l	@r15+,r8
+	bf	LOCAL(denorm0_shift)
+	rts
+	add	DBL0L,DBLRH
+
+LOCAL(no_denorm):
+	add	r2,r8		! add (exponent - 1) to sign
+
+LOCAL(denorm1_done):
+	shld	r3,DBLRH
+	mov	DBLRL,DBL0L
+	shld	r3,DBLRL
+
+	add	r8,DBLRH	! add in sign and (exponent - 1)
+	mov.l	@r15+,r9
+	add	#-32,r3
+	mov.l	@r15+,r8
+	shld	r3,DBL0L
+
+	rts
+	add	DBL0L,DBLRH
+
+LOCAL(long_norm_highset):
+	mov.l	LOCAL(x00200000),DBL1L	! shift 1, implicit 1
+	shll	r9
+	rotcl	DBLRL
+	mov	DBLRH,DBL0H
+	rotcl	DBLRH	! clears T
+#ifdef __pic__
+	mov.l	LOCAL(c__clz_tab),DBL1H
+#else
+	mov	r0,r9
+#endif /* __pic__ */
+	subc	DBL1L,r2
+	add	#-1,r3
+	bf	LOCAL(long_norm_lookup)
+LOCAL(denorm1_a):
+	shlr	DBLRH
+	rotcr	DBLRL
+	mov.l	@r15+,r9
+	or	r8,DBLRH
+
+	rts
+	mov.l	@r15+,r8
+
+	.balign	4
+LOCAL(denorm1_b):
+	mov	#-20,DBL0L
+	shad	DBL0L,r2
+	mov	DBLRH,DBL0L
+	shld	r2,DBLRH
+	shld	r2,DBLRL
+	or	r8,DBLRH
+	mov.l	@r15+,r9
+	add	#32,r2
+	mov.l	@r15+,r8
+	shld	r2,DBL0L
+	rts
+	or	DBL0L,DBLRL
+
+LOCAL(zero_or_ulp):
+	tst	r9,r9
+	bf	LOCAL(long_norm_ulp_done)
+	! return +0.0
+LOCAL(pop_r8_r9):
+	mov.l	@r15+,r9
+	rts
+	mov.l	@r15+,r8
+
+LOCAL(d20):
+	.word	20
+LOCAL(xff):
+	.word 0xff
+	.balign	4
+LOCAL(x7ff00000):
+	.long	0x7ff00000
+LOCAL(x001fffff):
+	.long	0x001fffff
+LOCAL(x80000000):
+	.long	0x80000000
+LOCAL(x000fffff):
+	.long	0x000fffff
+LOCAL(x800fffff):
+	.long	0x800fffff
+LOCAL(x001f0000):
+	.long	0x001f0000
+LOCAL(x00200000):
+	.long	0x00200000
+LOCAL(x7fffffff):
+	.long	0x7fffffff
+LOCAL(x00100000):
+	.long	0x00100000
+LOCAL(x02100000):
+	.long	0x02100000
+LOCAL(x01100000):
+	.long	0x01100000
+LOCAL(c__clz_tab):
+#ifdef __pic__
+	.long	GLOBAL(clz_tab) - .
+#else
+	.long	GLOBAL(clz_tab)
+#endif
+ENDFUNC(GLOBAL(adddf3))
+ENDFUNC(GLOBAL(subdf3))
diff -Naur gcc-4.2.1/gcc/config/sh/IEEE-754/m3/addsf3.S gcc-4.2.1-stm/gcc/config/sh/IEEE-754/m3/addsf3.S
--- gcc-4.2.1/gcc/config/sh/IEEE-754/m3/addsf3.S	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/IEEE-754/m3/addsf3.S	2007-08-10 13:08:59.000000000 +0100
@@ -0,0 +1,291 @@
+/* Copyright (C) 2004, 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+! addsf3 for the Renesas SH / STMicroelectronics ST40 CPUs.
+! Contributed by Joern Rennecke
+! joern.rennecke@st.com
+
+#ifdef L_add_sub_sf3
+	.balign 4
+	.global GLOBAL(subsf3)
+	FUNC(GLOBAL(subsf3))
+	.global GLOBAL(addsf3)
+	FUNC(GLOBAL(addsf3))
+GLOBAL(subsf3):
+	cmp/pz	r5
+	add	r5,r5
+	rotcr	r5
+	.balign 4
+GLOBAL(addsf3):
+	mov.l	LOCAL(x7f800000),r3
+	mov	r4,r6
+	add	r6,r6
+	mov	r5,r7
+	add	r7,r7
+	mov	r4,r0
+	or	r3,r0
+	cmp/hi	r6,r7
+	mov	r5,r1
+	bf/s	LOCAL(r4_hs)
+	 or	r3,r1
+	cmp/eq	r5,r1
+	bt	LOCAL(ret_r5) /* sole Inf or NaN, return unchanged.  */
+	shll8	r0	! r4 fraction
+	shll8	r1	! r5 fraction
+	mov	r6,r3
+	mov	#-24,r2
+	mov	r7,r6
+	shld	r2,r6	! r5 exp
+	mov	r0,r7
+	shld	r2,r3	! r4 exp
+	tst	r6,r6
+	sub	r6,r3	! exp difference (negative or 0)
+	bt	LOCAL(denorm_r4)
+LOCAL(denorm_r4_done): ! r1: u1.31
+	shld	r3,r0	! Get 31 upper bits, including 8 guard bits
+	mov.l	LOCAL(xff000000),r2
+	add	#31,r3
+	mov.l	r5,@-r15 ! push result sign.
+	cmp/pl	r3	! r0 has no more than one bit set -> return arg 1
+	shld	r3,r7	! copy of lowest guard bit in r0 and lower guard bits
+	bf	LOCAL(ret_stack)
+	div0s	r4,r5
+	bf/s	LOCAL(add)
+	 cmp/pl	r7	/* Is LSB in r0 clear, but any lower guards bit set?  */
+	subc	r0,r1
+	mov.l	LOCAL(c__clz_tab),r7
+	tst	r2,r1
+	mov	#-24,r3
+	bf/s LOCAL(norm_r0)
+	 mov	r1,r0
+	extu.w	r1,r1
+	bra	LOCAL(norm_check2)
+	 cmp/eq	r0,r1
+LOCAL(ret_r5):
+	rts
+	mov	r5,r0
+LOCAL(ret_stack):
+	rts
+	mov.l	@r15+,r0
+
+/* We leave the numbers denormalized, but we change the bit position to be
+   consistent with normalized numbers.  This also removes the spurious
+   leading one that was inserted before.  */
+LOCAL(denorm_r4):
+	tst	r3,r3
+	bf/s	LOCAL(denorm_r4_done)
+	add	r0,r0
+	bra	LOCAL(denorm_r4_done)
+	add	r1,r1
+LOCAL(denorm_r5):
+	tst	r6,r6
+	add	r1,r1
+	bf	LOCAL(denorm_r5_done)
+	clrt
+	bra	LOCAL(denorm_r5_done)
+	add	r0,r0
+
+/* If the exponent differs by two or more, normalization is minimal, and
+   few guard bits are needed for an exact final result, so sticky guard
+   bit compresion before subtraction (or add) works fine.
+   If the exponent differs by one, only one extra guard bit is generated,
+   and effectively no guard bit compression takes place.  */
+
+	.balign	4
+LOCAL(r4_hs):
+	cmp/eq	r4,r0
+	mov	#-24,r3
+	bt	LOCAL(inf_nan_arg0)
+	shld	r3,r7
+	shll8	r0
+	tst	r7,r7
+	shll8	r1
+	mov.l	LOCAL(xff000000),r2
+	bt/s	LOCAL(denorm_r5)
+	shld	r3,r6
+LOCAL(denorm_r5_done):
+	mov	r1,r3
+	subc	r6,r7
+	bf	LOCAL(same_exp)
+	shld	r7,r1	/* Get 31 upper bits.  */
+	add	#31,r7
+	mov.l	r4,@-r15 ! push result sign.
+	cmp/pl	r7
+	shld	r7,r3
+	bf	LOCAL(ret_stack)
+	div0s	r4,r5
+	bf/s	LOCAL(add)
+	 cmp/pl	r3	/* Is LSB in r1 clear, but any lower guard bit set?  */
+	subc	r1,r0
+	mov.l	LOCAL(c__clz_tab),r7
+LOCAL(norm_check):
+	tst	r2,r0
+	mov	#-24,r3
+	bf LOCAL(norm_r0)
+	extu.w	r0,r1
+	cmp/eq	r0,r1
+LOCAL(norm_check2):
+	mov	#-8,r3
+	bt LOCAL(norm_r0)
+	mov	#-16,r3
+LOCAL(norm_r0):
+	mov	r0,r1
+	shld	r3,r0
+#ifdef __pic__
+	add	r0,r7
+	mova  LOCAL(c__clz_tab),r0
+#endif
+	mov.b	@(r0,r7),r7
+	add	#25,r3
+	add	#-9+1,r6
+	mov	r1,r0
+	sub	r7,r3
+	mov.l	LOCAL(xbfffffff),r7
+	sub	r3,r6	/* generate exp-1  */
+	mov.w	LOCAL(d24),r2
+	cmp/pz	r6	/* check exp > 0  */
+	shld	r3,r0	/* Leading 1 becomes +1 exp adjustment.  */
+	bf	LOCAL(zero_denorm)
+LOCAL(denorm_done):
+	add	#30,r3
+	shld	r3,r1
+	mov.w   LOCAL(m1),r3
+	tst	r7,r1	! clear T if rounding up
+	shld	r2,r6
+	subc	r3,r0	! round - overflow will boost exp adjustment to 2.
+	mov.l	@r15+,r2
+	add	r6,r0	! overflow will generate inf
+	cmp/ge	r2,r3	! get sign into T
+	rts
+	rotcr	r0
+LOCAL(ret_r4):
+	rts
+	mov	r4,r0
+
+/* At worst, we are shifting the number back in place where an incoming
+   denormal was.  Thus, the shifts won't get out of range.  They still
+   might generate a zero fraction, but that's OK, that makes it 0.  */
+LOCAL(zero_denorm):
+	add	r6,r3
+	mov	r1,r0
+	mov	#0,r6	/* leading one will become free (except for rounding) */
+	bra	LOCAL(denorm_done)
+	shld	r3,r0
+
+/* Handle abs(r4) >= abs(r5), same exponents specially so we don't need
+   check for a zero fraction in the main path.  */
+LOCAL(same_exp):
+	div0s	r4,r5
+	mov.l	r4,@-r15
+	bf	LOCAL(add)
+	cmp/eq	r1,r0
+	mov.l	LOCAL(c__clz_tab),r7
+	bf/s	LOCAL(norm_check)
+	 sub	r1,r0
+	rts	! zero difference -> return +zero
+	mov.l	@r15+,r1
+
+/* r2: 0xff000000 */
+LOCAL(add):
+	addc	r1,r0
+	mov.w	LOCAL(x2ff),r7
+	shll8	r6
+	bf/s	LOCAL(no_carry)
+	shll16	r6
+	tst	r7,r0		
+	shlr8	r0
+	mov.l	@r15+,r3	! discard saved sign
+	subc	r2,r0
+	sett
+	addc	r6,r0
+	cmp/hs	r2,r0
+	bt/s	LOCAL(inf)
+	div0s	r7,r4 /* Copy sign.  */
+	rts
+	rotcr	r0
+LOCAL(inf):
+	mov	r2,r0
+	rts
+	rotcr	r0
+	
+LOCAL(no_carry):
+	mov.w	LOCAL(m1),r3
+	tst	r6,r6
+	bt	LOCAL(denorm_add)
+	add	r0,r0
+	tst	r7,r0		! check if lower guard bit set or round to even
+	shlr8	r0
+	mov.l	@r15+,r1	! discard saved sign
+	subc	r3,r0	! round ; overflow -> exp++
+	cmp/ge	r4,r3	/* Copy sign.  */
+	add	r6,r0	! overflow -> inf
+	rts
+	rotcr	r0
+
+LOCAL(denorm_add):
+	cmp/ge	r4,r3	/* Copy sign.  */
+	shlr8	r0
+	mov.l	@r15+,r1	! discard saved sign
+	rts
+	rotcr	r0
+
+LOCAL(inf_nan_arg0):
+	cmp/eq	r5,r1
+	bf	LOCAL(ret_r4)
+	div0s	r4,r5		/* Both are inf or NaN, check signs.  */
+	bt	LOCAL(ret_nan)	/* inf - inf, or NaN.  */
+	mov	r4,r0		! same sign; return NaN if either is NaN.
+	rts
+	or	r5,r0
+LOCAL(ret_nan):
+	rts
+	mov	#-1,r0
+
+LOCAL(d24):
+	.word	24
+LOCAL(x2ff):
+	.word	0x2ff
+LOCAL(m1):
+	.word	-1
+	.balign	4
+LOCAL(x7f800000):
+	.long	0x7f800000
+LOCAL(xbfffffff):
+	.long	0xbfffffff
+LOCAL(xff000000):
+	.long	0xff000000
+LOCAL(xfe000000):
+	.long	0xfe000000
+LOCAL(c__clz_tab):
+#ifdef __pic__
+	.long	GLOBAL(clz_tab) - .
+#else
+	.long	GLOBAL(clz_tab)
+#endif
+
+	ENDFUNC(GLOBAL(addsf3))
+	ENDFUNC(GLOBAL(subsf3))
+#endif /* L_add_sub_sf3 */
diff -Naur gcc-4.2.1/gcc/config/sh/IEEE-754/m3/divdf3-rt.S gcc-4.2.1-stm/gcc/config/sh/IEEE-754/m3/divdf3-rt.S
--- gcc-4.2.1/gcc/config/sh/IEEE-754/m3/divdf3-rt.S	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/IEEE-754/m3/divdf3-rt.S	2007-08-10 13:08:59.000000000 +0100
@@ -0,0 +1,519 @@
+/* Copyright (C) 2004, 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+! divdf3 for the Renesas SH / STMicroelectronics ST40 CPUs.
+! Contributed by Joern Rennecke joern.rennecke@st.com
+
+/* This version is not quite finshed, since I've found that I can
+   get better average performance with a slightly altered algorithm.
+   Still, if you want a version for hard real time, this version here might
+   be a good starting point, since it has effectively no conditional
+   branches in the path that deals with normal numbers
+   (branches with zero offset are effectively conditional execution),
+   and thus it has a uniform execution time in this path.  */
+
+/* y = 1/x  ; x (- [1,2)
+   y0 = 1.5 - x/2 - tab[(1-x)*64] = y + d ; abs(d)/y <= 0x1.0c/256
+
+   y1 = y0 - ((y0) * x - 1) * y0  =  y-x*d^2
+   y2 = y1 - ((y1) * x - 1) * y1 =~= y-x^3*d^4
+
+   z0 = y2*a ;  a1 = a - z0*x /# 32 * 64 -> 64 bit #/
+   z1 = y2*a1 (round to nearest odd 0.5 ulp);
+   a2 = a1 - z1*x /# 32 * 64 -> 64 bit #/
+
+   z = a/x = z0 + z1 - 0.5 ulp + (a2 > 0) * ulp
+
+   Unless stated otherwise, multiplies can be done in 32 * 32 bit or less
+   with suitable scaling and/or top truncation.
+   x truncated to 20 bits is sufficient to calculate y0 or even y1.
+   Table entries are adjusted by about +128 to use full signed byte range.
+   This adjustment has been perturbed slightly to allow cse with the
+   shift count constant -26.
+   The threshold point for the shift adjust before rounding is found by
+   comparing the fractions, which is exact, unlike the top bit of y2.
+   Therefore, the top bit of y2 becomes slightly random after the adjustment
+   shift, but that's OK because this can happen only at the boundaries of
+   the interval, and the baising of the error means that it can in fact happen
+   only at the bottom end.  And there, the carry propagation will make sure
+   that in the end we will have in effect an implicit 1 (or two whem rounding
+   up...)  */
+/* If an exact result exists, it can have no more bits than the divident.
+   Hence, we don't need to bother with the round-to-even tie breaker
+   unless the result is denormalized.  */
+/* 70 cycles through main path for sh4-300 .  Some cycles might be
+   saved by more careful register allocation.
+   122 cycles for sh4-200.  If execution time for sh4-200 is of concern,
+   a specially scheduled version makes sense.  */
+
+#define x_h r12
+#define yn  r3
+
+FUNC(GLOBAL(divdf3))
+ .global GLOBAL(divdf3)
+
+/* Adjust arg0 now, too.  We still have to come back to denorm_arg1_done,
+   since we heven't done any of the work yet that we do till the denorm_arg0
+   entry point.  We know that neither of the arguments is inf/nan, but
+   arg0 might be zero.  Check for that first to avoid having to establish an
+   rts return address.  */
+LOCAL(both_denorm):
+	mov.l	r9,@-r15
+	mov	DBL0H,r1
+	mov.l	r0,@-r15
+	shll2	r1
+	mov.w LOCAL(both_denorm_cleanup_off),r9
+	or	DBL0L,r1
+	tst	r1,r1
+	mov	DBL0H,r0
+	bf/s	LOCAL(zero_denorm_arg0_1)
+	shll2	r0
+	mov.l	@(4,r15),r9
+	add	#8,r15
+	bra	LOCAL(ret_inf_nan_0)
+	mov	r1,DBLRH
+
+LOCAL(both_denorm_cleanup):
+	mov.l	@r15+,r0
+	!
+	mov.l	@r15+,r9
+ mov	#12,r3
+ mov.l	LOCAL(xfffe2006),r1	! yn := (-1. << 17) + (0x80 << 6) ; shift #-26
+	bra	LOCAL(denorm_arg1_done)
+	!
+	add	r0,DBL0H
+
+/* Denorm handling leaves the incoming denorm argument with an exponent of +1
+   (implicit 1).  To leave the result exponent unaltered, the other
+   argument's exponent is adjusted by the the shift count.  */
+
+	.balign 4
+LOCAL(arg0_tiny):
+	bsr	LOCAL(clz)
+	mov	DBL0L,r0
+	shll	DBL0H
+	add	#1,r0
+	mov	DBL0L,DBL0H
+	shld	r0,DBL0H
+	rotcr	DBL0H
+	tst	DBL0L,DBL0L	/* Check for divide of zero.  */
+	add	#-33,r0
+	shld	r0,DBL0L
+	bf/s	LOCAL(adjust_arg1_exp)
+	add	#64,r0
+LOCAL(return_0): /* Return 0 with appropriate sign.  */
+	mov.l	@r15+,r10
+	mov	#0,DBLRH
+	mov.l	@r15+,r9
+	bra	LOCAL(ret_inf_nan_0)
+	mov.l	@r15+,r8
+
+	.balign 4
+LOCAL(arg1_tiny):
+	bsr	LOCAL(clz)
+	mov	DBL1L,r0
+	shll	DBL1H
+	add	#1,r0
+	mov	DBL1L,DBL1H
+	shld	r0,DBL1H
+	rotcr	DBL1H
+	tst	DBL1L,DBL1L	/* Check for divide by zero.  */
+	add	#-33,r0
+	shld	r0,DBL1L
+	bf/s	LOCAL(adjust_arg0_exp)
+	add	#64,r0
+	mov	DBL0H,r0
+	add	r0,r0
+	tst	r0,r0	! 0 / 0 ?
+	mov	#-1,DBLRH
+	bf	LOCAL(return_inf)
+	!
+	bt	LOCAL(ret_inf_nan_0)
+	!
+
+	.balign 4
+LOCAL(zero_denorm_arg1):
+	not	DBL0H,r3
+	mov	DBL1H,r0
+	tst	r2,r3
+	shll2	r0
+	bt	LOCAL(early_inf_nan_arg0)
+	tst	r0,r0
+	mov.w	LOCAL(xff00),r12
+	bt/s	LOCAL(arg1_tiny)
+	sts.l	pr,@-r15
+	bsr	LOCAL(clz)
+	shlr2	r0
+	!
+	shll	DBL1H
+	mov	DBL1L,r3
+	shld	r0,DBL1H
+	shld	r0,DBL1L
+	rotcr	DBL1H
+	add	#-32,r0
+	shld	r0,r3
+	add	#32,r0
+	or	r3,DBL1H
+LOCAL(adjust_arg0_exp):
+	tst	r2,DBL0H
+	mov	#20,r3
+	shld	r3,r0
+	bt	LOCAL(both_denorm)
+	add	DBL0H,r0
+	div0s	r0,DBL0H	! Check for obvious overflow.  */
+	not	r0,r3		! Check for more subtle overflow - lest
+	bt	LOCAL(return_inf)
+	mov	r0,DBL0H
+	tst	r2,r3		! we mistake it for NaN later
+	mov	#12,r3
+	bf	LOCAL(denorm_arg1_done)
+LOCAL(return_inf): /* Return infinity with appropriate sign.  */
+	mov	#20,r3
+	mov	#-2,DBLRH
+	bra	LOCAL(ret_inf_nan_0)
+	shad	r3,DBLRH
+
+/* inf/n -> inf; inf/0 -> inf; inf/inf -> nan; inf/nan->nan  nan/x -> nan */
+LOCAL(inf_nan_arg0):
+	mov.l	@r15+,r10
+	mov.l	@r15+,r9
+	mov.l	@r15+,r8
+LOCAL(early_inf_nan_arg0):
+	not	DBL1H,r3
+	mov	DBL0H,DBLRH
+	tst	r2,r3	! both inf/nan?
+	add	DBLRH,DBLRH
+	bf	LOCAL(ret_inf_nan_0)
+	mov	#-1,DBLRH
+LOCAL(ret_inf_nan_0):
+	mov	#0,DBLRL
+	mov.l	@r15+,r12
+	div0s	DBL0H,DBL1H
+	rts
+	rotcr	DBLRH
+	
+/* Already handled: inf/x, nan/x .  Thus: x/inf -> 0; x/nan -> nan */
+	.balign	4
+LOCAL(inf_nan_arg1):
+	mov	DBL1H,r2
+	mov	#12,r1
+	shld	r1,r2
+	mov.l	@r15+,r10
+	mov	#0,DBLRL
+	mov.l	@r15+,r9
+	or	DBL1L,r2
+	mov.l	@r15+,r8
+	cmp/hi	DBLRL,r2
+	mov.l	@r15+,r12
+	subc	DBLRH,DBLRH
+	div0s	DBL0H,DBL1H
+	rts
+	rotcr	DBLRH
+	
+	.balign 4
+LOCAL(zero_denorm_arg0):
+	mov.w	LOCAL(denorm_arg0_done_off),r9
+	not	DBL1H,r1
+	mov	DBL0H,r0
+	tst	r2,r1
+	shll2	r0
+	bt	LOCAL(inf_nan_arg1)
+LOCAL(zero_denorm_arg0_1):
+	tst	r0,r0
+	mov.w	LOCAL(xff00),r12
+	bt/s	LOCAL(arg0_tiny)
+	sts.l	pr,@-r15
+	bsr	LOCAL(clz)
+	shlr2	r0
+	shll	DBL0H
+	mov	DBL0L,r12
+	shld	r0,DBL0H
+	shld	r0,DBL0L
+	rotcr	DBL0H
+	add	#-32,r0
+	shld	r0,r12
+	add	#32,r0
+	or	r12,DBL0H
+LOCAL(adjust_arg1_exp):
+	mov	#20,r12
+	shld	r12,r0
+	add	DBL1H,r0
+	div0s	r0,DBL1H	! Check for obvious underflow.  */
+	not	r0,r12		! Check for more subtle underflow - lest
+	bt	LOCAL(return_0)
+	mov	r0,DBL1H
+	tst	r2,r12		! we mistake it for NaN later
+	bt	LOCAL(return_0)
+	!
+	braf	r9
+	mov	#13,r0
+LOCAL(zero_denorm_arg1_dispatch):
+
+LOCAL(xff00):	.word 0xff00
+LOCAL(denorm_arg0_done_off):
+	.word LOCAL(denorm_arg0_done)-LOCAL(zero_denorm_arg1_dispatch)
+LOCAL(both_denorm_cleanup_off):
+	.word LOCAL(both_denorm_cleanup)-LOCAL(zero_denorm_arg1_dispatch)
+
+ .balign	8
+GLOBAL(divdf3):
+ mov.l	LOCAL(x7ff00000),r2
+ mov	#12,r3
+ mov.l	LOCAL(xfffe2006),r1	! yn := (-1. << 17) + (0x80 << 6) ; shift #-26
+ tst	r2,DBL1H
+ mov.l	r12,@-r15
+ bt	LOCAL(zero_denorm_arg1)
+
+LOCAL(denorm_arg1_done):
+ mov	DBL1H,x_h	! x_h live in r12
+ shld	r3,x_h	! x - 1 ; u0.20
+ mov	x_h,yn
+ mova	LOCAL(ytab),r0
+ mov.l	r8,@-r15
+ shld	r1,yn	! x-1 ; u26.6
+ mov.b	@(r0,yn),yn
+ mov	#6,r0
+ mov.l	r9,@-r15
+ mov	x_h,r8
+ mov.l	r10,@-r15
+ shlr16	x_h	! x - 1; u16.16	! x/2 - 0.5 ; u15.17
+ add	x_h,r1	! SH4-200 single-issues this insn
+ shld	r0,yn
+ sub	r1,yn	! yn := y0 ; u15.17
+ mov	DBL1L,r1
+ mov	#-20,r10
+ mul.l	yn,x_h	! r12 dead
+ swap.w	yn,r9
+ shld	r10,r1
+ sts	macl,r0	! y0 * (x-1) - n ; u-1.32
+ add	r9,r0	! y0 * x - 1     ; s-1.32
+ tst	r2,DBL0H
+ dmuls.l r0,yn
+ mov.w	LOCAL(d13),r0
+ or	r1,r8	! x  - 1; u0.32
+ add	yn,yn	! yn = y0 ; u14.18
+ bt	LOCAL(zero_denorm_arg0)
+
+LOCAL(denorm_arg0_done):	! This label must stay aligned.
+ sts	mach,r1	!      d0 ; s14.18
+ sub	r1,yn	! yn = y1 ; u14.18 ; <= 0x3fffc
+ mov	DBL0L,r12
+ shld	r0,yn	! yn = y1 ; u1.31 ; <= 0x7fff8000
+ mov.w	LOCAL(d12),r9
+ dmulu.l yn,r8
+ shld	r10,r12
+ mov	yn,r0
+ mov	DBL0H,r8
+ add	yn,yn	! yn = y1 ; u0.32 ; <= 0xffff0000
+ sts	mach,r1	! y1 * (x-1); u1.31
+ add	r0,r1	! y1 * x    ; u1.31
+ dmulu.l yn,r1
+ not	DBL0H,r10
+ shld	r9,r8
+ tst	r2,r10
+ or	r8,r12	! a - 1; u0.32
+ bt	LOCAL(inf_nan_arg0)
+ sts	mach,r1	! d1+yn; u1.31
+ sett		! adjust y2 so that it can be interpreted as s1.31
+ not	DBL1H,r10
+ subc	r1,yn	! yn := y2 ; u1.31 ; can be 0x7fffffff
+ mov.l	LOCAL(x001fffff),r9
+ dmulu.l yn,r12
+ tst	r2,r10
+ or	DBL1H,r2
+ bt	LOCAL(inf_nan_arg1)
+ mov.l	r11,@-r15
+ sts	mach,r11	! y2*(a-1) ; u1.31
+ add	yn,r11		! z0       ; u1.31
+ dmulu.l r11,DBL1L
+ mov.l	LOCAL(x40000000),DBLRH	! bias + 1
+ and	r9,r2		! x ; u12.20
+ cmp/hi	DBL0L,DBL1L
+ sts	macl,r8
+ mov	#-24,r12
+ sts	mach,r9 	! r9:r8 := z0 * DBL1L; u-19.64
+ subc	DBL1H,DBLRH
+ mul.l	r11,r2  	! (r9+macl):r8 == z0*x; u-19.64
+ shll	r8
+ add	DBL0H,DBLRH	! result sign/exponent + 1
+ mov	r8,r10
+ sts	macl,DBLRL
+ add	DBLRL,r9
+ rotcl	r9		! r9:r8 := z*x; u-20.63
+ shld	r12,r10
+ mov.l	LOCAL(x7fe00000),DBLRL
+ sub	DBL0L,r9	! r9:r8 := -a ; u-20.63
+ mov.l	LOCAL(x00200000),r12
+FIXME: the following  shift might loose the sign.
+ shll8	r9
+ or	r10,r9	! -a1 ; s-28.32
+ mov.l	LOCAL(x00100000),r10
+ dmuls.l r9,yn	! r3 dead
+ mov	DBL1H,r3
+ mov.l LOCAL(xfff00000),DBL0L
+ xor	DBL0H,r3	! calculate expected sign & bit20
+ div0s	r3,DBLRH
+ xor	DBLRH,r3
+ bt	LOCAL(ret_denorm_inf)
+ tst	DBLRL,DBLRH
+ bt	LOCAL(ret_denorm)
+ sub	r12,DBLRH ! calculate sign / exponent minus implicit 1
+ tst	r10,r3	! set T if a >= x
+ sts	mach,r12! -z1 ; s-27.32
+ bt	0f
+ add	r11,r11	! z0 ; u1.31 / u0.31
+0: mov	#6,r3
+ negc	r3,r10 ! shift count := a >= x ? -7 : -6; T := 1
+ shll8	r8	! r9:r8 := -a1 ; s-28.64
+ shad	r10,r12	! -z1 ; truncate to s-20.32 / s-21.32
+ rotcl	r12	! -z1 ; s-21.32 / s-22.32 / round to odd 0.5 ulp ; T := sign
+ add	#20,r10
+ dmulu.l r12,DBL1L ! r12 signed, DBL1L unsigned
+ and	DBL0L,DBLRH	! isolate sign / exponent
+ shld	r10,r9
+ mov	r8,r3
+ shld	r10,r8
+ sts	macl,DBL0L
+ sts	mach,DBLRL
+ add	#-32,r10
+ shld	r10,r3
+ mul.l r12,r2
+ bf	0f	! adjustment for signed/unsigned multiply
+ sub	DBL1L,DBLRL	! DBL1L dead
+0: shar	r12	! -z1 ; truncate to s-20.32 / s-21.32
+ sts	macl,DBL1L
+ or	r3,r9	! r9:r8 := -a1 ;             s-41.64/s-42.64
+ !
+ cmp/hi	r8,DBL0L
+ add	DBLRL,DBL1L ! DBL1L:DBL0L := -z1*x ; s-41.64/s-42.64
+ subc	DBL1L,r9
+ not	r12,DBLRL ! z1, truncated to s-20.32 / s-21.32
+ shll	r9	! T :=  a2 > 0
+ mov	r11,r2
+ mov	#21,r7
+ shld	r7,r11
+ addc	r11,DBLRL
+ mov.l	@r15+,r11
+ mov.l	@r15+,r10
+ mov	#-11,r7
+ mov.l	@r15+,r9
+ shld	r7,r2
+ mov.l	@r15+,r8
+ addc	r2,DBLRH
+ rts
+ mov.l	@r15+,r12
+
+LOCAL(ret_denorm):
+	tst	r10,DBLRH
+	bra	LOCAL(denorm_have_count)
+	movt	DBLRH	! calculate shift count (off by 2)
+
+LOCAL(ret_denorm_inf):
+	mov	DBLRH,r12
+	add	r12,r12
+	cmp/pz	r12
+	mov	#-21,DBLRL
+	bt	LOCAL(ret_inf_late)
+	shld	DBLRL,DBLRH
+LOCAL(denorm_have_count):
+	add	#-2,DBLRH
+/* FIXME */
+	bra	LOCAL(return_0)
+	mov.l	@r15+,r11
+
+LOCAL(ret_inf_late):
+	mov.l	@r15+,r11
+	!
+	mov.l	@r15+,r10
+	!
+	mov.l	@r15+,r9
+	bra	LOCAL(return_inf)
+	mov.l	@r15+,r8
+
+	.balign	4
+LOCAL(clz):
+	mov.l	r8,@-r15
+	extu.w	r0,r8
+	mov.l	r9,@-r15
+	cmp/eq	r0,r8
+	bt/s	0f
+	mov	#8-11,r9
+	xtrct	r0,r8
+	add	#16,r9
+0:	tst	r12,r8	! 0xff00
+	mov.l	LOCAL(c_clz_tab),r0
+	bt	0f
+	shlr8	r8
+0:	bt	0f
+	add	#8,r9
+0:
+#ifdef	__PIC__
+	add	r0,r8
+	mova	LOCAL(c_clz_tab),r0
+#endif
+	mov.b	@(r0,r8),r8
+	mov	r9,r0
+	mov.l	@r15+,r9
+	!
+	!
+	!
+	sub	r8,r0
+	mov.l	@r15+,r8
+	rts
+	lds.l	@r15+,pr
+
+!	We encode even some words as pc-relative that would fit as immediate
+!	in the instruction in order to avoid some pipeline stalls on
+!	SH4-100 / SH4-200.
+LOCAL(d1):	.word 1
+LOCAL(d12):	.word 12
+LOCAL(d13):	.word 13
+
+	.balign 4
+LOCAL(x7ff00000): .long 0x7ff00000
+LOCAL(xfffe2006): .long 0xfffe2006
+LOCAL(x001fffff): .long 0x001fffff
+LOCAL(x40000000): .long 0x40000000
+LOCAL(x7fe00000): .long 0x7fe00000
+LOCAL(x00100000): .long 0x00100000
+LOCAL(x00200000): .long 0x00200000
+LOCAL(xfff00000): .long 0xfff00000
+LOCAL(c_clz_tab):
+#ifdef __pic__
+        .long   GLOBAL(clz_tab) - .
+#else
+        .long   GLOBAL(clz_tab)
+#endif
+LOCAL(ytab):
+        .byte   120, 105,  91,  78,  66,  54,  43,  33
+        .byte    24,  15,   8,   0,  -5, -12, -17, -22
+        .byte   -27, -31, -34, -37, -40, -42, -44, -45
+        .byte   -46, -46, -47, -46, -46, -45, -44, -42
+        .byte   -41, -39, -36, -34, -31, -28, -24, -20
+        .byte   -17, -12,  -8,  -4,   0,   5,  10,  16
+        .byte    21,  27,  33,  39,  45,  52,  58,  65
+        .byte    72,  79,  86,  93, 101, 109, 116, 124
+ENDFUNC(GLOBAL(divdf3))
diff -Naur gcc-4.2.1/gcc/config/sh/IEEE-754/m3/divdf3.S gcc-4.2.1-stm/gcc/config/sh/IEEE-754/m3/divdf3.S
--- gcc-4.2.1/gcc/config/sh/IEEE-754/m3/divdf3.S	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/IEEE-754/m3/divdf3.S	2007-08-10 13:08:59.000000000 +0100
@@ -0,0 +1,608 @@
+/* Copyright (C) 2004, 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+! divdf3 for the Renesas SH / STMicroelectronics ST40 CPUs.
+! Contributed by Joern Rennecke joern.rennecke@st.com
+
+/* y = 1/x  ; x (- [1,2)
+   y0 = 1.5 - x/2 - tab[(1-x)*64] = y + d ; abs(d)/y <= 0x1.0c/256
+
+   y1 = y0 - ((y0) * x - 1) * y0  =  y-x*d^2
+   y2 = y1 - ((y1) * x - 1) * y1 =~= y-x^3*d^4
+
+   z0 = y2*a ;  a1 = a - z0*x /# 32 * 64 -> 64 bit #/
+   z1 = y2*a1 (round to nearest odd 0.5 ulp);
+   a2 = a1 - z1*x /# 32 * 64 -> 64 bit #/
+
+   z = a/x = z0 + z1 - 0.5 ulp + (a2 > 0) * ulp
+
+   Unless stated otherwise, multiplies can be done in 32 * 32 bit or less
+   with suitable scaling and/or top truncation.
+   We use a slightly modified algorithm here that checks if the lower
+   bits in z1 are sufficient to determine the outcome of rounding - in that
+   case a2 is not computed.
+   -z1 is computed in units of 1/128 ulp, with an error in the range
+   -0x3.e/128 .. +0 ulp.
+   Thus, after adding three, the result can be safely rounded for normal
+   numbers if any of the bits 5..2 is set, or if the highest guard bit
+   (bit 6 if y <1, otherwise bit 7) is set.
+   (Because of the way truncation works, we would be fine for an open
+    error interval of (-4/128..+1/128) ulp )
+   For denormal numbers, the rounding point lies higher, but it would be
+   quite cumbersome to calculate where exactly; it is sufficient if any
+   of the bits 7..3 is set.
+   x truncated to 20 bits is sufficient to calculate y0 or even y1.
+   Table entries are adjusted by about +128 to use full signed byte range.
+   This adjustment has been perturbed slightly to allow cse with the
+   shift count constant -26.
+   The threshold point for the shift adjust before rounding is found by
+   comparing the fractions, which is exact, unlike the top bit of y2.
+   Therefore, the top bit of y2 becomes slightly random after the adjustment
+   shift, but that's OK because this can happen only at the boundaries of
+   the interval, and the biasing of the error means that it can in fact happen
+   only at the bottom end.  And there, the carry propagation will make sure
+   that in the end we will have in effect an implicit 1 (or two whem rounding
+   up...)  */
+/* If an exact result exists, it can have no more bits than the divident.
+   Hence, we don't need to bother with the round-to-even tie breaker
+   unless the result is denormalized.  */
+/* 64 cycles through main path for sh4-300 (about 93.7% of normalized numbers),
+   82 for the path for rounding tie-breaking for normalized numbers
+   (including one branch mispredict).
+   Some cycles might be saved by more careful register allocation.  */
+
+#define x_h r12
+#define yn  r3
+
+FUNC(GLOBAL(divdf3))
+ .global GLOBAL(divdf3)
+
+/* Adjust arg0 now, too.  We still have to come back to denorm_arg1_done,
+   since we heven't done any of the work yet that we do till the denorm_arg0
+   entry point.  We know that neither of the arguments is inf/nan, but
+   arg0 might be zero.  Check for that first to avoid having to establish an
+   rts return address.  */
+LOCAL(both_denorm):
+	mov.l	r9,@-r15
+	mov	DBL0H,r1
+	mov.l	r0,@-r15
+	shll2	r1
+	mov.w LOCAL(both_denorm_cleanup_off),r9
+	or	DBL0L,r1
+	tst	r1,r1
+	mov	DBL0H,r0
+	bf/s	LOCAL(zero_denorm_arg0_1)
+	shll2	r0
+	mov.l	@(4,r15),r9
+	add	#8,r15
+	bra	LOCAL(ret_inf_nan_0)
+	mov	r1,DBLRH
+
+LOCAL(both_denorm_cleanup):
+	mov.l	@r15+,r0
+	!
+	mov.l	@r15+,r9
+ mov	#12,r3
+ mov.l	LOCAL(xfffe2006),r1	! yn := (-1. << 17) + (0x80 << 6) ; shift #-26
+	bra	LOCAL(denorm_arg1_done)
+	!
+	add	r0,DBL0H
+
+/* Denorm handling leaves the incoming denorm argument with an exponent of +1
+   (implicit 1).  To leave the result exponent unaltered, the other
+   argument's exponent is adjusted by the the shift count.  */
+
+	.balign 4
+LOCAL(arg0_tiny):
+	bsr	LOCAL(clz)
+	mov	DBL0L,r0
+	shll	DBL0H
+	add	#1,r0
+	mov	DBL0L,DBL0H
+	shld	r0,DBL0H
+	rotcr	DBL0H
+	tst	DBL0L,DBL0L	/* Check for divide of zero.  */
+	add	#-33,r0
+	shld	r0,DBL0L
+	bf/s	LOCAL(adjust_arg1_exp)
+	add	#64,r0
+LOCAL(return_0): /* Return 0 with appropriate sign.  */
+	mov.l	@r15+,r10
+	mov	#0,DBLRH
+	mov.l	@r15+,r9
+	bra	LOCAL(ret_inf_nan_0)
+	mov.l	@r15+,r8
+
+	.balign 4
+LOCAL(arg1_tiny):
+	bsr	LOCAL(clz)
+	mov	DBL1L,r0
+	shll	DBL1H
+	add	#1,r0
+	mov	DBL1L,DBL1H
+	shld	r0,DBL1H
+	rotcr	DBL1H
+	tst	DBL1L,DBL1L	/* Check for divide by zero.  */
+	add	#-33,r0
+	shld	r0,DBL1L
+	bf/s	LOCAL(adjust_arg0_exp)
+	add	#64,r0
+	mov	DBL0H,r0
+	add	r0,r0
+	tst	r0,r0	! 0 / 0 ?
+	mov	#-1,DBLRH
+	bf	LOCAL(return_inf)
+	!
+	bt	LOCAL(ret_inf_nan_0)
+	!
+
+	.balign 4
+LOCAL(zero_denorm_arg1):
+	not	DBL0H,r3
+	mov	DBL1H,r0
+	tst	r2,r3
+	shll2	r0
+	bt	LOCAL(early_inf_nan_arg0)
+	tst	r0,r0
+	mov.w	LOCAL(xff00),r12
+	bt/s	LOCAL(arg1_tiny)
+	sts.l	pr,@-r15
+	bsr	LOCAL(clz)
+	shlr2	r0
+	!
+	shll	DBL1H
+	mov	DBL1L,r3
+	shld	r0,DBL1H
+	shld	r0,DBL1L
+	rotcr	DBL1H
+	add	#-32,r0
+	shld	r0,r3
+	add	#32,r0
+	or	r3,DBL1H
+LOCAL(adjust_arg0_exp):
+	tst	r2,DBL0H
+	mov	#20,r3
+	shld	r3,r0
+	bt	LOCAL(both_denorm)
+	add	DBL0H,r0
+	div0s	r0,DBL0H	! Check for obvious overflow.  */
+	not	r0,r3		! Check for more subtle overflow - lest
+	bt	LOCAL(return_inf)
+	mov	r0,DBL0H
+	tst	r2,r3		! we mistake it for NaN later
+	mov	#12,r3
+	bf	LOCAL(denorm_arg1_done)
+LOCAL(return_inf): /* Return infinity with appropriate sign.  */
+	mov	#20,r3
+	mov	#-2,DBLRH
+	bra	LOCAL(ret_inf_nan_0)
+	shad	r3,DBLRH
+
+/* inf/n -> inf; inf/0 -> inf; inf/inf -> nan; inf/nan->nan  nan/x -> nan */
+LOCAL(inf_nan_arg0):
+	mov.l	@r15+,r10
+	mov.l	@r15+,r9
+	mov.l	@r15+,r8
+LOCAL(early_inf_nan_arg0):
+	not	DBL1H,r3
+	mov	DBL0H,DBLRH
+	tst	r2,r3	! both inf/nan?
+	add	DBLRH,DBLRH
+	bf	LOCAL(ret_inf_nan_0)
+	mov	#-1,DBLRH
+LOCAL(ret_inf_nan_0):
+	mov	#0,DBLRL
+	mov.l	@r15+,r12
+	div0s	DBL0H,DBL1H
+	rts
+	rotcr	DBLRH
+	
+/* Already handled: inf/x, nan/x .  Thus: x/inf -> 0; x/nan -> nan */
+	.balign	4
+LOCAL(inf_nan_arg1):
+	mov	DBL1H,r2
+	mov	#12,r1
+	shld	r1,r2
+	mov.l	@r15+,r10
+	mov	#0,DBLRL
+	mov.l	@r15+,r9
+	or	DBL1L,r2
+	mov.l	@r15+,r8
+	cmp/hi	DBLRL,r2
+	mov.l	@r15+,r12
+	subc	DBLRH,DBLRH
+	div0s	DBL0H,DBL1H
+	rts
+	rotcr	DBLRH
+	
+	.balign 4
+LOCAL(zero_denorm_arg0):
+	mov.w	LOCAL(denorm_arg0_done_off),r9
+	not	DBL1H,r1
+	mov	DBL0H,r0
+	tst	r2,r1
+	shll2	r0
+	bt	LOCAL(inf_nan_arg1)
+LOCAL(zero_denorm_arg0_1):
+	tst	r0,r0
+	mov.w	LOCAL(xff00),r12
+	bt/s	LOCAL(arg0_tiny)
+	sts.l	pr,@-r15
+	bsr	LOCAL(clz)
+	shlr2	r0
+	shll	DBL0H
+	mov	DBL0L,r12
+	shld	r0,DBL0H
+	shld	r0,DBL0L
+	rotcr	DBL0H
+	add	#-32,r0
+	shld	r0,r12
+	add	#32,r0
+	or	r12,DBL0H
+LOCAL(adjust_arg1_exp):
+	mov	#20,r12
+	shld	r12,r0
+	add	DBL1H,r0
+	div0s	r0,DBL1H	! Check for obvious underflow.  */
+	not	r0,r12		! Check for more subtle underflow - lest
+	bt	LOCAL(return_0)
+	mov	r0,DBL1H
+	tst	r2,r12		! we mistake it for NaN later
+	bt	LOCAL(return_0)
+	!
+	braf	r9
+	mov	#13,r0
+LOCAL(zero_denorm_arg1_dispatch):
+
+LOCAL(xff00):	.word 0xff00
+LOCAL(denorm_arg0_done_off):
+	.word LOCAL(denorm_arg0_done)-LOCAL(zero_denorm_arg1_dispatch)
+LOCAL(both_denorm_cleanup_off):
+	.word LOCAL(both_denorm_cleanup)-LOCAL(zero_denorm_arg1_dispatch)
+
+ .balign	8
+GLOBAL(divdf3):
+ mov.l	LOCAL(x7ff00000),r2
+ mov	#12,r3
+ mov.l	LOCAL(xfffe2006),r1	! yn := (-1. << 17) + (0x80 << 6) ; shift #-26
+ tst	r2,DBL1H
+ mov.l	r12,@-r15
+ bt	LOCAL(zero_denorm_arg1)
+
+LOCAL(denorm_arg1_done):
+ mov	DBL1H,x_h	! x_h live in r12
+ shld	r3,x_h	! x - 1 ; u0.20
+ mov	x_h,yn
+ mova	LOCAL(ytab),r0
+ mov.l	r8,@-r15
+ shld	r1,yn	! x-1 ; u26.6
+ mov.b	@(r0,yn),yn
+ mov	#6,r0
+ mov.l	r9,@-r15
+ mov	x_h,r8
+ mov.l	r10,@-r15
+ shlr16	x_h	! x - 1; u16.16	! x/2 - 0.5 ; u15.17
+ add	x_h,r1	! SH4-200 single-issues this insn
+ shld	r0,yn
+ sub	r1,yn	! yn := y0 ; u15.17
+ mov	DBL1L,r1
+ mov	#-20,r10
+ mul.l	yn,x_h	! r12 dead
+ swap.w	yn,r9
+ shld	r10,r1
+ sts	macl,r0	! y0 * (x-1) - n ; u-1.32
+ add	r9,r0	! y0 * x - 1     ; s-1.32
+ tst	r2,DBL0H
+ dmuls.l r0,yn
+ mov.w	LOCAL(d13),r0
+ or	r1,r8	! x  - 1; u0.32
+ add	yn,yn	! yn = y0 ; u14.18
+ bt	LOCAL(zero_denorm_arg0)
+
+LOCAL(denorm_arg0_done):
+ sts	mach,r1	!      d0 ; s14.18
+ sub	r1,yn	! yn = y1 ; u14.18 ; <= 0x3fffc
+ mov	DBL0L,r12
+ shld	r0,yn	! yn = y1 ; u1.31 ; <= 0x7fff8000
+ mov.w	LOCAL(d12),r9
+ dmulu.l yn,r8
+ shld	r10,r12
+ mov	yn,r0
+ mov	DBL0H,r8
+ add	yn,yn	! yn = y1 ; u0.32 ; <= 0xffff0000
+ sts	mach,r1	! y1 * (x-1); u1.31
+ add	r0,r1	! y1 * x    ; u1.31
+ dmulu.l yn,r1
+ not	DBL0H,r10
+ shld	r9,r8
+ tst	r2,r10
+ or	r8,r12	! a - 1; u0.32
+ bt	LOCAL(inf_nan_arg0)
+ sts	mach,r1	! d1+yn; u1.31
+ sett		! adjust y2 so that it can be interpreted as s1.31
+ not	DBL1H,r10
+ subc	r1,yn	! yn := y2 ; u1.31 ; can be 0x7fffffff
+ mov.l	LOCAL(x001fffff),r9
+ dmulu.l yn,r12
+ tst	r2,r10
+ or	DBL1H,r2
+ bt	LOCAL(inf_nan_arg1)
+ mov.l	r11,@-r15
+ sts	mach,r12	! y2*(a-1) ; u1.31
+ add	yn,r12		! z0       ; u1.31
+ dmulu.l r12,DBL1L
+ mov.l	LOCAL(x40000000),DBLRH ! bias + 1
+ and	r9,r2		! x ; u12.20
+ cmp/hi	DBL0L,DBL1L
+ sts	macl,r8
+ mov	#-24,r11
+ sts	mach,r9 	! r9:r8 := z0 * DBL1L; u-19.64
+ subc	DBL1H,DBLRH
+ mul.l	r12,r2  	! (r9+macl):r8 == z0*x; u-19.64
+ shll	r8
+ add	DBL0H,DBLRH	! result sign/exponent + 1
+ mov	r8,r10
+ sts	macl,DBLRL
+ add	DBLRL,r9
+ rotcl	r9		! r9:r8 := z*x; u-20.63
+ shld	r11,r10
+ mov.l	LOCAL(x7fe00000),DBLRL
+ sub	DBL0L,r9	! r9:r8 := -a ; u-20.63
+ cmp/pz	r9		! In corner cases this shift can loose ..
+ shll8	r9		!  .. the sign, so check it first.
+ mov.l	LOCAL(x00200000),r11
+ or	r10,r9	! -a1 ; s-28.32
+ mov.l	LOCAL(x00100000),r10
+ dmulu.l r9,yn	! sign for r9 is in T
+ xor	DBL0H,DBL1H	! calculate expected sign & bit20
+ mov.w	LOCAL(d120),DBL0H ! to test bits 6..4
+ xor	DBLRH,DBL1H
+ !
+ sts	mach,DBL0L	! -z1 ; s-27.32
+ bt 0f
+ sub	yn,DBL0L	! multiply adjust for -a1 negative; r3 dies here
+0:tst	r10,DBL1H		! set T if a >= x
+ mov.l LOCAL(xfff00000),r3
+ bt	0f
+ add	DBL0L,DBL0L	! z1 ; s-27.32 / s-28.32
+0:bt 0f
+ add	r12,r12	! z0 ; u1.31 / u0.31
+0:add	#6-64,DBL0L
+ and	r3,DBLRH	! isolate sign / exponent
+ tst	DBL0H,DBL0L
+ bf/s	LOCAL(exact)	! make the hot path taken for best branch prediction
+ cmp/pz	DBL1H
+
+! Unless we follow the next branch, we need to test which way the rounding
+! should go.
+! For normal numbers, we know that the result is not exact, so the sign
+! of the rest will be conclusive.
+! We generate a number that looks safely rounded so that denorm handling
+! can safely test the number twice.
+! r10:r8 == 0 will indicate if the number was exact, which can happen
+! when we come here for denormals to check a number that is close or
+! equal to a result in whole ulps.
+ bf	LOCAL(ret_denorm_inf)	! denorm or infinity, DBLRH has inverted sign
+ add	#64,DBL0L
+LOCAL(find_adjust): tst	r10,DBL1H ! set T if a >= x
+ mov	#-2,r10
+ addc	r10,r10
+ mov	DBL0L,DBLRL	! z1 ; s-27.32 / s-28.32 ; lower 4 bits unsafe.
+ shad	r10,DBLRL	! tentatively rounded z1 ; s-24.32
+ shll8	r8		! r9:r8 := -a1 ; s-28.64
+ clrt
+ dmuls.l DBLRL,DBL1L	! DBLRL signed, DBL1L unsigned
+ mov	r8,r10
+ shll16	r8		! r8  := lowpart  of -a1 ; s-44.48
+ xtrct	r9,r10		! r10 := highpart of -a1 ; s-44.48
+ !
+ sts	macl,r3
+ subc	r3,r8
+ sts	mach,r3
+ subc	r3,r10
+ cmp/pz	DBL1L
+ mul.l	DBLRL,r2
+ bt	0f
+ sub	DBLRL,r10	! adjust for signed/unsigned multiply
+0: mov.l	LOCAL(x7fe00000),DBLRL
+ mov	#-26,r2
+ sts	macl,r9
+ sub	r9,r10		! r10:r8 := -a2
+ add	#-64+16,DBL0L	! the denorm code negates this adj. for exact results
+ shld	r2,r10		! convert sign into adjustment in the range 32..63
+ sub	r10,DBL0L
+ cmp/pz	DBL1H
+
+ .balign 4
+LOCAL(exact):
+ bf	LOCAL(ret_denorm_inf)	! denorm or infinity, DBLRH has inverted sign
+ tst	DBLRL,DBLRH
+ bt	LOCAL(ret_denorm_inf)	! denorm, DBLRH has correct sign
+ mov	#-7,DBL1H
+ cmp/pz	DBL0L		! T is sign extension of z1
+ not	DBL0L,DBLRL
+ subc	r11,DBLRH	! calculate sign / exponent minus implicit 1 minus T
+ mov.l	@r15+,r11
+ mov.l	@r15+,r10
+ shad	DBL1H,DBLRL
+ mov.l	@r15+,r9
+ mov	#-11,DBL1H
+ mov	r12,r8		! z0 contributes to DBLRH and DBLRL
+ shld	DBL1H,r12
+ mov	#21,DBL1H
+ clrt
+ shld	DBL1H,r8
+ addc	r8,DBLRL
+ mov.l	@r15+,r8
+ addc	r12,DBLRH
+ rts
+ mov.l	@r15+,r12
+
+!	sign in DBLRH ^ DBL1H
+! If the last 7 bits are in the range 64..64+7, we might have an exact
+! value in the preceding bits - or we might not. For denorms, we need to
+! find out.
+! if r10:r8 is zero, we just have found out that there is an exact value.
+	.balign	4
+LOCAL(ret_denorm_inf):
+	mov	DBLRH,r3
+	add	r3,r3
+	div0s	DBL1H,r3
+	mov	#120,DBLRL
+	bt	LOCAL(ret_inf_late)
+	add	#64,DBL0L
+	tst	DBLRL,DBL0L
+	mov	#-21,DBLRL
+	bt	LOCAL(find_adjust)
+	or	r10,r8
+	tst	r8,r8		! check if find_adjust found an exact value.
+	shad	DBLRL,r3
+	bf	0f
+	add	#-16,DBL0L	! if yes, cancel adjustment
+0:	mov	#-8,DBLRL	! remove the three lowest (inexact) bits
+	and	DBLRL,DBL0L
+	add	#-2-11,r3	! shift count for denorm generation
+	mov	DBL0L,DBLRL
+	mov	#28,r2
+	mov.l	@r15+,r11
+	mov.l	@r15+,r10
+	shll2	DBLRL
+	mov.l	@r15+,r9
+	shld	r2,DBL0L
+	mov.l	@r15+,r8
+	mov	#-31,r2
+	cmp/ge	r2,r3
+	shll2	DBLRL
+	bt/s	0f
+	add	DBL0L,r12	! fraction in r12:DBLRL ; u1.63
+	negc	DBLRL,DBLRL	! T := DBLRL != 0
+	add	#31,r3
+	mov	r12,DBLRL
+	rotcl	DBLRL		! put in sticky bit
+	movt	r12
+	cmp/ge	r2,r3
+	bt/s	LOCAL(return_0_late)
+0:	div0s	DBL1H,DBLRH	! calculate sign
+	mov	r12,DBLRH
+	shld	r3,DBLRH
+	mov	DBLRL,r2
+	shld	r3,DBLRL
+	add	#32,r3
+	add	DBLRH,DBLRH
+	mov.l	LOCAL(x80000000),DBL1H
+	shld	r3,r12
+	rotcr	DBLRH		! combine sign with highpart
+	add	#-1,r3
+	shld	r3,r2
+	mov	#0,r3
+	rotl	r2
+	cmp/hi	DBL1H,r2
+	addc	r12,DBLRL
+	mov.l	@r15+,r12
+	rts
+	addc	r3,DBLRH
+
+LOCAL(ret_inf_late):
+	mov.l	@r15+,r11
+	mov.l	@r15+,r10
+	mov	DBLRH,DBL0H
+	mov.l	@r15+,r9
+	bra	LOCAL(return_inf)
+	mov.l	@r15+,r8
+
+LOCAL(return_0_late):
+	div0s	DBLRH,DBL1H
+	mov.l	@r15+,r12
+	mov	#0,DBLRH
+	rts
+	rotcr	DBLRH
+
+	.balign	4
+LOCAL(clz):
+	mov.l	r8,@-r15
+	extu.w	r0,r8
+	mov.l	r9,@-r15
+	cmp/eq	r0,r8
+	bt/s	0f
+	mov	#21,r9
+	xtrct	r0,r8
+	add	#-16,r9
+0:	tst	r12,r8	! 0xff00
+	mov.l	LOCAL(c_clz_tab),r0
+	bt	0f
+	shlr8	r8
+0:	bt	0f
+	add	#-8,r9
+0:
+#ifdef	__PIC__
+	add	r0,r8
+	mova	LOCAL(c_clz_tab),r0
+#endif
+	mov.b	@(r0,r8),r8
+	mov	r9,r0
+	mov.l	@r15+,r9
+	!
+	!
+	!
+	sub	r8,r0
+	mov.l	@r15+,r8
+	rts
+	lds.l	@r15+,pr
+
+!	We encode even some words as pc-relative that would fit as immediate
+!	in the instruction in order to avoid some pipeline stalls on
+!	SH4-100 / SH4-200.
+LOCAL(d1):	.word 1
+LOCAL(d12):	.word 12
+LOCAL(d13):	.word 13
+LOCAL(d120):	.word 120
+
+	.balign 4
+LOCAL(x7ff00000): .long 0x7ff00000
+LOCAL(xfffe2006): .long 0xfffe2006
+LOCAL(x001fffff): .long 0x001fffff
+LOCAL(x40000000): .long 0x40000000
+LOCAL(x7fe00000): .long 0x7fe00000
+LOCAL(x00100000): .long 0x00100000
+LOCAL(x00200000): .long 0x00200000
+LOCAL(xfff00000): .long 0xfff00000
+LOCAL(x80000000): .long 0x80000000
+LOCAL(c_clz_tab):
+#ifdef __pic__
+        .long   GLOBAL(clz_tab) - .
+#else
+        .long   GLOBAL(clz_tab)
+#endif
+LOCAL(ytab):
+        .byte   120, 105,  91,  78,  66,  54,  43,  33
+        .byte    24,  15,   8,   0,  -5, -12, -17, -22
+        .byte   -27, -31, -34, -37, -40, -42, -44, -45
+        .byte   -46, -46, -47, -46, -46, -45, -44, -42
+        .byte   -41, -39, -36, -34, -31, -28, -24, -20
+        .byte   -17, -12,  -8,  -4,   0,   5,  10,  16
+        .byte    21,  27,  33,  39,  45,  52,  58,  65
+        .byte    72,  79,  86,  93, 101, 109, 116, 124
+ENDFUNC(GLOBAL(divdf3))
diff -Naur gcc-4.2.1/gcc/config/sh/IEEE-754/m3/divsf3.S gcc-4.2.1-stm/gcc/config/sh/IEEE-754/m3/divsf3.S
--- gcc-4.2.1/gcc/config/sh/IEEE-754/m3/divsf3.S	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/IEEE-754/m3/divsf3.S	2007-08-10 13:08:59.000000000 +0100
@@ -0,0 +1,365 @@
+/* Copyright (C) 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+! divsf3 for the Renesas SH / STMicroelectronics ST40 CPUs.
+! Contributed by Joern Rennecke
+! joern.rennecke@st.com
+!
+! This code is optimized for SH4 without FPU, but can also be used for SH3.
+
+! long 0th..3rd significant byte
+#ifdef __LITTLE_ENDIAN__
+#define L0SB	3
+#define L1SB	2
+#define L2SB	1
+#define L3SB	0
+#else
+#define L0SB	0
+#define L1SB	1
+#define L2SB	2
+#define L3SB	3
+#endif
+
+! clobbered: r0,r1,r2,r3,r6,r7,T (and for sh.md's purposes PR)
+!
+! Note: When the divisor is larger than the divident, we have to adjust the
+! exponent down by one.  We do this automatically when subtracting the entire
+! exponent/fraction bitstring as an integer, by means of the borrow from
+! bit 23 to bit 24.
+! Note: non-denormal rounding of a division result cannot cause fraction
+! overflow / exponent change. (r4 > r5 : fraction must stay in (2..1] interval;
+! r4 < r5: having an extra bit of precision available, even the smallest
+! possible difference of the result from one is rounded in all rounding modes
+! to a fraction smaller than one.)
+! sh4-200: 59 cycles
+! sh4-300: 44 cycles
+! tab indent: exponent / sign computations
+! tab+space indent: fraction computation
+FUNC(GLOBAL(divsf3))
+	.global GLOBAL(divsf3)
+	.balign	4
+GLOBAL(divsf3):
+	mov.l	LOCAL(x7f800000),r3
+	mov	#1,r2
+	mov	r4,r6
+	 shll8	 r6
+	mov	r5,r7
+	 shll8	 r7
+	rotr	r2
+	tst	r3,r4
+	or	r2,r6
+	bt/s	LOCAL(denorm_arg0)
+	or	r2,r7
+	tst	r3,r5
+	bt	LOCAL(denorm_arg1)
+	 shlr	 r6
+	mov.l	LOCAL(x3f000000),r3	! bias minus explict leading 1
+	 div0u
+LOCAL(denorm_done):
+	 div1	 r7,r6
+	mov.l	r8,@-r15
+	 bt	 0f
+	 div1	r7,r6
+0:	mov.l	r9,@-r15
+	 div1	 r7,r6
+	add	r4,r3
+	 div1	 r7,r6
+	sub	r5,r3	! result sign/exponent minus 1 if no overflow/underflow
+	 div1	 r7,r6
+	or	r3,r2
+	 div1	 r7,r6
+	mov.w	LOCAL(xff00),r9
+	 div1	 r7,r6
+	mov.l	r2,@-r15 ! L0SB is 0xff iff denorm / infinity exp is computed
+	 div1	 r7,r6
+	mov.w	LOCAL(m23),r2
+	 div1	 r7,r6
+	mov	r4,r0
+	 div1	 r7,r6
+	 extu.b	 r6,r1
+	 and	 r9,r6
+	 swap.w	 r1,r1	! first 8 bits of result fraction in bit 23..16
+	 div1	 r7,r6
+	shld	r2,r0
+	 div1	 r7,r6
+	mov.b	r0,@(L3SB,r15)	! 0xff iff divident was infinity / nan
+	 div1	 r7,r6
+	mov	r5,r0
+	 div1	 r7,r6
+	shld	r2,r0
+	 div1	 r7,r6
+	mov.b	r0,@(L2SB,r15)	! 0xff iff divisor was infinity / nan
+	 div1	 r7,r6
+	mov	r4,r0
+	 div1	 r7,r6
+	mov.w	LOCAL(m31),r2
+	 div1	 r7,r6
+	 extu.b	 r6,r8	! second 8 bits of result fraction in bit 7..0
+	 and	 r9,r6
+	mov.l	LOCAL(xff800000),r9
+	 div1	 r7,r6
+	xor	r5,r0	! msb := correct result sign
+	 div1	 r7,r6
+	xor	r3,r0	! xor with sign of result sign/exponent word
+	 div1	 r7,r6
+	shad	r2,r0
+	 div1	 r7,r6
+	mov.b	r0,@(L1SB,r15)	! 0xff	iff exponent over/underflows
+	and	r9,r3	! isolate sign / exponent
+	 mov.w	 LOCAL(xff01),r2
+	 div1	 r7,r6
+	swap.b	r8,r0	! second 8 bits of result fraction in bit 15..8
+	 div1	 r7,r6
+	or	r1,r0	! first 16 bits of result fraction in bit 23..8
+	 div1	 r7,r6
+	mov.w	LOCAL(m1),r9
+	 div1	 r7,r6
+	mov.l	@r15+,r8 ! load encoding of unusal exponent conditions
+	 and	 r6,r2	! rest | result lsb
+	 mov	 #0,r1
+	 bf	 0f	! bit below lsb clear -> no rounding
+	 cmp/hi	r1,r2
+0:	 extu.b	 r6,r1
+	 or	 r1,r0	! 24 bit result fraction with explicit leading 1
+	addc	r3,r0	! add in exponent / sign
+	cmp/str	r9,r8
+	! (no stall *here* for SH4-100 / SH4-200)
+	bt/s	LOCAL(inf_nan_denorm_zero)
+	mov.l	@r15+,r9
+	rts
+	mov.l	@r15+,r8
+
+/* The exponennt adjustment for denormal numbers is done by leaving an
+   adjusted value in r3; r4/r5 are not changed.  */
+	.balign	4
+LOCAL(denorm_arg0):
+	mov.w	LOCAL(xff00),r1
+	sub	r2,r6	! 0x800000000 : remove implict 1
+	tst	r6,r6
+	sts.l	pr,@-r15
+	bt	LOCAL(div_zero)
+	bsr	LOCAL(clz)
+	mov	r6,r0
+	shld	r0,r6
+	tst	r3,r5
+	mov.l	LOCAL(x3f800000),r3	! bias - 1 + 1
+	mov	#23,r1
+	shld	r1,r0
+	bt/s	LOCAL(denorm_arg1_2)
+	sub	r0,r3
+	 shlr	 r6
+	bra	LOCAL(denorm_done)
+	 div0u
+
+LOCAL(denorm_arg1):
+	mov.l	LOCAL(x3f000000),r3	! bias - 1
+LOCAL(denorm_arg1_2):
+	sub	r2,r7	! 0x800000000 : remove implict 1
+	mov.w	LOCAL(xff00),r1
+	tst	r7,r7
+	sts.l	pr,@-r15
+	bt	LOCAL(div_by_zero)
+	bsr	LOCAL(clz)
+	mov	r7,r0
+	shld	r0,r7
+	add	#-1,r0
+	mov	#23,r1
+	shld	r1,r0
+	add	r0,r3
+	 shlr	 r6
+	bra	LOCAL(denorm_done)
+	 div0u
+
+	.balign	4
+LOCAL(inf_nan_denorm_zero):
+! r0 has the rounded result, r6 has the non-rounded lowest bits & rest.
+! the bit just below the LSB of r6 is available as ~Q
+
+! Alternative way to get at ~Q:
+! if rounding took place, ~Q must be set.
+! if the rest appears to be zero, ~Q must be set.
+! if the rest appears to be nonzero, but rounding didn't take place,
+! ~Q must be clear;  the apparent rest will then require adjusting to test if 
+! the actual rest is nonzero.
+	mov	r0,r2
+	not	r8,r0
+	tst	#0xff,r0
+	shlr8	r0
+	mov.l	@r15+,r8
+	bt/s	LOCAL(div_inf_or_nan)
+	tst	#0xff,r0
+	mov	r4,r0
+	bt	LOCAL(div_by_inf_or_nan)
+	add	r0,r0
+	mov	r5,r1
+	add	r1,r1
+	cmp/hi	r1,r0
+	mov	r6,r0
+	bt	LOCAL(overflow)
+	sub	r2,r0
+	exts.b	r0,r0	! -1 if rounding took place
+	shlr8	r6	! isolate div1-mangled rest
+	addc	r2,r0	! generate carry if rounding took place
+	shlr8	r7
+	sub	r3,r0	! pre-rounding fraction
+	bt	0f ! going directly to denorm_sticky would cause mispredicts
+	tst	r6,r6	! rest can only be zero if lost bit was set
+0:	add	r7,r6	! (T ? corrupt : reconstruct) actual rest
+	bt	0f
+	cmp/pl	r6
+0:	mov.w	LOCAL(m24),r1
+	addc	r0,r0	! put in sticky bit
+	add	#-1,r3
+	mov.l	LOCAL(x40000000),r6
+	add	r3,r3
+	mov	r0,r2
+	shad	r1,r3	! exponent ; s32.0
+	!
+	shld	r3,r0
+	add	#30,r3
+	cmp/pl	r3
+	shld	r3,r2
+	bf	LOCAL(zero_nan)	! return zero
+	rotl	r2
+	cmp/hi	r6,r2
+	mov	#0,r7
+	addc	r7,r0
+	div0s	r4,r5
+	rts
+	rotcr	r0
+	
+! ????
+! undo normal rounding (lowest bits still in r6). then do denormal rounding.
+	
+LOCAL(overflow):
+	mov.l	LOCAL(xff000000),r0
+	div0s	r4,r5
+	rts
+	rotcl	r0
+	
+LOCAL(div_inf_or_nan):
+	mov	r4,r0
+	bra	LOCAL(nan_if_t)
+	add	r0,r0
+	
+LOCAL(div_by_inf_or_nan):
+	mov.l	LOCAL(xff000000),r1
+	mov	#0,r0
+	mov	r5,r2
+	add	r2,r2
+	bra	LOCAL(nan_if_t)
+	cmp/hi	r1,r2
+
+
+
+! still need to check for divide by zero or divide by nan
+! r3: 0x7f800000
+	.balign	4
+LOCAL(div_zero):
+	mov	r5,r1
+	add	r1,r1
+	tst	r1,r1	! 0 / 0 -> nan
+	not	r5,r1
+	bt	LOCAL(nan)
+	add	r3,r3
+	cmp/hi	r3,r1	! 0 / nan -> nan (but 0 / inf -> 0)
+LOCAL(zero_nan):
+	mov	#0,r0
+LOCAL(nan_if_t):
+	bf	0f:
+LOCAL(nan):
+	mov	#-1,r0
+0:	div0s	r4,r5	! compute sign
+	rts
+	rotcr	r0	! insert sign
+
+LOCAL(div_by_zero):
+	mov.l	LOCAL(xff000000),r0
+	mov	r5,r2
+	add	r2,r2
+	bra	LOCAL(nan_if_t)
+	cmp/hi	r0,r2
+	
+	.balign	4
+LOCAL(clz):
+	mov.l	r8,@-r15
+	extu.w	r0,r8
+	mov.l	r9,@-r15
+	cmp/eq	r0,r8
+	bt/s	0f
+	mov	#8-8,r9
+	xtrct	r0,r8
+	add	#16,r9
+0:	tst	r1,r8	! 0xff00
+	mov.l	LOCAL(c_clz_tab),r0
+	bt	0f
+	shlr8	r8
+0:	bt	0f
+	add	#8,r9
+0:
+#ifdef	__PIC__
+	add	r0,r8
+	mova	LOCAL(c_clz_tab),r0
+#endif
+	mov.b	@(r0,r8),r8
+	mov	r9,r0
+	mov.l	@r15+,r9
+	!
+	!
+	!
+	sub	r8,r0
+	mov.l	@r15+,r8
+	rts
+	lds.l	@r15+,pr
+
+!	We encode even some words as pc-relative that would fit as immediate
+!	in the instruction in order to avoid some pipeline stalls on
+!	SH4-100 / SH4-200.
+LOCAL(m23):	.word -23
+LOCAL(m24):	.word -24
+LOCAL(m31):	.word -31
+LOCAL(xff01):	.word 0xff01
+	.balign	4
+LOCAL(xff000000): .long 0xff000000
+#ifdef __LITTLE_ENDIAN__
+LOCAL(xff00):	.word 0xff00
+LOCAL(m1):	.word -1
+#else
+LOCAL(m1):	.word -1
+LOCAL(xff00):	.word 0xff00
+#endif
+LOCAL(x7f800000): .long 0x7f800000
+LOCAL(x3f000000): .long 0x3f000000
+LOCAL(x3f800000): .long 0x3f800000
+LOCAL(xff800000): .long 0xff800000
+LOCAL(x40000000): .long 0x40000000
+LOCAL(c_clz_tab):
+#ifdef __pic__
+        .long   GLOBAL(clz_tab) - .
+#else
+        .long   GLOBAL(clz_tab)
+#endif
+ENDFUNC(GLOBAL(divsf3))
diff -Naur gcc-4.2.1/gcc/config/sh/IEEE-754/m3/fixdfsi.S gcc-4.2.1-stm/gcc/config/sh/IEEE-754/m3/fixdfsi.S
--- gcc-4.2.1/gcc/config/sh/IEEE-754/m3/fixdfsi.S	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/IEEE-754/m3/fixdfsi.S	2007-08-10 13:08:59.000000000 +0100
@@ -0,0 +1,115 @@
+/* Copyright (C) 2004, 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!! fixdfsi for Renesas SH / STMicroelectronics ST40 CPUs
+!! Contributed by J"orn Rennecke joern.rennecke@st.com
+
+#ifdef L_fixdfsi
+	! What is a bit unusal about this implementation is that the
+	! sign bit influences the result for NANs: for cleared sign bit, you
+	! get UINT_MAX, for set sign bit, you get 0.
+	! However, since the result for NANs is undefined, this should be no
+	! problem.
+	! N.B. This is scheduled both for SH4-200 and SH4-300
+	.balign 4
+	.global GLOBAL(fixdfsi)
+	FUNC(GLOBAL(fixdfsi))
+	.balign	4
+GLOBAL(fixdfsi):
+	mov.w	LOCAL(x413),r1
+	mov	DBL0H,r0
+	shll	DBL0H
+	mov.l	LOCAL(mask),r3
+	mov	#-21,r2
+	shld	r2,DBL0H	! SH4-200 will start this insn in a new cycle
+	bt/s	LOCAL(neg)
+	sub	r1,DBL0H
+	cmp/pl	DBL0H		! SH4-200 will start this insn in a new cycle
+	and	r3,r0
+	bf/s	LOCAL(ignore_low)
+	addc	r3,r0	! uses T == 1; sets implict 1
+	mov	#10,r2
+	shld	DBL0H,r0	! SH4-200 will start this insn in a new cycle
+	cmp/gt	r2,DBL0H
+	add	#-32,DBL0H
+	bt	LOCAL(retmax)
+	shld	DBL0H,DBL0L
+	rts
+	or	DBL0L,r0
+
+	.balign	8
+LOCAL(ignore_low):
+	mov	#-21,r2
+	cmp/gt	DBL0H,r2	! SH4-200 will start this insn in a new cycle
+	bf	0f		! SH4-200 will start this insn in a new cycle
+	mov	#-31,DBL0H	! results in 0 return
+0:	add	#1,r0
+	rts
+	shld	DBL0H,r0
+
+	.balign 4
+LOCAL(neg):
+	cmp/pl	DBL0H
+	and	r3,r0
+	bf/s	LOCAL(ignore_low_neg)
+	addc	r3,r0	! uses T == 1; sets implict 1
+	mov	#10,r2
+	shld	DBL0H,r0	! SH4-200 will start this insn in a new cycle
+	cmp/gt	r2,DBL0H
+	add	#-32,DBL0H
+	bt	LOCAL(retmin)
+	shld	DBL0H,DBL0L
+	or	DBL0L,r0	! SH4-200 will start this insn in a new cycle
+	rts
+	neg	r0,r0
+
+	.balign 4
+LOCAL(ignore_low_neg):
+	mov	#-21,r2
+	cmp/gt	DBL0H,r2	! SH4-200 will start this insn in a new cycle
+	add	#1,r0
+	shld	DBL0H,r0
+	bf	0f
+	mov	#0,r0		! results in 0 return
+0:	rts
+	neg	r0,r0
+
+LOCAL(retmax):
+	mov	#-1,r0
+	rts
+	shlr	r0
+
+LOCAL(retmin):
+	mov	#1,r0
+	rts
+	rotr	r0
+
+LOCAL(x413): .word 0x413
+
+	.balign 4
+LOCAL(mask): .long 0x000fffff
+	ENDFUNC(GLOBAL(fixdfsi))
+#endif /* L_fixdfsi */
diff -Naur gcc-4.2.1/gcc/config/sh/IEEE-754/m3/fixunsdfsi.S gcc-4.2.1-stm/gcc/config/sh/IEEE-754/m3/fixunsdfsi.S
--- gcc-4.2.1/gcc/config/sh/IEEE-754/m3/fixunsdfsi.S	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/IEEE-754/m3/fixunsdfsi.S	2007-08-10 13:08:59.000000000 +0100
@@ -0,0 +1,82 @@
+/* Copyright (C) 2004, 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!! fixunsdfsi for Renesas SH / STMicroelectronics ST40 CPUs
+!! Contributed by J"orn Rennecke joern.rennecke@st.com
+
+#ifdef L_fixunsdfsi
+	! What is a bit unusal about this implementation is that the
+	! sign bit influences the result for NANs: for cleared sign bit, you
+	! get INT_MAX, for set sign bit, you get INT_MIN.
+	! However, since the result for NANs is undefined, this should be no
+	! problem.
+	! N.B. This is scheduled both for SH4-200 and SH4-300
+	.balign 4
+	.global GLOBAL(fixunsdfsi)
+	FUNC(GLOBAL(fixunsdfsi))
+	.balign	4
+GLOBAL(fixunsdfsi):
+	mov.w	LOCAL(x413),r1	! bias + 20
+	mov	DBL0H,r0
+	shll	DBL0H
+	mov.l	LOCAL(mask),r3
+	mov	#-21,r2
+	shld	r2,DBL0H	! SH4-200 will start this insn in a new cycle
+	bt/s	LOCAL(ret0)
+	sub	r1,DBL0H
+	cmp/pl	DBL0H		! SH4-200 will start this insn in a new cycle
+	and	r3,r0
+	bf/s	LOCAL(ignore_low)
+	addc	r3,r0	! uses T == 1; sets implict 1
+	mov	#11,r2
+	shld	DBL0H,r0	! SH4-200 will start this insn in a new cycle
+	cmp/gt	r2,DBL0H
+	add	#-32,DBL0H
+	bt	LOCAL(retmax)
+	shld	DBL0H,DBL0L
+	rts
+	or	DBL0L,r0
+
+	.balign	8
+LOCAL(ignore_low):
+	mov	#-21,r2
+	cmp/gt	DBL0H,r2	! SH4-200 will start this insn in a new cycle
+	add	#1,r0
+	bf	0f
+LOCAL(ret0): mov #0,r0		! results in 0 return
+0:	rts
+	shld	DBL0H,r0
+
+LOCAL(retmax):
+	rts
+	mov	#-1,r0
+
+LOCAL(x413): .word 0x413
+
+	.balign 4
+LOCAL(mask): .long 0x000fffff
+	ENDFUNC(GLOBAL(fixunsdfsi))
+#endif /* L_fixunsdfsi */
diff -Naur gcc-4.2.1/gcc/config/sh/IEEE-754/m3/floatsidf.S gcc-4.2.1-stm/gcc/config/sh/IEEE-754/m3/floatsidf.S
--- gcc-4.2.1/gcc/config/sh/IEEE-754/m3/floatsidf.S	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/IEEE-754/m3/floatsidf.S	2007-08-10 13:08:59.000000000 +0100
@@ -0,0 +1,103 @@
+/* Copyright (C) 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+! floatsidf for the Renesas SH / STMicroelectronics ST40 CPUs.
+! Contributed by Joern Rennecke
+! joern.rennecke@st.com
+!
+! This code is optimized for SH4 without FPU, but can also be used for SH3.
+
+FUNC(GLOBAL(floatsidf))
+	.global GLOBAL(floatsidf)
+	.balign	4
+GLOBAL(floatsidf):
+	tst	r4,r4
+	mov	r4,r1
+	bt	LOCAL(ret0)
+	cmp/pz	r4
+	bt	0f
+	neg	r4,r1
+0:	mov.l	LOCAL(c_clz_tab),r0
+	extu.w	r1,r5
+	mov.w	LOCAL(xff00),r3
+	cmp/eq	r1,r5
+	mov	#21,r2
+	bt	0f
+	mov	r1,r5
+	shlr16	r5
+	add	#-16,r2
+0:	tst	r3,r5	! 0xff00
+	bt	0f
+	shlr8	r5
+0:	bt	0f
+	add	#-8,r2
+0:
+#ifdef	__PIC__
+	add	r0,r5
+	mova	LOCAL(c_clz_tab),r0
+#endif
+	mov.b	@(r0,r5),r5
+	cmp/pz	r4
+	mov.l	LOCAL(x41200000),r3	! bias + 20 - implicit 1
+	bt	0f
+	mov.l	LOCAL(xc1200000),r3	! sign + bias + 20 - implicit 1
+0:	mov	r1,r0	! DBLRL & DBLRH
+	sub	r5,r2
+	mov	r2,r5
+	shld	r2,DBLRH
+	cmp/pz	r2
+	add	r3,DBLRH
+	add	#32,r2
+	shld	r2,DBLRL
+	bf	0f
+	mov.w	LOCAL(d0),DBLRL
+0:	mov	#20,r2
+	shld	r2,r5
+	rts
+	sub	r5,DBLRH
+LOCAL(ret0):
+	mov	#0,DBLRL
+	rts
+	mov	#0,DBLRH
+
+LOCAL(xff00):	.word 0xff00
+	.balign	4
+LOCAL(x41200000):
+#ifdef __LITTLE_ENDIAN__
+LOCAL(d0):	  .word 0
+		  .word 0x4120
+#else
+		  .word 0x4120
+LOCAL(d0):	  .word 0
+#endif
+LOCAL(xc1200000): .long 0xc1200000
+LOCAL(c_clz_tab):
+#ifdef __pic__
+        .long   GLOBAL(clz_tab) - .
+#else
+        .long   GLOBAL(clz_tab)
+#endif
+ENDFUNC(GLOBAL(floatsidf))
diff -Naur gcc-4.2.1/gcc/config/sh/IEEE-754/m3/floatsisf.S gcc-4.2.1-stm/gcc/config/sh/IEEE-754/m3/floatsisf.S
--- gcc-4.2.1/gcc/config/sh/IEEE-754/m3/floatsisf.S	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/IEEE-754/m3/floatsisf.S	2007-08-10 13:08:59.000000000 +0100
@@ -0,0 +1,106 @@
+/* Copyright (C) 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+! floatsisf for the Renesas SH / STMicroelectronics ST40 CPUs.
+! Contributed by Joern Rennecke
+! joern.rennecke@st.com
+!
+! This code is optimized for SH4 without FPU, but can also be used for SH3.
+
+FUNC(GLOBAL(floatsisf))
+	.global GLOBAL(floatsisf)
+	.balign	4
+GLOBAL(floatsisf):
+	cmp/pz	r4
+	mov	r4,r5
+	bt	0f
+	neg	r4,r5
+0:	mov.l	LOCAL(c_clz_tab),r0
+	extu.w	r5,r1
+	mov.w	LOCAL(xff00),r3
+	cmp/eq	r5,r1
+	mov	#24,r2
+	bt	0f
+	mov	r5,r1
+	shlr16	r1
+	add	#-16,r2
+0:	tst	r3,r1	! 0xff00
+	bt	0f
+	shlr8	r1
+0:	bt	0f
+	add	#-8,r2
+0:
+#ifdef	__PIC__
+	add	r0,r1
+	mova	LOCAL(c_clz_tab),r0
+#endif
+	mov.b	@(r0,r1),r1
+	cmp/pz	r4
+	mov.l	LOCAL(x4a800000),r3	! bias + 23 - implicit 1
+	bt	0f
+	mov.l	LOCAL(xca800000),r3	! sign + bias + 23 - implicit 1
+0:	mov	r5,r0
+	sub	r1,r2
+	mov.l	LOCAL(x80000000),r1
+	shld	r2,r0
+	cmp/pz	r2
+	add	r3,r0
+	bt	LOCAL(noround)
+	add	#31,r2
+	shld	r2,r5
+	add	#-31,r2
+	rotl	r5
+	cmp/hi	r1,r5
+	mov	#0,r3
+	addc	r3,r0
+	mov	#23,r1
+	shld	r1,r2
+	rts
+	sub	r2,r0
+	.balign	8
+LOCAL(noround):
+	mov	#23,r1
+	tst	r4,r4
+	shld	r1,r2
+	bt	LOCAL(ret0)
+	rts
+	sub	r2,r0
+LOCAL(ret0):
+	rts
+	mov	#0,r0
+
+LOCAL(xff00):	.word 0xff00
+	.balign	4
+LOCAL(x4a800000): .long 0x4a800000
+LOCAL(xca800000): .long 0xca800000
+LOCAL(x80000000): .long 0x80000000
+LOCAL(c_clz_tab):
+#ifdef __pic__
+        .long   GLOBAL(clz_tab) - .
+#else
+        .long   GLOBAL(clz_tab)
+#endif
+ENDFUNC(GLOBAL(floatsisf))
diff -Naur gcc-4.2.1/gcc/config/sh/IEEE-754/m3/floatunssidf.S gcc-4.2.1-stm/gcc/config/sh/IEEE-754/m3/floatunssidf.S
--- gcc-4.2.1/gcc/config/sh/IEEE-754/m3/floatunssidf.S	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/IEEE-754/m3/floatunssidf.S	2007-08-10 13:08:59.000000000 +0100
@@ -0,0 +1,96 @@
+/* Copyright (C) 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+! floatunssidf for the Renesas SH / STMicroelectronics ST40 CPUs.
+! Contributed by Joern Rennecke
+! joern.rennecke@st.com
+!
+! This code is optimized for SH4 without FPU, but can also be used for SH3.
+
+FUNC(GLOBAL(floatunsidf))
+	.global GLOBAL(floatunsidf)
+	.balign	4
+GLOBAL(floatunsidf):
+	mov.l	LOCAL(c_clz_tab),r0
+	extu.w	r4,r1
+	mov.w	LOCAL(0xff00),r3
+	cmp/eq	r4,r1
+	mov	#21,r2
+	bt	0f
+	mov	r4,r1
+	shlr16	r1
+	add	#-16,r2
+0:	tst	r3,r1	! 0xff00
+	bt	0f
+	shlr8	r1
+0:	bt	0f
+	add	#-8,r2
+0:
+#ifdef	__PIC__
+	add	r0,r1
+	mova	LOCAL(c_clz_tab),r0
+#endif
+	mov.b	@(r0,r1),r5
+	mov	r4,DBLRL
+	mov.l	LOCAL(x41200000),r3	! bias + 20 - implicit 1
+	tst	r4,r4
+	mov	r4,DBLRH
+	bt	LOCAL(ret0)
+	sub	r5,r2
+	mov	r2,r5
+	shld	r2,DBLRH
+	cmp/pz	r2
+	add	r3,DBLRH
+	add	#32,r2
+	shld	r2,DBLRL
+	bf	0f
+	mov.w	LOCAL(d0),DBLRL
+0:	mov	#20,r2
+	shld	r2,r5
+	rts
+	sub	r5,DBLRH
+LOCAL(ret0):
+	mov	r4,DBLRL
+	rts
+	mov	r4,DBLRH
+
+LOCAL(0xff00):	.word  0xff00
+	.balign	4
+LOCAL(x41200000):
+#ifdef __LITTLE_ENDIAN__
+LOCAL(d0):	  .word 0
+		  .word 0x4120
+#else
+		  .word 0x4120
+LOCAL(d0):	  .word 0
+#endif
+LOCAL(c_clz_tab):
+#ifdef __pic__
+        .long   GLOBAL(clz_tab) - .
+#else
+        .long   GLOBAL(clz_tab)
+#endif
+ENDFUNC(GLOBAL(floatunsidf))
diff -Naur gcc-4.2.1/gcc/config/sh/IEEE-754/m3/floatunssisf.S gcc-4.2.1-stm/gcc/config/sh/IEEE-754/m3/floatunssisf.S
--- gcc-4.2.1/gcc/config/sh/IEEE-754/m3/floatunssisf.S	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/IEEE-754/m3/floatunssisf.S	2007-08-10 13:08:59.000000000 +0100
@@ -0,0 +1,94 @@
+/* Copyright (C) 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+! floatsisf for the Renesas SH / STMicroelectronics ST40 CPUs.
+! Contributed by Joern Rennecke
+! joern.rennecke@st.com
+!
+! This code is optimized for SH4 without FPU, but can also be used for SH3.
+
+FUNC(GLOBAL(floatunsisf))
+	.global GLOBAL(floatunsisf)
+	.balign	4
+GLOBAL(floatunsisf):
+	mov.l	LOCAL(c_clz_tab),r0
+	extu.w	r4,r1
+	mov.w	LOCAL(xff00),r3
+	cmp/eq	r4,r1
+	mov	#24,r2
+	bt	0f
+	mov	r4,r1
+	shlr16	r1
+	add	#-16,r2
+0:	tst	r3,r1	! 0xff00
+	bt	0f
+	shlr8	r1
+0:	bt	0f
+	add	#-8,r2
+0:
+#ifdef	__PIC__
+	add	r0,r1
+	mova	LOCAL(c_clz_tab),r0
+#endif
+	mov.b	@(r0,r1),r1
+	mov	r4,r0
+	mov.l	LOCAL(x4a800000),r3	! bias + 23 - implicit 1
+	tst	r4,r4
+	bt	LOCAL(ret0)
+	!
+	sub	r1,r2
+	mov.l	LOCAL(x80000000),r1
+	shld	r2,r0
+	cmp/pz	r2
+	add	r3,r0
+	bt	LOCAL(noround)
+	add	#31,r2
+	shld	r2,r4
+	rotl	r4
+	add	#-31,r2
+	cmp/hi	r1,r4
+	mov	#0,r3
+	addc	r3,r0
+LOCAL(noround):
+	mov	#23,r1
+	shld	r1,r2
+	rts
+	sub	r2,r0
+LOCAL(ret0):
+	rts
+	nop
+
+LOCAL(xff00):	.word 0xff00
+	.balign	4
+LOCAL(x4a800000): .long 0x4a800000
+LOCAL(x80000000): .long 0x80000000
+LOCAL(c_clz_tab):
+#ifdef __pic__
+        .long   GLOBAL(clz_tab) - .
+#else
+        .long   GLOBAL(clz_tab)
+#endif
+ENDFUNC(GLOBAL(floatunsisf))
diff -Naur gcc-4.2.1/gcc/config/sh/IEEE-754/m3/muldf3.S gcc-4.2.1-stm/gcc/config/sh/IEEE-754/m3/muldf3.S
--- gcc-4.2.1/gcc/config/sh/IEEE-754/m3/muldf3.S	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/IEEE-754/m3/muldf3.S	2007-08-10 13:08:59.000000000 +0100
@@ -0,0 +1,486 @@
+/* Copyright (C) 2004, 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+! muldf3 for the Renesas SH / STMicroelectronics ST40 CPUs.
+! Contributed by Joern Rennecke
+! joern.rennecke@st.com
+!
+! This code is optimized for SH4 without FPU, but can also be used for SH3.
+! Normal numbers are multiplied in 53 or 54 cycles on SH4-200.
+
+FUNC(GLOBAL(muldf3))
+	.global GLOBAL(muldf3)
+LOCAL(inf_nan_denorm_or_zero_a):
+	mov.l	r8,@-r15
+	sub	r3,DBL0H	! isolate high fraction
+	mov.l	@(4,r15),r8	! original DBL0H (with sign & exp)
+	sub	r3,r1		! 0x7ff00000
+	mov.l	LOCAL(x60000000),r3
+	shll16	r2		! 0xffff0000
+	!			  no stall here for sh4-200
+	!
+	tst	r1,r8
+	mov.l	r0,@-r15
+	bf	LOCAL(inf_nan_a)
+	tst	r1,r0		! test for DBL1 inf, nan or small
+	bt	LOCAL(ret_inf_nan_zero)
+LOCAL(normalize_arg):
+	tst	DBL0H,DBL0H
+	bf	LOCAL(normalize_arg53)
+	tst	DBL0L,DBL0L
+	bt	LOCAL(a_zero)
+	tst	r2,DBL0L
+	mov	DBL0L,DBL0H
+	bt	LOCAL(normalize_arg16)
+	shlr16	DBL0H
+	mov.w	LOCAL(m15),r2	! 1-16
+	bra	LOCAL(normalize_arg48)
+	shll16	DBL0L
+
+LOCAL(normalize_arg53):
+	tst	r2,DBL0H
+	mov	#1,r2
+	bt	LOCAL(normalize_arg48)
+	mov	DBL0H,r1
+	shlr16	r1
+	bra	LOCAL(normalize_DBL0H)
+	mov	#21-16,r3
+
+LOCAL(normalize_arg16):
+	mov.w	LOCAL(m31),r2 ! 1-32
+	mov	#0,DBL0L
+LOCAL(normalize_arg48):
+	mov	DBL0H,r1
+	mov	#21,r3
+LOCAL(normalize_DBL0H):
+	extu.b	r1,r8
+	mov.l	LOCAL(c__clz_tab),r0
+	cmp/eq	r8,r1
+	!
+	bt	0f
+	shlr8	r1
+0:
+#ifdef	__pic__
+	add	r0,r1
+
+	mova	LOCAL(c__clz_tab),r0
+
+#endif /* __pic__ */
+	mov.b	@(r0,r1),r8
+	mov	DBL0L,r1
+	mov.l	@r15+,r0
+	bt	0f
+	add	#-8,r3
+0:	clrt
+	sub	r8,r3
+	mov.w	LOCAL(d20),r8
+	shld	r3,DBL0H
+	shld	r3,DBL0L
+	sub	r3,r2
+	add	#-32,r3
+	shld	r3,r1
+	mov.l	LOCAL(x00100000),r3
+	or	r1,DBL0H
+	shld	r8,r2
+	mov.l	@r15+,r8
+	add	r2,DBL1H
+	mov.l	LOCAL(x001fffff),r2
+	dmulu.l	DBL0L,DBL1L
+	bra	LOCAL(arg_denorm_done)
+	or	r3,r0		! set implicit 1 bit
+
+LOCAL(a_zero):
+	mov.l	@(4,r15),r8
+	add	#8,r15
+LOCAL(zero):
+	mov	#0,DBLRH
+	bra	LOCAL(pop_ret)
+	mov	#0,DBLRL
+
+! both inf / nan -> result is nan if at least one is none, else inf.
+! BBL0 inf/nan, DBL1 zero   -> result is nan
+! DBL0 inf/nan, DBL1 finite -> result is DBL0 with sign adjustemnt
+LOCAL(inf_nan_a):
+	mov	r8,DBLRH
+	mov.l	@(4,r15),r8
+	add	#8,r15
+	tst	r1,r0	! arg1 inf/nan ?
+	mov	DBL0L,DBLRL
+	bt	LOCAL(both_inf_nan)
+	tst	DBL1L,DBL1L
+	mov	DBL1H,r1
+	bf	LOCAL(pop_ret)
+	add	r1,r1
+	tst	r1,r1
+	!
+	bf	LOCAL(pop_ret)
+LOCAL(nan):
+	mov	#-1,DBLRL
+	bra	LOCAL(pop_ret)
+	mov	#-1,DBLRH
+
+LOCAL(both_inf_nan):
+	or	DBL1L,DBLRL
+	bra	LOCAL(pop_ret)
+	or	DBL1H,DBLRH
+
+LOCAL(ret_inf_nan_zero):
+	tst	r1,r0
+	mov.l	@(4,r15),r8
+	or	DBL0L,DBL0H
+	bf/s	LOCAL(zero)
+	add	#8,r15
+	tst	DBL0H,DBL0H
+	bt	LOCAL(nan)
+LOCAL(inf_nan_b):
+	mov	DBL1L,DBLRL
+	mov	DBL1H,DBLRH
+LOCAL(pop_ret):
+	mov.l	@r15+,DBL0H
+	add	DBLRH,DBLRH
+
+
+	div0s	DBL0H,DBL1H
+
+	rts
+	rotcr	DBLRH
+
+	.balign	4
+/* Argument a has already been tested for being zero or denorm.
+   On the other side, we have to swap a and b so that we can share the
+   normalization code.
+   a: sign/exponent : @r15 fraction: DBL0H:DBL0L
+   b: sign/exponent: DBL1H fraction:    r0:DBL1L  */
+LOCAL(inf_nan_denorm_or_zero_b):
+	sub	r3,r1		! 0x7ff00000
+	mov.l	@r15,r2		! get original DBL0H
+	tst	r1,DBL1H
+	sub	r3,r0		! isolate high fraction
+	bf	LOCAL(inf_nan_b)
+	mov.l	DBL1H,@r15
+	mov	r0,DBL0H
+	mov.l	r8,@-r15
+	mov	r2,DBL1H
+	mov.l	LOCAL(0xffff0000),r2
+	mov.l	r1,@-r15
+	mov	DBL1L,r1
+	mov	DBL0L,DBL1L
+	bra	LOCAL(normalize_arg)
+	mov	r1,DBL0L
+
+LOCAL(d20):
+	.word	20
+LOCAL(m15):
+	.word	-15
+LOCAL(m31):
+	.word	-31
+LOCAL(xff):
+	.word	0xff
+
+	.balign	4
+LOCAL(0xffff0000): .word 0xffff0000
+
+	! calculate a (DBL0H:DBL0L) * b (DBL1H:DBL1L)
+	.balign	4
+GLOBAL(muldf3):
+	mov.l	LOCAL(xfff00000),r3
+	mov	DBL1H,r0
+	dmulu.l	DBL0L,DBL1L
+	mov.l	LOCAL(x7fe00000),r1
+	sub	r3,r0
+	mov.l	DBL0H,@-r15
+	sub	r3,DBL0H
+	tst	r1,DBL0H
+	or	r3,DBL0H
+	mov.l	LOCAL(x001fffff),r2
+	bt	LOCAL(inf_nan_denorm_or_zero_a)
+	tst	r1,r0
+	or	r3,r0		! r0:DBL1L    := b fraction ; u12.52
+	bt	LOCAL(inf_nan_denorm_or_zero_b) ! T clear on fall-through
+LOCAL(arg_denorm_done):
+	and	r2,r0		! r0:DBL1L    := b fraction ; u12.52
+	sts	macl,r3
+	sts	mach,r1
+	dmulu.l	DBL0L,r0
+	and	r2,DBL0H	! DBL0H:DBL0L := a fraction ; u12.52
+	mov.l	r8,@-r15
+	mov	#0,DBL0L
+	mov.l	r9,@-r15
+	sts	macl,r2
+	sts	mach,r8
+	dmulu.l	DBL0H,DBL1L
+	addc	r1,r2
+
+	addc	DBL0L,r8	! add T; clears T
+
+	sts	macl,r1
+	sts	mach,DBL1L
+	dmulu.l	DBL0H,r0
+	addc	r1,r2
+	mov.l	LOCAL(x7ff00000),DBL0H
+	addc	DBL1L,r8	! clears T
+	mov.l	@(8,r15),DBL1L	! a sign/exp w/fraction
+	sts	macl,DBLRL
+	sts	mach,DBLRH
+	and	DBL0H,DBL1L	! a exponent
+	mov.w	LOCAL(x200),r9
+	addc	r8,DBLRL
+	mov.l	LOCAL(x3ff00000),r8	! bias
+	addc	DBL0L,DBLRH	! add T
+	cmp/hi	DBL0L,r3	! 32 guard bits -> sticky: T := r3 != 0
+	movt	r3
+	tst	r9,DBLRH	! T := fraction < 2
+	or	r3,r2		! DBLRH:DBLRL:r2 := result fraction; u24.72
+	bt/s	LOCAL(shll12)
+	sub	r8,DBL1L
+	mov.l	LOCAL(x002fffff),r8
+	and	DBL1H,DBL0H	! b exponent
+	mov.l	LOCAL(x00100000),r9
+	add	DBL0H,DBL1L ! result exponent - 1
+	tst	r8,r2
+	mov.w	LOCAL(m20),r8
+	subc	DBL0L,r9
+	addc	r2,r9 ! r2 value is still needed for denormal rounding
+	mov.w	LOCAL(d11),DBL0L
+	rotcr	r9
+	clrt
+	shld	r8,r9
+	mov.w	LOCAL(m21),r8
+	mov	DBLRL,r3
+	shld	DBL0L,DBLRL
+	addc	r9,DBLRL
+	mov.l	@r15+,r9
+	shld	r8,r3
+	mov.l	@r15+,r8
+	shld	DBL0L,DBLRH
+	mov.l	@r15+,DBL0H
+	addc	r3,DBLRH
+	mov.l	LOCAL(x7ff00000),DBL0L
+	add	DBL1L,DBLRH	! implicit 1 adjusts exponent
+	mov.l	LOCAL(xffe00000),r3
+	cmp/hs	DBL0L,DBLRH
+	add	DBLRH,DBLRH
+	bt	LOCAL(ill_exp_11)
+	tst	r3,DBLRH
+	bt	LOCAL(denorm_exp0_11)
+	div0s	DBL0H,DBL1H
+	rts
+	rotcr	DBLRH
+
+
+LOCAL(shll12):
+	mov.l	LOCAL(x0017ffff),r8
+	extu.b	DBLRH,DBLRH	! remove implicit 1.
+	mov.l	LOCAL(x00080000),r9
+	and	DBL1H,DBL0H	! b exponent
+	add	DBL0H,DBL1L	! result exponent
+	tst	r8,r2		! rounding adjust for lower guard ...
+	mov.w	LOCAL(m19),r8
+	subc	DBL0L,r9	! ... bits and round to even; clear T
+	addc	r2,r9 ! r2 value is still needed for denormal rounding
+	mov.w	LOCAL(d12),DBL0L
+	rotcr	r9
+	clrt
+	shld	r8,r9
+	mov.w	LOCAL(m20),r8
+	mov	DBLRL,r3
+	shld	DBL0L,DBLRL
+	addc	r9,DBLRL
+	mov.l	@r15+,r9
+	shld	r8,r3
+	mov.l	@r15+,r8
+	shld	DBL0L,DBLRH
+	mov.l	LOCAL(x7ff00000),DBL0L
+	addc	r3,DBLRH
+	mov.l	@r15+,DBL0H
+	add	DBL1L,DBLRH
+	mov.l	LOCAL(xffe00000),r3
+	cmp/hs	DBL0L,DBLRH
+	add	DBLRH,DBLRH
+	bt	LOCAL(ill_exp_12)
+	tst	r3,DBLRH
+	bt	LOCAL(denorm_exp0_12)
+LOCAL(insert_sign):
+	div0s	DBL0H,DBL1H
+	rts
+	rotcr	DBLRH
+
+LOCAL(overflow):
+	mov	r3,DBLRH
+	mov	#0,DBLRL
+	bra	LOCAL(insert_sign)
+	mov.l	@r15+,r8
+
+LOCAL(denorm_exp0_11):
+	mov.l	r8,@-r15
+	mov	#-21,r8
+	mov.l	r9,@-r15
+	bra	LOCAL(denorm)
+	mov	#-2,DBL1L	! one for denormal, and one for sticky bit
+
+LOCAL(ill_exp_11):
+	mov	DBL1H,DBL1L
+	and	r3,DBL0L	! 0x7fe00000
+	add	DBL1L,DBL1L
+	mov.l	r8,@-r15
+	cmp/hi	DBL1L,DBL0L	! check if exp a was large
+	mov	#-20,DBL0L
+	bf	LOCAL(overflow)
+	mov	#-21,r8
+	mov	DBLRH,DBL1L
+	rotcr	DBL1L		! shift in negative sign
+	mov.l	r9,@-r15
+	shad	DBL0L,DBL1L	! exponent ; s32
+	bra	LOCAL(denorm)
+	add	#-2,DBL1L	! add one for denormal, and one for sticky bit
+
+LOCAL(denorm_exp0_12):
+	mov.l	r8,@-r15
+	mov	#-20,r8
+	mov.l	r9,@-r15
+	bra	LOCAL(denorm)
+	mov	#-2,DBL1L	! one for denormal, and one for sticky bit
+
+	.balign 4		! also aligns LOCAL(denorm)
+LOCAL(ill_exp_12):
+	and	r3,DBL0L	! 0x7fe00000
+	mov	DBL1H,DBL1L
+	add	DBL1L,DBL1L
+	mov.l	r8,@-r15
+	cmp/hi	DBL1L,DBL0L	! check if exp a was large
+	bf	LOCAL(overflow)
+	mov	DBLRH,DBL1L
+	rotcr	DBL1L		! shift in negative sign
+	mov	#-20,r8
+	shad	r8,DBL1L	! exponent ; s32
+	mov.l	r9,@-r15
+	add	#-2,DBL1L	! add one for denormal, and one for sticky bit
+LOCAL(denorm):
+	not	r3,r9		! 0x001fffff
+	mov.l	r10,@-r15
+	mov	r2,r10
+	shld	r8,r10	! 11 or 12 lower bit valid
+	and	r9,DBLRH ! Mask away vestiges of exponent.
+	add	#32,r8
+	sub	r3,DBLRH ! Make leading 1 explicit.
+	shld	r8,r2	! r10:r2 := unrounded result lowpart
+	shlr	DBLRH	! compensate for doubling at end of normal code
+	sub	DBLRL,r10	! reconstruct effect of previous rounding
+	exts.b	r10,r9
+	shad	r3,r10	! sign extension
+	mov	#0,r3
+	clrt
+	addc	r9,DBLRL	! Undo previous rounding.
+	mov.w	LOCAL(m32),r9
+	addc	r10,DBLRH
+	cmp/hi	r3,r2
+	rotcl	DBLRL	! fit in the rest of r2 as a sticky bit.
+	mov.l	@r15+,r10
+	rotcl	DBLRH
+	cmp/ge	r9,DBL1L
+	bt	LOCAL(small_norm_shift)
+	cmp/hi	r3,DBLRL
+	add	#32,DBL1L
+	movt	DBLRL
+	cmp/gt	r9,DBL1L
+	or	DBLRH,DBLRL
+	bt/s	LOCAL(small_norm_shift)
+	mov	r3,DBLRH
+	mov	r3,DBLRL	! exponent too negative to shift - return zero
+	mov.l	@r15+,r9
+	mov.l	@r15+,r8
+	div0s	DBL0H,DBL1H
+	rts
+	rotcr	DBLRH
+	.balign	4
+LOCAL(small_norm_shift):
+	mov	DBLRL,r2	! stash away guard bits
+	shld	DBL1L,DBLRL
+	mov	DBLRH,DBL0L
+	shld	DBL1L,DBLRH
+	mov.l	LOCAL(x7fffffff),r9
+	add	#32,DBL1L
+	shld	DBL1L,r2
+	shld	DBL1L,DBL0L
+	or	DBL0L,DBLRL
+	shlr	DBL0L
+	addc	r2,r9
+	mov.l	@r15+,r9
+	mov.l	@r15+,r8
+	addc	r3,DBLRL
+	addc	r3,DBLRH
+	div0s	DBL0H,DBL1H
+	add	DBLRH,DBLRH
+	rts
+	rotcr	DBLRH
+
+
+LOCAL(x200):
+	.word 0x200
+LOCAL(m19):
+	.word	-19
+LOCAL(m20):
+	.word	-20
+LOCAL(m21):
+	.word	-21
+LOCAL(m32):
+	.word	-32
+LOCAL(d11):
+	.word	11
+LOCAL(d12):
+	.word	12
+	.balign	4
+LOCAL(x60000000):
+	.long	0x60000000
+LOCAL(c__clz_tab):
+#ifdef __pic__
+	.long	GLOBAL(clz_tab) - .
+#else
+	.long	GLOBAL(clz_tab)
+#endif
+LOCAL(xfff00000):
+	.long	0xfff00000
+LOCAL(x7fffffff):
+	.long	0x7fffffff
+LOCAL(x00100000):
+	.long	0x00100000
+LOCAL(x7fe00000):
+	.long	0x7fe00000
+LOCAL(x001fffff):
+	.long	0x001fffff
+LOCAL(x7ff00000):
+	.long	0x7ff00000
+LOCAL(x3ff00000):
+	.long	0x3ff00000
+LOCAL(x002fffff):
+	.long	0x002fffff
+LOCAL(xffe00000):
+	.long	0xffe00000
+LOCAL(x0017ffff):
+	.long	0x0017ffff
+LOCAL(x00080000):
+	.long	0x00080000
+ENDFUNC(GLOBAL(muldf3))
diff -Naur gcc-4.2.1/gcc/config/sh/IEEE-754/m3/mulsf3.S gcc-4.2.1-stm/gcc/config/sh/IEEE-754/m3/mulsf3.S
--- gcc-4.2.1/gcc/config/sh/IEEE-754/m3/mulsf3.S	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/IEEE-754/m3/mulsf3.S	2007-08-10 13:08:59.000000000 +0100
@@ -0,0 +1,246 @@
+/* Copyright (C) 2004 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+! mulsf3 for the Renesas SH / STMicroelectronics ST40 CPUs.
+! Contributed by Joern Rennecke
+! joern.rennecke@st.com
+
+	.balign 4
+	.global GLOBAL(mulsf3)
+	FUNC(GLOBAL(mulsf3))
+GLOBAL(mulsf3):
+	mov.l	LOCAL(x7f800000),r1
+	not	r4,r2
+	mov	r4,r3
+	not	r5,r0
+	tst	r1,r2
+	or	r1,r3
+	bt/s	LOCAL(inf_nan_arg0)
+	 tst	r1,r0
+	bt	LOCAL(inf_nan_arg1)
+	tst	r1,r5
+	mov	r1,r2
+	shll8	r3
+	or	r5,r1
+	bt/s	LOCAL(zero_denorm_arg1)
+	 shll8	r1
+	tst	r2,r4
+	bt	LOCAL(zero_denorm_arg0)
+	dmulu.l	r3,r1
+	mov	r4,r0
+	and	r2,r0
+LOCAL(arg_norm):
+	and	r5,r2
+	mov.l	LOCAL(x3f800000),r3
+	sts	mach,r1
+	sub	r3,r0
+	sts	macl,r3
+	add	r2,r0
+	cmp/pz	r1
+	mov.w	LOCAL(x100),r2
+	bf/s	LOCAL(norm_frac)
+	 tst	r3,r3
+	shll2	r1	/* Shift one up, replace leading 1 with 0.  */
+	shlr	r1
+	tst	r3,r3
+LOCAL(norm_frac):
+	mov.w	LOCAL(mx80),r3
+	bf	LOCAL(round_frac)
+	tst	r2,r1
+LOCAL(round_frac):
+	mov.l	LOCAL(xff000000),r2
+	subc	r3,r1	/* Even overflow gives right result: exp++, frac=0.  */
+	shlr8	r1
+	add	r1,r0
+	shll	r0
+	bt	LOCAL(ill_exp)
+	tst	r2,r0
+	bt	LOCAL(denorm0)
+	cmp/hs	r2,r0
+	bt	LOCAL(inf)
+LOCAL(insert_sign):
+	div0s	r4,r5
+	rts
+	rotcr	r0
+LOCAL(denorm0):
+	sub	r2,r0
+	bra	LOCAL(insert_sign)
+	 shlr	r0
+LOCAL(zero_denorm_arg1):
+	mov.l	LOCAL(x60000000),r2	/* Check exp0 >= -64	*/
+	add	r1,r1
+	tst	r1,r1	/* arg1 == 0 ? */
+	mov	#0,r0
+	bt	LOCAL(insert_sign) /* argument 1 is zero ==> return 0  */
+	tst	r4,r2
+	bt	LOCAL(insert_sign) /* exp0 < -64  ==> return 0 */
+	mov.l	LOCAL(c__clz_tab),r0
+	mov	r3,r2
+	mov	r1,r3
+	bra	LOCAL(arg_normalize)
+	mov	r2,r1
+LOCAL(zero_denorm_arg0):
+	mov.l	LOCAL(x60000000),r2	/* Check exp1 >= -64	*/
+	add	r3,r3
+	tst	r3,r3	/* arg0 == 0 ? */
+	mov	#0,r0
+	bt	LOCAL(insert_sign) /* argument 0 is zero ==> return 0  */
+	tst	r5,r2
+	bt	LOCAL(insert_sign) /* exp1 < -64  ==> return 0 */
+	mov.l	LOCAL(c__clz_tab),r0
+LOCAL(arg_normalize):
+	mov.l	r7,@-r15
+	extu.w	r3,r7
+	cmp/eq	r3,r7
+	mov.l	LOCAL(xff000000),r7
+	mov	#-8,r2
+	bt	0f
+	tst	r7,r3
+	mov	#-16,r2
+	bt	0f
+	mov	#-24,r2
+0:
+	mov	r3,r7
+	shld	r2,r7
+#ifdef __pic__
+	add	r0,r7
+	mova  LOCAL(c__clz_tab),r0
+#endif
+	mov.b	@(r0,r7),r0
+	add	#32,r2
+	mov	r2,r7
+	mov	#23,r2
+	sub	r0,r7
+	mov.l	LOCAL(x7f800000),r0
+	shld	r7,r3
+	shld	r2,r7
+	mov	r0,r2
+	and	r4,r0
+	sub	r7,r0
+	mov.l	@r15+,r7
+	bra	LOCAL(arg_norm)
+	 dmulu.l	r3,r1
+#if 0 /* This is slightly slower, but could be used if table lookup causes
+         cache thrashing.  */
+	bt	LOCAL(insert_sign) /* exp1 < -64  ==> return 0 */
+	mov.l	LOCAL(xff000000),r2
+	mov	r4,r0
+LOCAL(arg_normalize):
+	tst	r2,r3
+	bf	LOCAL(arg_bit_norm)
+LOCAL(arg_byte_loop):
+	tst	r2,r3
+	add	r2,r0
+	shll8	r3
+	bt	LOCAL(arg_byte_loop)
+	add	r4,r0
+LOCAL(arg_bit_norm):
+	mov.l	LOCAL(x7f800000),r2
+	rotl	r3
+LOCAL(arg_bit_loop):
+	add	r2,r0
+	bf/s	LOCAL(arg_bit_loop)
+	 rotl	r3
+	rotr	r3
+	rotr	r3
+	sub	r2,r0
+	bra	LOCAL(arg_norm)
+	 dmulu.l	r3,r1
+#endif /* 0 */
+LOCAL(inf):
+	bra	LOCAL(insert_sign)
+	 mov	r2,r0
+LOCAL(inf_nan_arg0):
+	bt	LOCAL(inf_nan_both)
+	add	r0,r0
+	cmp/eq	#-1,r0	/* arg1 zero? -> NAN */
+	bt	LOCAL(insert_sign)
+	mov	r4,r0
+LOCAL(inf_insert_sign):
+	bra	LOCAL(insert_sign)
+	 add	r0,r0
+LOCAL(inf_nan_both):
+	mov	r4,r0
+	bra	LOCAL(inf_insert_sign)
+	 or	r5,r0
+LOCAL(inf_nan_arg1):
+	mov	r2,r0
+	add	r0,r0
+	cmp/eq	#-1,r0	/* arg0 zero? */
+	bt	LOCAL(insert_sign)
+	bra	LOCAL(inf_insert_sign)
+	 mov	r5,r0
+LOCAL(ill_exp):
+	cmp/pz	r0
+	mov	#-24,r3
+	bt	LOCAL(inf)
+	add	r1,r1
+	mov	r0,r2
+	sub	r1,r2	! remove fraction to get back pre-rounding exponent.
+	sts	mach,r0
+	sts	macl,r1
+	shad	r3,r2
+	mov	r0,r3
+	shld	r2,r0
+	add	#32,r2
+	cmp/pz	r2
+	shld	r2,r3
+	bf	LOCAL(zero)
+	or	r1,r3
+	mov	#-1,r1
+	tst	r3,r3
+	mov.w	LOCAL(x100),r3
+	bf/s	LOCAL(denorm_round_up)
+	mov	#-0x80,r1
+	tst	r3,r0
+LOCAL(denorm_round_up):
+	mov	#-7,r3
+	subc	r1,r0
+	bra	LOCAL(insert_sign)
+	 shld	r3,r0
+LOCAL(zero):
+	bra	LOCAL(insert_sign)
+	 mov #0,r0
+LOCAL(x100):
+	.word	0x100
+LOCAL(mx80):
+	.word	-0x80
+	.balign	4
+LOCAL(x7f800000):
+	.long 0x7f800000
+LOCAL(x3f800000):
+	.long 0x3f800000
+LOCAL(xff000000):
+	.long	0xff000000
+LOCAL(x60000000):
+	.long	0x60000000
+LOCAL(c__clz_tab):
+#ifdef __pic__
+	.long	GLOBAL(clz_tab) - .
+#else
+	.long	GLOBAL(clz_tab)
+#endif
+	ENDFUNC(GLOBAL(mulsf3))
diff -Naur gcc-4.2.1/gcc/config/sh/IEEE-754/muldf3.S gcc-4.2.1-stm/gcc/config/sh/IEEE-754/muldf3.S
--- gcc-4.2.1/gcc/config/sh/IEEE-754/muldf3.S	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/IEEE-754/muldf3.S	2007-08-10 13:08:59.000000000 +0100
@@ -0,0 +1,601 @@
+/* Copyright (C) 2004, 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+!multiplication of two double precision floating point numbers
+!Author:Aanchal Khanna
+!SH1 Support / Simplifications: Joern Rennecke
+!
+!Entry:
+!r4,r5:operand 1
+!
+!r6,r7:operand 2
+!
+!Exit:
+!r0,r1:result
+!
+!Notes: argument 1 is passed in regs r4 and r5 and argument 2 is passed in regs
+!r6 and r7, result is returned in regs r0 and r1. operand 1 is referred as op1
+!and operand 2 as op2.
+!
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+	.text
+	.align	5	
+	.global	GLOBAL (muldf3)
+	FUNC (GLOBAL (muldf3))
+
+GLOBAL (muldf3):
+
+#ifdef  __LITTLE_ENDIAN__
+        mov     r4,r1
+        mov     r5,r4
+        mov     r1,r5
+
+        mov     r6,r1
+        mov     r7,r6
+        mov     r1,r7
+#endif
+	mov.l	.L_mask_sign,r0
+	mov	r4,r2
+
+	and	r0,r2		
+	mov	#0,r1
+
+	shll	r4
+	and	r6,r0		
+	
+	xor     r2,r0		!r0 contains the result's sign bit
+	shlr	r4
+
+	mov.l   .L_inf,r2
+	shll	r6
+
+	mov	r4,r3
+	shlr	r6
+	
+.L_chk_a_inv:
+	!chk if op1 is Inf/NaN
+	and	r2,r3
+	mov.l	r8,@-r15
+
+	cmp/eq	r3,r2
+	mov.l	.L_mask_high_mant,r8
+
+	mov	r2,r3
+	bf	.L_chk_b_inv
+
+	mov	r8,r3
+	and	r4,r8
+
+	cmp/hi  r1,r8		
+	bt	.L_return_a	!op1 NaN, return op1
+
+	cmp/hi  r1,r5	
+	mov	r2,r8
+
+	bt      .L_return_a	!op1 NaN, return op1
+	and	r6,r8
+
+	cmp/eq	r8,r2		
+	and	r6,r3
+
+	bt      .L_b_inv
+	cmp/eq	r1,r6		
+
+	bf	.L_return_a	!op1 Inf,op2= normal no return op1
+	cmp/eq	r1,r7
+
+	bf	.L_return_a	!op1 Inf,op2= normal no return op1
+	mov.l   @r15+,r8	
+
+	rts
+	mov	#-1,DBLRH	!op1=Inf, op2=0,return nan
+
+.L_b_inv:
+	!op2 is NaN/Inf
+	cmp/hi	r1,r7
+	mov	r1,r2
+
+	mov	r5,r1
+	bt	.L_return_b	!op2=NaN,return op2
+
+	cmp/hi	r2,r6
+	or	r4,r0
+
+	bt	.L_return_b	!op2=NaN,return op2
+	mov.l   @r15+,r8
+
+#ifdef __LITTLE_ENDIAN__
+        mov     r0,r2
+        mov     r1,r0
+        mov     r2,r1
+#endif
+
+	rts			!op1=Inf,op2=Inf,return Inf with sign
+	nop
+
+.L_chk_b_inv:
+	!Chk if op2 is NaN/Inf
+	and	r6,r2
+	cmp/eq	r3,r2
+
+	bf	.L_chk_a_for_zero
+	and	r6,r8
+
+	cmp/hi	r1,r8
+	bt	.L_return_b	 !op2=NaN,return op2
+
+	cmp/hi	r1,r7
+	bt	.L_return_b	 !op2=NaN,return op2
+
+	cmp/eq	r5,r1
+	bf      .L_return_b	 !op1=normal number,op2=Inf,return Inf
+
+	mov	r7,r1
+	cmp/eq	r4,r1
+
+	bf	.L_return_b	/* op1=normal number, op2=Inf,return Inf */
+	mov.l   @r15+,r8
+
+	rts
+	mov	#-1,DBLRH	!op1=0,op2=Inf,return NaN
+
+.L_return_a:
+	mov	r5,r1
+	or	r4,r0
+
+#ifdef __LITTLE_ENDIAN__
+        mov     r0,r2
+        mov     r1,r0
+        mov     r2,r1
+#endif
+
+	rts
+	mov.l   @r15+,r8
+
+.L_return_b:
+	mov	r7,r1
+	or	r6,r0	
+	
+#ifdef __LITTLE_ENDIAN__
+        mov     r0,r2
+        mov     r1,r0
+        mov     r2,r1
+#endif
+
+	rts
+	mov.l	@r15+,r8
+	
+.L_chk_a_for_zero:
+	!Chk if op1 is zero
+	cmp/eq	r1,r4
+	bf	.L_chk_b_for_zero
+	
+	cmp/eq	r1,r5
+	bf	.L_chk_b_for_zero
+
+#ifdef __LITTLE_ENDIAN__
+        mov     r0,r2
+        mov     r1,r0
+        mov     r2,r1
+#endif
+	rts
+	mov.l	@r15+,r8
+
+.L_chk_b_for_zero:
+	!op1=0,chk if op2 is zero
+        cmp/eq  r1,r6
+        mov	r1,r3
+	
+	mov.l   .L_inf,r1
+	bf      .L_normal_nos
+
+        cmp/eq  r3,r7
+        bf      .L_normal_nos
+
+	mov	r3,r1
+	mov.l   @r15+,r8
+
+#ifdef __LITTLE_ENDIAN__
+        mov     r0,r2
+        mov     r1,r0
+        mov     r2,r1
+#endif
+	rts
+	nop
+
+.L_normal_nos:
+	!op1 and op2 are normal nos
+	mov.l	r9,@-r15
+	mov	r4,r3
+
+	mov     #-20,r9	
+	and	r1,r3	
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r9,r2
+#else
+        SHLR20 (r2)
+#endif
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r9,r3
+#else
+        SHLR20 (r3)
+#endif
+	cmp/pl	r3
+
+	bf	.L_norm_a	!normalize op1
+.L_chk_b:	
+	cmp/pl	r2
+	bf	.L_norm_b	!normalize op2
+
+.L_mul1:
+	add	r3,r2
+	mov.l  .L_1023,r1
+	
+	!resultant exponent in r2
+	add     r1,r2
+	mov.l   .L_2047,r1	
+
+	!Chk the exponent for overflow
+	cmp/ge	r1,r2
+	and     r8,r4
+
+	bt	.L_return_inf
+	mov.l	.L_imp_bit,r1
+	
+	or	r1,r4		
+	and	r8,r6
+
+	or	r1,r6
+	clrt
+
+	!multiplying the mantissas
+	DMULU_SAVE
+	DMULUL	(r7,r5,r1) 	!bits 0-31 of product 	
+
+	DMULUH	(r3)
+	
+	DMULUL	(r4,r7,r8)
+
+	addc	r3,r8
+
+	DMULUH	(r3)
+
+	movt	r9
+	clrt
+
+	DMULUL	(r5,r6,r7)
+
+	addc	r7,r8		!bits 63-32 of product
+
+	movt	r7
+	add	r7,r9
+
+	DMULUH	(r7)
+
+	add	r7,r3
+
+	add	r9,r3
+	clrt
+
+	DMULUL	(r4,r6,r7)
+
+	addc	r7,r3		!bits 64-95 of product
+
+	DMULUH	(r7)
+	DMULU_RESTORE
+	
+	mov	#0,r5
+	addc	r5,r7		!bits 96-105 of product
+
+	cmp/eq	r5,r1
+	mov     #1,r4
+
+	bt	.L_skip
+	or	r4,r8
+.L_skip:
+	mov.l   .L_106_bit,r4
+	mov	r8,r9
+
+.L_chk_extra_msb:
+	!chk if exra MSB is generated
+	and     r7,r4
+	cmp/eq	r5,r4
+
+	mov     #12,r4
+	SL(bf,	.L_shift_rt_by_1,
+	 mov     #31,r5)
+	
+.L_pack_mantissa:
+	!scale the mantissa t0 53 bits
+	mov	#-19,r6
+	mov.l	.L_mask_high_mant,r5
+
+        SHLRN (19, r6, r8)
+
+	and	r3,r5
+
+	shlr	r8
+	movt	r1
+
+        SHLLN (12, r4, r5)
+
+	add	#-1,r6
+
+	or	r5,r8		!lower bits of resulting mantissa
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r6,r3
+#else
+        SHLR20 (r3)
+#endif
+
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r4,r7
+#else
+        SHLL12 (r7)
+#endif
+	clrt
+
+	or	r7,r3		!higher bits of resulting mantissa
+	mov     #0,r7
+
+	!chk the exponent for underflow
+	cmp/ge	r2,r7
+	bt	.L_underflow
+
+	addc    r1,r8           !rounding
+	mov	r8,r1
+
+	addc	r7,r3		!rounding
+	mov.l	.L_mask_22_bit,r5
+
+	and	r3,r5
+	!chk if extra msb is generated after rounding
+	cmp/eq	r7,r5
+
+	mov.l	.L_mask_high_mant,r8
+	bt	.L_pack_result
+
+	add	#1,r2
+	mov.l	.L_2047,r6
+
+	cmp/ge	r6,r2
+
+	bt	.L_return_inf
+	shlr	r3
+
+	rotcr	r1
+
+.L_pack_result:
+	!pack the result, r2=exponent, r3=higher mantissa, r1=lower mantissa
+	!r0=sign bit
+	mov	#20,r6
+	and	r8,r3
+	
+#if !defined (__sh1__) && !defined (__sh2__) && !defined (__SH2E__)
+        shld    r6,r2
+#else
+        SHLL20 (r2)
+#endif
+	or	r3,r0
+	
+	or      r2,r0
+	mov.l   @r15+,r9
+
+#ifdef __LITTLE_ENDIAN__
+        mov     r0,r2
+        mov     r1,r0
+        mov     r2,r1
+#endif
+	rts
+	mov.l   @r15+,r8
+
+.L_norm_a:
+	!normalize op1
+	shll	r5
+	mov.l	.L_imp_bit,r1
+
+	rotcl	r4
+	add	#-1,r3
+
+	tst	r1,r4
+	bt	.L_norm_a
+
+	bra	.L_chk_b
+	add	#1,r3
+
+.L_norm_b:
+	!normalize op2
+        shll    r7
+        mov.l   .L_imp_bit,r1
+
+        rotcl   r6
+        add     #-1,r2
+
+        tst     r1,r6
+        bt      .L_norm_b
+
+        bra     .L_mul1
+        add     #1,r2
+
+.L_shift_rt_by_1:
+	!adjust the extra msb
+
+	add     #1,r2           !add 1 to exponent
+	mov.l	.L_2047,r6
+
+	cmp/ge	r6,r2
+	mov	#20,r6
+
+	bt	.L_return_inf
+	shlr	r7		!r7 contains bit 96-105 of product
+
+	rotcr	r3		!r3 contains bit 64-95 of product
+
+	rotcr	r8		!r8 contains bit 32-63 of product
+	bra	.L_pack_mantissa
+
+	rotcr	r1		!r1 contains bit 31-0 of product
+
+.L_return_inf:
+	!return Inf
+	mov.l	.L_inf,r2
+	mov     #0,r1
+
+	or	r2,r0
+	mov.l   @r15+,r9
+
+#ifdef __LITTLE_ENDIAN__
+        mov     r0,r2
+        mov     r1,r0
+        mov     r2,r1
+#endif
+	rts
+	mov.l   @r15+,r8
+	
+.L_underflow:
+	!check if the result needs to be denormalized
+	mov	#-53,r1
+	add	#1,r2
+
+	cmp/gt	r2,r1
+	mov	#32,r4
+
+	add	#-2,r2
+	bt	.L_return_zero
+
+	add	r2,r4
+	mov	r7,r1
+	
+	cmp/ge	r7,r4
+	mov	r2,r6
+
+	mov	#-54,r2
+	bt	.L_denorm
+
+	mov	#-32,r6
+	
+.L_denorm:
+	!denormalize the result
+	shlr	r8
+	rotcr	r1	
+
+	shll	r8
+	add	#1,r6
+
+	shlr	r3
+	rotcr	r8
+
+	cmp/eq	r7,r6
+	bf	.L_denorm
+
+	mov	r4,r6
+	cmp/eq	r2,r4
+
+	bt	.L_break
+	mov	r7,r5
+
+	cmp/gt	r6,r7
+	bf	.L_break
+
+	mov	r2,r4
+	mov	r1,r5
+
+	mov	r7,r1
+	bt	.L_denorm
+
+.L_break:
+	mov	#0,r2
+
+	cmp/gt	r1,r2
+
+	addc	r2,r8
+	mov.l	.L_comp_1,r4
+	
+	addc	r7,r3
+	or	r3,r0
+
+	cmp/eq	r9,r7
+	bf	.L_return
+
+	cmp/eq	r7,r5
+	mov.l	.L_mask_sign,r6
+
+	bf	.L_return
+	cmp/eq	r1,r6
+	
+	bf	.L_return
+	and	r4,r8
+
+.L_return:
+	mov.l	@r15+,r9
+	mov	r8,r1
+
+#ifdef __LITTLE_ENDIAN__
+        mov     r0,r2
+        mov     r1,r0
+        mov     r2,r1
+#endif
+	rts
+	mov.l   @r15+,r8
+
+.L_return_zero:
+	mov.l	@r15+,r9
+	mov	r7,r1
+
+#ifdef __LITTLE_ENDIAN__
+        mov     r0,r2
+        mov     r1,r0
+        mov     r2,r1
+#endif
+
+	rts
+	mov.l	@r15+,r8
+
+	.align	2
+
+.L_mask_high_mant:
+	.long	0x000fffff
+.L_inf:
+	.long	0x7ff00000	
+.L_mask_sign:
+	.long	0x80000000
+.L_1023:
+	.long	-1023
+.L_2047:
+	.long	2047
+.L_imp_bit:
+	.long	0x00100000
+.L_mask_22_bit:
+	.long	0x00200000
+.L_106_bit:
+	.long	0x00000200
+.L_comp_1:
+	.long	0xfffffffe
+
+ENDFUNC (GLOBAL (muldf3))
diff -Naur gcc-4.2.1/gcc/config/sh/IEEE-754/mulsf3.S gcc-4.2.1-stm/gcc/config/sh/IEEE-754/mulsf3.S
--- gcc-4.2.1/gcc/config/sh/IEEE-754/mulsf3.S	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/IEEE-754/mulsf3.S	2007-08-10 13:08:59.000000000 +0100
@@ -0,0 +1,352 @@
+/* Copyright (C) 2004, 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+! Routine for multiplying two floating point numbers
+
+! Author: Rakesh Kumar
+
+! Arguments: r4 and r5
+! Result: r0
+
+! The arguments are referred as op1 and op2
+!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+
+        .text
+        .align 5
+        .global GLOBAL (mulsf3)
+        FUNC (GLOBAL (mulsf3))
+
+GLOBAL (mulsf3):
+	! Extract the sign bits
+	mov.l	.L_sign,r3
+	mov	r3,r0
+
+	and	r4,r3		! sign bit for op1
+	mov.l	.L_sign_mask,r6
+
+	! Mask out the sign bit from op1 and op2
+	and	r5,r0		! sign bit for op2
+	mov.l	.L_inf,r2
+
+	and	r6,r4
+	xor	r3,r0		! Final sign in r0
+
+	and	r6,r5
+	tst	r4,r4
+
+	! Check for zero
+	mov	r5,r7
+	! Check op1 for zero
+	SL(bt,	.L_op1_zero,
+	 mov	r4,r6)
+
+	tst	r5,r5
+	bt	.L_op2_zero	! op2 is zero
+
+	! Extract the exponents
+	and	r2,r6		! Exponent of op1
+	cmp/eq	r2,r6
+
+	and	r2,r7
+	bt	.L_inv_op1	! op1 is NaN or Inf
+
+	mov.l	.L_mant,r3
+	cmp/eq	r2,r7
+
+	and	r3,r4	! Mantissa of op1
+	bt	.L_ret_op2	! op2 is Nan or Inf
+
+	and	r3,r5	! Mantissa of op2
+
+	mov	#-23,r3
+#if defined (__sh1__) || defined (__sh2__) || defined (__SH2E__)
+	SHLR23 (r6)
+	SHLR23 (r7)
+#else
+	shld	r3,r6
+	shld	r3,r7
+#endif
+	! Check for denormals
+	mov.l	.L_24bit,r3
+	tst	r6,r6
+
+	bt	.L_norm_op1	! op1 is denormal
+	add	#-127,r6	! Unbias op1's exp
+
+	tst	r7,r7
+	bt	.L_norm_op2	! op2 is denormal
+
+	add	#-127,r7	! Unbias op2's exp
+
+.L_multiply:
+	add	r6,r7	! Final exponent in r7
+	mov.l	.L_24bit,r1
+
+	! set 24th bit of mantissas
+	mov	#127,r3
+	or	r1,r4
+
+	DMULU_SAVE
+
+	! Multiply
+	or	r1,r5
+	DMULUL	(r4,r5,r4)
+
+	DMULUH	(r5)
+
+	DMULU_RESTORE
+
+	mov.l	.L_16bit,r6
+
+	! Check for extra MSB generated
+	tst	r5,r6
+
+	mov.l	.L_255,r1
+	bf	.L_shift_by_1	! Adjust the extra MSB
+	
+! Normalize the result with rounding
+.L_epil:
+	! Bias the exponent
+	add	#127,r7
+	cmp/ge	r1,r7
+	
+	! Check exponent overflow and underflow
+	bt	.L_ret_inf
+
+	cmp/pl	r7
+	bf	.L_denorm
+
+.L_epil_0:
+	mov	#-23,r3
+	shll	r5
+	mov	#0,r6
+
+! Fit resultant mantissa in 24 bits
+! Apply default rounding
+.L_loop_epil_0:
+        tst	r3,r3
+	bt	.L_loop_epil_out
+
+	add	#1,r3
+	shlr	r4
+
+	bra	.L_loop_epil_0
+	rotcr	r6
+
+! Round mantissa
+.L_loop_epil_out:
+	shll8	r5
+	or	r5,r4
+
+	mov.l	.L_mant,r2
+	mov	#23,r3
+
+	! Check last bit shifted out of result
+	tst	r6,r6
+	bt	.L_epil_2
+
+	! Round
+	shll	r6
+	movt	r5
+
+	add	r5,r4
+
+	! If this is the only ON bit shifted
+	! Round towards LSB = 0
+	tst	r6,r6
+	bf	.L_epil_2
+
+	shlr	r4
+	shll	r4
+
+.L_epil_2:
+	! Rounding may have produced extra MSB.
+	mov.l	.L_25bit,r5
+	tst	r4,r5
+
+	bt	.L_epil_1
+
+	add	#1,r7
+	shlr	r4
+
+.L_epil_1:
+#if defined (__sh1__) || defined (__sh2__) || defined (__SH2E__)
+	SHLL23 (r7)
+#else
+	shld	r3,r7
+#endif
+
+	and	r2,r4
+
+	or	r7,r4
+	rts
+	or	r4,r0
+
+.L_denorm:
+	mov	#0,r3
+
+.L_den_1:
+	shlr	r5
+	rotcr	r4
+
+	cmp/eq	r3,r7
+	bt	.L_epil_0
+
+	bra	.L_den_1
+	add	#1,r7
+	
+
+! Normalize the first argument
+.L_norm_op1:
+	shll	r4
+	tst	r3,r4
+
+	add	#-1,r6
+	bt	.L_norm_op1
+
+	! The biasing is by 126
+	add	#-126,r6
+	tst	r7,r7
+
+	bt      .L_norm_op2
+
+	bra	.L_multiply
+	add	#-127,r7
+
+! Normalize the second argument
+.L_norm_op2:
+	shll	r5
+	tst	r3,r5
+
+	add	#-1,r7
+	bt	.L_norm_op2
+
+	bra	.L_multiply
+	add	#-126,r7
+
+! op2 is zero. Check op1 for exceptional cases
+.L_op2_zero:
+	mov.l	.L_inf,r2
+	and	r2,r6
+
+	! Check if op1 is deterministic
+	cmp/eq	r2,r6
+	SL(bf,	.L_ret_op2,
+	 mov	#1,r1)
+
+	! Return NaN
+	rts
+	mov	#-1,r0
+
+! Adjust the extra MSB
+.L_shift_by_1:
+	shlr	r5
+	rotcr	r4
+
+	add	#1,r7		! Show the shift in exponent
+
+	cmp/gt	r3,r7
+	bf	.L_epil
+
+	! The resultant exponent is invalid
+	mov.l	.L_inf,r1
+	rts
+	or	r1,r0
+
+.L_ret_op1:
+	rts
+	or	r4,r0
+
+! op1 is zero. Check op2 for exceptional cases
+.L_op1_zero:
+	mov.l	.L_inf,r2
+	and	r2,r7
+	
+	! Check if op2 is deterministic
+	cmp/eq	r2,r7
+	SL(bf,	.L_ret_op1,
+	 mov	#1,r1)
+
+	! Return NaN
+	rts
+	mov	#-1,r0
+
+.L_inv_op1:
+	mov.l	.L_mant,r3
+	mov	r4,r6
+
+	and	r3,r6
+	tst	r6,r6
+
+	bf	.L_ret_op1	! op1 is Nan
+	! op1 is not Nan. It is Inf
+
+	cmp/eq	r2,r7
+	bf	.L_ret_op1	! op2 has a valid exponent
+
+! op2 has a invalid exponent. It could be Inf, -Inf, Nan.
+! It doesn't make any difference.
+.L_ret_op2:
+	rts
+	or	r5,r0
+
+.L_ret_inf:
+	rts
+	or	r2,r0
+
+.L_ret_zero:
+	mov	#0,r2
+	rts
+	or	r2,r0
+
+	
+	.align 2
+.L_mant:
+	.long 0x007FFFFF
+
+.L_inf:
+	.long 0x7F800000
+
+.L_24bit:
+	.long 0x00800000
+
+.L_25bit:
+	.long 0x01000000
+
+.L_16bit:
+	.long 0x00008000
+
+.L_sign:
+	.long 0x80000000
+
+.L_sign_mask:
+	.long 0x7FFFFFFF
+
+.L_255:
+	.long 0x000000FF
+
+ENDFUNC (GLOBAL (mulsf3))
diff -Naur gcc-4.2.1/gcc/config/sh/ieee-754-df.S gcc-4.2.1-stm/gcc/config/sh/ieee-754-df.S
--- gcc-4.2.1/gcc/config/sh/ieee-754-df.S	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/ieee-754-df.S	2007-08-10 13:08:59.000000000 +0100
@@ -0,0 +1,789 @@
+/* Copyright (C) 2004, 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!! libgcc software floating-point routines for Renesas SH /
+!! STMicroelectronics ST40 CPUs
+!! Contributed by J"orn Rennecke joern.rennecke@st.com
+
+#ifndef __SH_FPU_DOUBLE__
+
+#include "lib1funcs.h"
+#include "insn-constants.h"
+
+/* Double-precision floating-point emulation.
+   We handle NANs, +-infinity, and +-zero.
+   However, we assume that for NANs, the topmost bit of the fraction is set.  */
+
+#ifdef __LITTLE_ENDIAN__
+#define DBL0L r4
+#define DBL0H r5
+#define DBL1L r6
+#define DBL1H r7
+#define DBLRL r0
+#define DBLRH r1
+#else
+#define DBL0L r5
+#define DBL0H r4
+#define DBL1L r7
+#define DBL1H r6
+#define DBLRL r1
+#define DBLRH r0
+#endif
+
+#ifdef __SH_FPU_ANY__
+#define RETURN_R0_MAIN
+#define RETURN_R0 bra LOCAL(return_r0)
+#define RETURN_FR0
+LOCAL(return_r0): \
+ lds r0,fpul; \
+ rts; \
+ fsts fpul,fr0
+#define ARG_TO_R4 \
+ flds fr4,fpul; \
+ sts fpul,r4
+#else /* ! __SH_FPU_ANY__ */
+#define RETURN_R0_MAIN rts
+#define RETURN_R0 rts
+#define RETURN_FR0
+#define ARG_TO_R4
+#endif /* ! __SH_FPU_ANY__ */
+
+#ifdef L_nedf2f
+/* -ffinite-math-only -mb inline version, T := r4:DF == r6:DF
+	cmp/eq	r5,r7
+	mov	r4,r0
+	bf	0f
+	cmp/eq	r4,r6
+	bt	0f
+	or	r6,r0
+	add	r0,r0
+	or	r5,r0
+	tst	r0,r0
+	0:			*/
+	.balign 4
+	.global GLOBAL(nedf2f)
+	HIDDEN_FUNC(GLOBAL(nedf2f))
+GLOBAL(nedf2f):
+	cmp/eq	DBL0L,DBL1L
+	mov.l   LOCAL(c_DF_NAN_MASK),r1
+	bf LOCAL(ne)
+	cmp/eq	DBL0H,DBL1H
+	not	DBL0H,r0
+	bt	LOCAL(check_nan)
+	mov	DBL0H,r0
+	or	DBL1H,r0
+	add	r0,r0
+	rts
+	or	DBL0L,r0
+LOCAL(check_nan):
+	tst	r1,r0
+	rts
+	movt	r0
+LOCAL(ne):
+	rts
+	mov #1,r0
+	.balign 4
+LOCAL(c_DF_NAN_MASK):
+	.long DF_NAN_MASK
+	ENDFUNC(GLOBAL(nedf2f))
+#endif /* L_nedf2f */
+
+#ifdef L_unorddf2
+	.balign 4
+	.global GLOBAL(unorddf2)
+	HIDDEN_FUNC(GLOBAL(unorddf2))
+GLOBAL(unorddf2):
+	mov.l	LOCAL(c_DF_NAN_MASK),r1
+	not	DBL0H,r0
+	tst	r1,r0
+	not	r6,r0
+	bt	LOCAL(unord)
+	tst	r1,r0
+LOCAL(unord):
+	rts
+	movt	r0
+	.balign	4
+LOCAL(c_DF_NAN_MASK):
+	.long DF_NAN_MASK
+	ENDFUNC(GLOBAL(unorddf2))
+#endif /* L_unorddf2 */
+
+#if defined(L_gtdf2t) || defined(L_gtdf2t_trap)
+#ifdef L_gtdf2t
+#define fun_label GLOBAL(gtdf2t)
+#else
+#define fun_label GLOBAL(gtdf2t_trap)
+#endif
+	.balign 4
+	.global fun_label
+	HIDDEN_FUNC(fun_label)
+fun_label:
+	/* If the raw values compare greater, the result true, unless
+	   any of them is a nan (but infinity is fine), or both values are
+	   +- zero.  Otherwise, the result false.  */
+	mov.l	LOCAL(c_DF_NAN_MASK),r1
+	cmp/pz	DBL0H
+	not	DBL1H,r0
+	SLC(bf,	LOCAL(neg),
+	 tst	r1,r0)
+	mov	DBL0H,r0
+	bt	LOCAL(nan) /* return zero if DBL1 is NAN.  */
+	cmp/eq	DBL1H,DBL0H
+	bt	LOCAL(cmp_low)
+	cmp/gt	DBL1H,DBL0H
+	or	DBL1H,r0
+	SLC(bf,	LOCAL(check_nan),
+	 cmp/gt	DBL0H,r1)
+	add	r0,r0
+	bf	LOCAL(nan) /* return zero if DBL0 is NAN.  */
+	or	DBL0L,r0
+	rts
+	or	DBL1L,r0 /* non-zero unless both DBL0 and DBL1 are +-zero.  */
+LOCAL(cmp_low):
+	cmp/hi	DBL1L,DBL0L
+	rts
+	movt	r0
+LOCAL(neg):
+	SLI(tst	r1,r0)
+	bt	LOCAL(nan) /* return zero if DBL1 is NAN.  */
+	cmp/eq	DBL1H,DBL0H
+	SLC(bt,	LOCAL(neg_cmp_low),
+	 cmp/hi	DBL0L,DBL1L)
+	not	DBL0H,r0
+	tst	r1,r0
+	bt	LOCAL(nan) /* return zero if DBL0 is NAN.  */
+	cmp/hi	DBL0H,DBL1H
+	SLI(rts	!,)
+	SLI(movt r0 !,)
+LOCAL(neg_cmp_low):
+	SLI(cmp/hi	DBL0L,DBL1L)
+	rts
+	movt	r0
+LOCAL(check_nan):
+#ifdef L_gtdf2t
+LOCAL(nan):
+	rts
+	mov	#0,r0
+#else
+	SLI(cmp/gt DBL0H,r1)
+	bf	LOCAL(nan) /* return zero if DBL0 is NAN.  */
+	rts
+	mov	#0,r0
+LOCAL(nan):
+	mov	#0,r0
+	trapa	#0
+#endif
+	.balign	4
+LOCAL(c_DF_NAN_MASK):
+	.long DF_NAN_MASK
+	ENDFUNC(fun_label)
+#endif /* defined(L_gtdf2t) || defined(L_gtdf2t_trap) */
+
+#ifdef L_gedf2f
+	.balign 4
+	.global GLOBAL(gedf2f)
+	HIDDEN_FUNC(GLOBAL(gedf2f))
+GLOBAL(gedf2f):
+	/* If the raw values compare greater or equal, the result is
+	   true, unless any of them is a nan, or both are the
+	   same infinity.  If both are -+zero, the result is true;
+	   otherwise, it is false.
+	   We use 0 as true and nonzero as false for this function.  */
+	mov.l	LOCAL(c_DF_NAN_MASK),r1
+	cmp/pz	DBL1H
+	not	DBL0H,r0
+	SLC(bf,	LOCAL(neg),
+	 tst	r1,r0)
+	mov	DBL0H,r0
+	bt	LOCAL(nan)
+	cmp/eq	DBL0H,DBL1H
+	bt	LOCAL(cmp_low)
+	cmp/gt	DBL0H,DBL1H
+	or	DBL1H,r0
+	SLC(bf,	LOCAL(check_nan),
+	 cmp/ge	r1,DBL1H)
+	add	r0,r0
+	bt	LOCAL(nan)
+	or	DBL0L,r0
+	rts
+	or	DBL1L,r0
+LOCAL(cmp_low):
+	cmp/hi	DBL0L,DBL1L
+#if defined(L_gedf2f) && defined(DELAYED_BRANCHES)
+LOCAL(nan): LOCAL(check_nan):
+#endif
+	rts
+	movt	r0
+#if defined(L_gedf2f) && ! defined(DELAYED_BRANCHES)
+LOCAL(check_nan):
+	SLI(cmp/ge	r1,DBL1H)
+LOCAL(nan):
+	rts
+	movt	r0
+#elif defined(L_gedf2f_trap)
+LOCAL(check_nan):
+	SLI(cmp/ge	r1,DBL1H)
+	bt	LOCAL(nan)
+	rts
+LOCAL(nan):
+	movt	r0
+	trapa	#0
+#endif /* L_gedf2f_trap */
+LOCAL(neg):
+	SLI(tst	r1,r0)
+	bt	LOCAL(nan)
+	cmp/eq	DBL0H,DBL1H
+	not	DBL1H,r0
+	SLC(bt,	LOCAL(neg_cmp_low),
+	 cmp/hi	DBL1L,DBL0L)
+	tst	r1,r0
+	bt	LOCAL(nan)
+	cmp/hi	DBL1H,DBL0H
+	SLI(rts !,)
+	SLI(movt	r0 !,)
+LOCAL(neg_cmp_low):
+	SLI(cmp/hi	DBL1L,DBL0L)
+	rts
+	movt	r0
+	.balign	4
+LOCAL(c_DF_NAN_MASK):
+	.long DF_NAN_MASK
+	ENDFUNC(GLOBAL(gedf2f))
+#endif /* L_gedf2f */
+
+#ifndef DYN_SHIFT /* SH1 / SH2 code */
+#ifdef L_extendsfdf2
+	.balign 4
+	.global GLOBAL(extendsfdf2)
+	FUNC(GLOBAL(extendsfdf2))
+GLOBAL(extendsfdf2):
+	ARG_TO_R4
+	mov.l	LOCAL(x7f800000),r3
+	mov	r4,DBLRL
+	tst	r3,r4
+	bt	LOCAL(zero_denorm)
+	mov.l	LOCAL(xe0000000),r2
+	rotr	DBLRL
+	rotr	DBLRL
+	rotr	DBLRL
+	and	r2,DBLRL
+	mov	r4,DBLRH
+	not	r4,r2
+	tst	r3,r2
+	mov.l	LOCAL(x38000000),r2
+	bf	0f
+	add	r2,r2	! infinity / NaN adjustment
+0:	shll	DBLRH
+	shlr2	DBLRH
+	shlr2	DBLRH
+	add	DBLRH,DBLRH
+	rotcr	DBLRH
+	rts
+	add	r2,DBLRH
+LOCAL(zero_denorm):
+	mov.l	r4,@-r15
+	add	r4,r4
+	tst	r4,r4
+	bt	LOCAL(zero)
+	shlr8	r3	/* 0x007f8000 */
+	mov.w	LOCAL(x389),r2
+LOCAL(shift_byte):
+	tst	r3,r4
+	shll8	r4
+	SL(bt,	LOCAL(shift_byte),
+	 add	#-8,r2)
+LOCAL(shift_bit):
+	shll	r4
+	SL(bf,	LOCAL(shift_bit),
+	 add	#-1,r2)
+	mov	#0,DBLRL
+	mov	r4,DBLRH
+	mov.l	@r15+,r4
+	shlr8	DBLRH
+	shlr2	DBLRH
+	shlr	DBLRH
+	rotcr	DBLRL
+	cmp/gt	r4,DBLRH	! get sign
+	rotcr	DBLRH
+	rotcr	DBLRL
+	shll16	r2
+	shll8	r2
+	rts
+	add	r2,DBLRH
+LOCAL(zero):
+	mov.l	@r15+,DBLRH
+	rts
+	mov	#0,DBLRL
+LOCAL(x389):	.word 0x389
+	.balign	4
+LOCAL(x7f800000):
+	.long	0x7f800000
+LOCAL(x38000000):
+	.long	0x38000000
+LOCAL(xe0000000):
+	.long	0xe0000000
+	ENDFUNC(GLOBAL(extendsfdf2))
+#endif /* L_extendsfdf2 */
+
+#ifdef L_truncdfsf2
+	.balign 4
+	.global GLOBAL(truncdfsf2)
+	FUNC(GLOBAL(truncdfsf2))
+GLOBAL(truncdfsf2):
+	mov.l	LOCAL(x38000000),r3	! exponent adjustment DF -> SF
+	mov	DBL0H,r1
+	mov.l	LOCAL(x70000000),r2	! mask for out-of-range exponent bits
+	mov	DBL0H,r0
+	mov.l	DBL0L,@-r15
+	sub	r3,r1
+	tst	r2,r1
+	shll8	r0			!
+	shll2	r0			! Isolate highpart fraction.
+	shll2	r0			!
+	bf	LOCAL(ill_exp)
+	shll2	r1
+	mov.l	LOCAL(x2fffffff),r2 /* Fraction lsb | lower guard bits.  */
+	shll2	r1
+	mov.l	LOCAL(xff000000),r3
+	shlr8	r0
+	tst	r2,DBL0L /* Check if msb guard bit wants rounding up.  */
+	shlr16	DBL0L
+	shlr8	DBL0L
+	shlr2	DBL0L
+	SL1(bt,	LOCAL(add_frac),
+	 shlr2	DBL0L)
+	add	#1,DBL0L
+LOCAL(add_frac):
+	add	DBL0L,r0
+	mov.l	LOCAL(x01000000),r2
+	and	r3,r1
+	mov.l	@r15+,DBL0L
+	add	r1,r0
+	tst	r3,r0
+	bt	LOCAL(inf_denorm0)
+	cmp/hs	r3,r0
+LOCAL(denorm_noup_sh1):
+	bt	LOCAL(inf)
+	div0s	DBL0H,r2	/* copy orig. sign into T.  */
+RETURN_R0_MAIN
+	rotcr	r0
+RETURN_FR0
+LOCAL(inf_denorm0):	!  We might need to undo previous rounding.
+	mov.l	LOCAL(x2fffffff),r3 /* Old fraction lsb | lower guard bits.  */
+	tst	r1,r1
+	bf	LOCAL(inf)
+	add	#-1,r0
+	tst	r3,DBL0L /* Check if msb guard bit was rounded up.  */
+	mov.l	LOCAL(x5fffffff),r3 /* Fraction lsb | lower guard bits.  */
+	addc	r2,r0
+	shlr	r0
+	tst	r3,DBL0L /* Check if msb guard bit wants rounding up.  */
+#ifdef DELAYED_BRANCHES
+	bt/s	LOCAL(denorm_noup)
+#else
+	bt	LOCAL(denorm_noup_sh1)
+#endif
+	div0s	DBL0H,r2	/* copy orig. sign into T.  */
+	add	#1,r0
+LOCAL(denorm_noup):
+	RETURN_R0
+	rotcr	r0
+LOCAL(ill_exp):
+	div0s	DBL0H,r1
+	mov.l	LOCAL(x7ff80000),r2
+	add	r1,r1
+	bf	LOCAL(inf_nan)
+	mov.w	LOCAL(m32),r3 /* Handle denormal or zero.  */
+	shlr16	r1
+	exts.w	r1,r1
+	shll2	r1
+	add	r1,r1
+	shlr8	r1
+	exts.w	r1,r1
+	add	#-8,r1	/* Go from 9 to 1 guard bit in MSW.  */
+	cmp/gt	r3,r1
+	mov.l	@r15+,r3 /* DBL0L */
+	bf	LOCAL(zero)
+	mov.l	DBL0L, @-r15
+	shll8	DBL0L
+	rotcr	r0	/* Insert leading 1.  */
+	shlr16	r3
+	shll2	r3
+	add	r3,r3
+	shlr8	r3
+	cmp/pl	DBL0L	/* Check lower 23 guard bits if guard bit 23 is 0.  */
+	addc	r3,r0	/* Assemble fraction with compressed guard bits.  */
+	mov.l	@r15+,DBL0L
+	mov	#0,r2
+	neg	r1,r1
+LOCAL(denorm_loop):
+	shlr	r0
+	rotcl	r2
+	dt	r1
+	bf	LOCAL(denorm_loop)
+	tst	#2,r0
+	rotcl	r0
+	tst	r2,r2
+	rotcl	r0
+	xor	#3,r0
+	add	#3,r0	/* Even overflow gives the correct result.  */
+	shlr2	r0
+	div0s	r0,DBL0H
+	RETURN_R0
+	rotcr	r0
+LOCAL(zero):
+	mov	#0,r0
+	div0s	r0,DBL0H
+	RETURN_R0
+	rotcr	r0
+LOCAL(inf_nan):
+	not	DBL0H,r0
+	tst	r2,r0
+	mov.l	@r15+,DBL0L
+	bf	LOCAL(inf)
+	RETURN_R0
+	mov	#-1,r0	/* NAN */
+LOCAL(inf):	/* r2 must be positive here.  */
+	mov.l	LOCAL(xffe00000),r0
+	div0s	r2,DBL0H
+	RETURN_R0
+	rotcr	r0
+LOCAL(m32):
+	.word	-32
+	.balign	4
+LOCAL(x38000000):
+	.long	0x38000000
+LOCAL(x70000000):
+	.long	0x70000000
+LOCAL(x2fffffff):
+	.long	0x2fffffff
+LOCAL(x01000000):
+	.long	0x01000000
+LOCAL(xff000000):
+	.long	0xff000000
+LOCAL(x5fffffff):
+	.long	0x5fffffff
+LOCAL(x7ff80000):
+	.long	0x7ff80000
+LOCAL(xffe00000):
+	.long	0xffe00000
+	ENDFUNC(GLOBAL(truncdfsf2))
+#endif /*  L_truncdfsf2 */
+#ifdef L_add_sub_df3
+#include "IEEE-754/adddf3.S"
+#endif /* _add_sub_df3 */
+
+#ifdef L_muldf3
+#include "IEEE-754/muldf3.S"
+#endif /* L_muldf3 */
+
+#ifdef L_fixunsdfsi
+#include "IEEE-754/fixunsdfsi.S"
+#endif /* L_fixunsdfsi */
+
+#ifdef L_fixdfsi
+#include "IEEE-754/fixdfsi.S"
+#endif /* L_fixdfsi */
+
+#ifdef L_floatunssidf
+#include "IEEE-754/floatunssidf.S"
+#endif /* L_floatunssidf */
+
+#ifdef L_floatsidf
+#include "IEEE-754/floatsidf.S"
+#endif /* L_floatsidf */
+
+#ifdef L_divdf3
+#include "IEEE-754/divdf3.S"
+#endif /* L_divdf3 */
+#endif /* ! DYN_SHIFT */
+
+/* The actual arithmetic uses dynamic shift.  Supporting SH1 / SH2 here would
+   make this code too hard to maintain, so if you want to add SH1 / SH2
+   support, do it in a separate copy.  */
+#ifdef DYN_SHIFT
+#ifdef L_extendsfdf2
+	.balign 4
+	.global GLOBAL(extendsfdf2)
+	FUNC(GLOBAL(extendsfdf2))
+GLOBAL(extendsfdf2):
+	ARG_TO_R4
+	mov.l	LOCAL(x7f800000),r2
+	mov	#29,r3
+	mov	r4,DBLRL
+	not	r4,DBLRH
+	tst	r2,r4
+	shld	r3,DBLRL
+	bt	LOCAL(zero_denorm)
+	mov	#-3,r3
+	tst	r2,DBLRH
+	mov	r4,DBLRH
+	mov.l	LOCAL(x38000000),r2
+	bt/s	LOCAL(inf_nan)
+	 shll	DBLRH
+	shld	r3,DBLRH
+	rotcr	DBLRH
+	rts
+	add	r2,DBLRH
+	.balign	4
+LOCAL(inf_nan):
+	shld	r3,DBLRH
+	add	r2,r2
+	rotcr	DBLRH
+	rts
+	add	r2,DBLRH
+LOCAL(zero_denorm):
+	mov.l	r4,@-r15
+	add	r4,r4
+	tst	r4,r4
+	extu.w	r4,r2
+	bt	LOCAL(zero)
+	cmp/eq	r4,r2
+	extu.b	r4,r1
+	bf/s	LOCAL(three_bytes)
+	 mov.l	LOCAL(c__clz_tab),r0
+	cmp/eq	r4,r1
+	mov	#22,DBLRH
+	bt	LOCAL(one_byte)
+	shlr8	r2
+	mov	#14,DBLRH
+LOCAL(one_byte):
+#ifdef __pic__
+	add	r0,r2
+	mova  LOCAL(c__clz_tab),r0
+#endif
+	mov.b	@(r0,r2),r2
+	mov	#21,r3
+	mov.w	LOCAL(x0),DBLRL
+	sub	r2,DBLRH
+LOCAL(norm_shift):
+	shld	DBLRH,r4
+	mov.l	@r15+,r2
+	shld	r3,DBLRH
+	mov.l	LOCAL(xb7ffffff),r3
+	add	r4,DBLRH
+	cmp/pz	r2
+	mov	r2,r4
+	rotcr	DBLRH
+	rts
+	sub	r3,DBLRH
+LOCAL(three_bytes):
+	mov	r4,r2
+	shlr16	r2
+#ifdef __pic__
+	add	r0,r2
+	mova  LOCAL(c__clz_tab),r0
+#endif
+	mov.b	@(r0,r2),r2
+	mov	#21,r3
+	mov	#6-32,DBLRH
+	sub	r2,DBLRH
+	mov	r4,DBLRL
+	shld	DBLRH,DBLRL
+	bra	LOCAL(norm_shift)
+	add	#32,DBLRH
+LOCAL(zero):
+	rts	/* DBLRL has already been zeroed above.  */
+	mov.l @r15+,DBLRH
+LOCAL(x0):
+	.word 0
+	.balign	4
+LOCAL(x7f800000):
+	.long	0x7f800000
+LOCAL(x38000000):
+	.long	0x38000000
+LOCAL(xb7ffffff):
+	/* Flip sign back, do exponent adjustment, and remove leading one.  */
+	.long 0x80000000 + 0x38000000 - 1
+LOCAL(c__clz_tab):
+#ifdef __pic__
+	.long	GLOBAL(clz_tab) - .
+#else
+	.long	GLOBAL(clz_tab)
+#endif
+	ENDFUNC(GLOBAL(extendsfdf2))
+#endif /* L_extendsfdf2 */
+
+#ifdef L_truncdfsf2
+	.balign 4
+	.global GLOBAL(truncdfsf2)
+	FUNC(GLOBAL(truncdfsf2))
+GLOBAL(truncdfsf2):
+	mov.l	LOCAL(x38000000),r3
+	mov	DBL0H,r1
+	mov.l	LOCAL(x70000000),r2
+	mov	DBL0H,r0
+	sub	r3,r1
+	mov.l	DBL0L,@-r15
+	tst	r2,r1
+	mov	#12,r3
+	shld	r3,r0			! Isolate highpart fraction.
+	bf	LOCAL(ill_exp)
+	shll2	r1
+	mov.l	LOCAL(x2fffffff),r2 /* Fraction lsb | lower guard bits.  */
+	shll2	r1
+	mov.l	LOCAL(xff000000),r3
+	shlr8	r0
+	tst	r2,DBL0L /* Check if msb guard bit wants rounding up.  */
+	mov	#-28,r2
+	bt/s	LOCAL(add_frac)
+	 shld	r2,DBL0L
+	add	#1,DBL0L
+LOCAL(add_frac):
+	add	DBL0L,r0
+	mov.l	LOCAL(x01000000),r2
+	and	r3,r1
+	mov.l	@r15+,DBL0L
+	add	r1,r0
+	tst	r3,r0
+	bt	LOCAL(inf_denorm0)
+#if 0	// No point checking overflow -> infinity if we dont't raise a signal.
+	cmp/hs	r3,r0
+	bt	LOCAL(inf)
+#endif
+	div0s	DBL0H,r2	/* copy orig. sign into T.  */
+	RETURN_R0_MAIN
+	rotcr	r0
+RETURN_FR0
+LOCAL(inf_denorm0):	! We might need to undo previous rounding.
+	mov.l	LOCAL(x2fffffff),r3 /* Old fraction lsb | lower guard bits.  */
+	tst	r1,r1
+	bf	LOCAL(inf)
+	add	#-1,r0
+	tst	r3,DBL0L /* Check if msb guard bit was rounded up.  */
+	mov.l	LOCAL(x5fffffff),r3 /* Fraction lsb | lower guard bits.  */
+	addc	r2,r0
+	shlr	r0
+	tst	r3,DBL0L /* Check if msb guard bit wants rounding up.  */
+	bt/s	LOCAL(denorm_noup)
+	 div0s	DBL0H,r2	/* copy orig. sign into T.  */
+	add	#1,r0
+LOCAL(denorm_noup):
+	RETURN_R0
+	rotcr	r0
+LOCAL(ill_exp):
+	div0s	DBL0H,r1
+	mov.l	LOCAL(x7ff80000),r2
+	add	r1,r1
+	bf	LOCAL(inf_nan)
+	mov.w	LOCAL(m32),r3 /* Handle denormal or zero.  */
+	mov	#-21,r2
+	shad	r2,r1
+	add	#-8,r1	/* Go from 9 to 1 guard bit in MSW.  */
+	cmp/gt	r3,r1
+	mov.l	@r15+,r3 /* DBL0L */
+	bf	LOCAL(zero)
+	mov.l	DBL0L, @-r15
+	shll8	DBL0L
+	rotcr	r0	/* Insert leading 1.  */
+	shld	r2,r3
+	cmp/pl	DBL0L	/* Check lower 23 guard bits if guard bit 23 is 0.  */
+	addc	r3,r0	/* Assemble fraction with compressed guard bits.  */
+	mov	r0,r2
+	shld	r1,r0
+	mov.l	@r15+,DBL0L
+	add	#32,r1
+	shld	r1,r2
+	tst	#2,r0
+	rotcl	r0
+	tst	r2,r2
+	rotcl	r0
+	xor	#3,r0
+	add	#3,r0	/* Even overflow gives the correct result.  */
+	shlr2	r0
+	div0s	r0,DBL0H
+	RETURN_R0
+	rotcr	r0
+LOCAL(zero):
+	mov	#0,r0
+	div0s	r0,DBL0H
+	RETURN_R0
+	rotcr	r0
+LOCAL(inf_nan):
+	not	DBL0H,r0
+	tst	r2,r0
+	mov.l	@r15+,DBL0L
+	bf	LOCAL(inf)
+	RETURN_R0
+	mov	#-1,r0	/* NAN */
+LOCAL(inf):	/* r2 must be positive here.  */
+	mov.l	LOCAL(xffe00000),r0
+	div0s	r2,DBL0H
+	RETURN_R0
+	rotcr	r0
+LOCAL(m32):
+	.word	-32
+	.balign	4
+LOCAL(x38000000):
+	.long	0x38000000
+LOCAL(x70000000):
+	.long	0x70000000
+LOCAL(x2fffffff):
+	.long	0x2fffffff
+LOCAL(x01000000):
+	.long	0x01000000
+LOCAL(xff000000):
+	.long	0xff000000
+LOCAL(x5fffffff):
+	.long	0x5fffffff
+LOCAL(x7ff80000):
+	.long	0x7ff80000
+LOCAL(xffe00000):
+	.long	0xffe00000
+	ENDFUNC(GLOBAL(truncdfsf2))
+#endif /* L_truncdfsf2 */
+
+
+#ifdef L_add_sub_df3
+#include "IEEE-754/m3/adddf3.S"
+#endif /* _add_sub_df3 */
+
+#ifdef L_muldf3
+#include "IEEE-754/m3/muldf3.S"
+#endif /* L_muldf3 */
+
+#ifdef L_fixunsdfsi
+#include "IEEE-754/m3/fixunsdfsi.S"
+#endif /* L_fixunsdfsi */
+
+#ifdef L_fixdfsi
+#include "IEEE-754/m3/fixdfsi.S"
+#endif /* L_fixdfsi */
+
+#ifdef L_floatunssidf
+#include "IEEE-754/m3/floatunssidf.S"
+#endif /* L_floatunssidf */
+
+#ifdef L_floatsidf
+#include "IEEE-754/m3/floatsidf.S"
+#endif /* L_floatsidf */
+
+#ifdef L_divdf3
+#include "IEEE-754/m3/divdf3.S"
+#endif /* L_divdf3 */
+#endif /* DYN_SHIFT */
+
+#endif /* __SH_FPU_DOUBLE__ */
diff -Naur gcc-4.2.1/gcc/config/sh/ieee-754-sf.S gcc-4.2.1-stm/gcc/config/sh/ieee-754-sf.S
--- gcc-4.2.1/gcc/config/sh/ieee-754-sf.S	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/ieee-754-sf.S	2007-08-10 13:08:59.000000000 +0100
@@ -0,0 +1,697 @@
+/* Copyright (C) 2004, 2006 Free Software Foundation, Inc.
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+!! libgcc software floating-point routines for Renesas SH /
+!! STMicroelectronics ST40 CPUs
+!! Contributed by J"orn Rennecke joern.rennecke@st.com
+
+#ifndef __SH_FPU_ANY__
+
+#include "lib1funcs.h"
+#include "insn-constants.h"
+
+/* Single-precision floating-point emulation.
+   We handle NANs, +-infinity, and +-zero.
+   However, we assume that for NANs, the topmost bit of the fraction is set.  */
+#ifdef L_nesf2f
+/* -ffinite-math-only inline version, T := r4:SF == r5:SF
+	cmp/eq	r4,r5
+	mov	r4,r0
+	bt	0f
+	or	r5,r0
+	add	r0,r0
+	tst	r0,r0	! test for +0.0 == -0.0 ; -0.0 == +0.0
+	0:			*/
+	.balign 4
+	.global GLOBAL(nesf2f)
+	HIDDEN_FUNC(GLOBAL(nesf2f))
+GLOBAL(nesf2f):
+        /* If the raw values are unequal, the result is unequal, unless
+	   both values are +-zero.
+	   If the raw values are equal, the result is equal, unless
+	   the values are NaN.  */
+	cmp/eq	r4,r5
+	mov.l   LOCAL(c_SF_NAN_MASK),r1
+	not	r4,r0
+	bt	LOCAL(check_nan)
+	mov	r4,r0
+	or	r5,r0
+	rts
+	add	r0,r0
+LOCAL(check_nan):
+	tst	r1,r0
+	rts
+	movt	r0
+	.balign 4
+LOCAL(c_SF_NAN_MASK):
+	.long SF_NAN_MASK
+	ENDFUNC(GLOBAL(nesf2f))
+#endif /* L_nesf2f */
+
+#ifdef L_unordsf2
+	.balign 4
+	.global GLOBAL(unordsf2)
+	HIDDEN_FUNC(GLOBAL(unordsf2))
+GLOBAL(unordsf2):
+	mov.l	LOCAL(c_SF_NAN_MASK),r1
+	not	r4,r0
+	tst	r1,r0
+	not	r5,r0
+	bt	LOCAL(unord)
+	tst	r1,r0
+LOCAL(unord):
+	rts
+	movt	r0
+	.balign	4
+LOCAL(c_SF_NAN_MASK):
+	.long SF_NAN_MASK
+	ENDFUNC(GLOBAL(unordsf2))
+#endif /* L_unordsf2 */
+
+#if defined(L_gtsf2t) || defined(L_gtsf2t_trap)
+/* -ffinite-math-only inline version, T := r4:SF > r5:SF ? 0 : 1
+	cmp/pz	r4
+	mov	r4,r0
+	bf/s	0f
+	 cmp/hs	r5,r4
+	cmp/ge	r4,r5
+	or	r5,r0
+	bt	0f
+	add	r0,r0
+	tst	r0,r0
+	0:			*/
+#ifdef L_gtsf2t
+#define fun_label GLOBAL(gtsf2t)
+#else
+#define fun_label GLOBAL(gtsf2t_trap)
+#endif
+	.balign 4
+	.global fun_label
+	HIDDEN_FUNC(fun_label)
+fun_label:
+	/* If the raw values compare greater, the result true, unless
+	   any of them is a nan (but infinity is fine), or both values are
+	   +- zero.  Otherwise, the result false.  */
+	mov.l	LOCAL(c_SF_NAN_MASK),r1
+	cmp/pz	r4
+	not	r5,r0
+	SLC(bf,	LOCAL(neg),
+	 tst	r1,r0)
+	mov	r4,r0
+	bt	LOCAL(nan)
+	cmp/gt	r5,r4
+	SLC(bf,	LOCAL(check_nan),
+	 cmp/gt	r4,r1)
+	bf	LOCAL(nan)
+	or	r5,r0
+	rts
+	add	r0,r0
+LOCAL(neg):
+	SLI(tst	r1,r0)
+	bt	LOCAL(nan)
+	not	r4,r0
+	tst	r1,r0
+	bt	LOCAL(nan)
+	cmp/hi	r4,r5
+#if defined(L_gtsf2t) && defined(DELAYED_BRANCHES)
+LOCAL(check_nan):
+#endif /* DELAYED_BRANCHES */
+	rts
+	movt	r0
+#ifdef L_gtsf2t
+LOCAL(check_nan):
+LOCAL(nan):
+	rts
+	mov	#0,r0
+#else /* ! L_gtsf2t */
+LOCAL(check_nan):
+	SLI(cmp/gt	r4,r1)
+	bf	LOCAL(nan)
+	rts
+	movt	r0
+LOCAL(nan):
+	mov	#0,r0
+	trapa	#0
+#endif /* ! L_gtsf2t */
+	.balign	4
+LOCAL(c_SF_NAN_MASK):
+	.long SF_NAN_MASK
+	ENDFUNC(fun_label)
+#endif /* L_gtsf2t */
+
+#if defined(L_gesf2f) || defined(L_gesf2f_trap)
+/* -ffinite-math-only inline version, T := r4:SF >= r5:SF */
+	cmp/pz	r5
+	mov	r4,r0
+	bf/s	0f
+	 cmp/hs	r4,r5
+	cmp/ge	r5,r4
+	or	r5,r0
+	bt	0f
+	add	r0,r0
+	tst	r0,r0
+	0:
+#ifdef L_gesf2f
+#define fun_label GLOBAL(gesf2f)
+#else
+#define fun_label GLOBAL(gesf2f_trap)
+#endif
+	.balign 4
+	.global fun_label
+	HIDDEN_FUNC(fun_label)
+fun_label:
+	/* If the raw values compare greater or equal, the result is
+	   true, unless any of them is a nan.  If both are -+zero, the
+	   result is true; otherwise, it is false.
+	   We use 0 as true and nonzero as false for this function.  */
+	mov.l	LOCAL(c_SF_NAN_MASK),r1
+	cmp/pz	r5
+	not	r4,r0
+	SLC(bf,	LOCAL(neg),
+	 tst	r1,r0)
+	mov	r4,r0
+	bt	LOCAL(nan)
+	cmp/gt	r4,r5
+	SLC(bf,	LOCAL(check_nan),
+	 cmp/ge	r1,r5)
+	bt	LOCAL(nan)
+	or	r5,r0
+	rts
+	add	r0,r0
+LOCAL(neg):
+	SLI(tst	r1,r0)
+	bt	LOCAL(nan)
+	not	r5,r0
+	tst	r1,r0
+	bt	LOCAL(nan)
+	cmp/hi	r5,r4
+#if defined(L_gesf2f) && defined(DELAYED_BRANCHES)
+LOCAL(nan): LOCAL(check_nan):
+#endif
+	rts
+	movt	r0
+#if defined(L_gesf2f) && ! defined(DELAYED_BRANCHES)
+LOCAL(check_nan):
+	cmp/ge	r1,r5
+LOCAL(nan):
+	rts
+	movt	r0
+#endif /* ! DELAYED_BRANCHES */
+#ifdef L_gesf2f_trap
+LOCAL(check_nan):
+	SLI(cmp/ge	r1,r5)
+	bt	LOCAL(nan)
+	rts
+LOCAL(nan):
+	movt	r0
+	trapa	#0
+#endif /* L_gesf2f_trap */
+	.balign	4
+LOCAL(c_SF_NAN_MASK):
+	.long SF_NAN_MASK
+	ENDFUNC(GLOBAL(gesf2f))
+#endif /* L_gesf2f */
+
+#ifndef DYN_SHIFT /* SH1 / SH2 code */
+#ifdef L_add_sub_sf3
+#include "IEEE-754/addsf3.S"
+#endif /* _add_sub_sf3 */
+
+#ifdef L_mulsf3
+#include "IEEE-754/mulsf3.S"
+#endif /* L_mulsf3 */
+
+#ifdef L_fixunssfsi
+#include "IEEE-754/fixunssfsi.S"
+#endif /* L_fixunssfsi */
+
+#ifdef L_fixsfsi
+#include "IEEE-754/fixsfsi.S"
+#endif /* L_fixsfsi */
+
+#ifdef L_floatunssisf
+#include "IEEE-754/floatunssisf.S"
+#endif /* L_floatunssisf */
+
+#ifdef L_floatsisf
+#include "IEEE-754/floatsisf.S"
+#endif /* L_floatsisf */
+
+#ifdef L_divsf3
+#include "IEEE-754/divsf3.S"
+#endif /* L_divsf3 */
+#endif /* ! DYN_SHIFT */
+
+/* The actual arithmetic uses dynamic shift.  Supporting SH1 / SH2 here would
+   make this code too hard to maintain, so if you want to add SH1 / SH2
+   support, do it in a separate copy.  */
+#ifdef DYN_SHIFT
+#ifdef L_add_sub_sf3
+#include "IEEE-754/m3/addsf3.S"
+#endif /* L_add_sub_sf3 */
+
+#ifdef L_mulsf3
+#include "IEEE-754/m3/mulsf3.S"
+#endif /* L_mulsf3 */
+
+#ifdef L_fixunssfsi
+	! What is a bit unusal about this implementation is that the
+	! sign bit influences the result for NANs: for cleared sign bit, you
+	! get UINT_MAX, for set sign bit, you get 0.
+	! However, since the result for NANs is undefined, this should be no
+	! problem.
+	! N.B. This is scheduled both for SH4-200 and SH4-300
+	.balign 4
+	.global GLOBAL(fixunssfsi)
+	FUNC(GLOBAL(fixunssfsi))
+GLOBAL(fixunssfsi):
+	mov.l	LOCAL(max),r2
+	mov	#-23,r1
+	mov	r4,r0
+	shad	r1,r4
+	mov.l	LOCAL(mask),r1
+	add	#-127,r4
+	cmp/ge	r2,r0
+	or	r2,r0
+	bt	LOCAL(retmax)
+	cmp/pz	r4
+	and	r1,r0
+	bf	LOCAL(ret0)
+	add	#-23,r4
+	rts
+	shld	r4,r0
+LOCAL(ret0):
+LOCAL(retmax):
+	rts
+	subc	r0,r0
+	.balign 4
+LOCAL(mask):
+	.long	0x00ffffff
+LOCAL(max):
+	.long	0x4f800000
+	ENDFUNC(GLOBAL(fixunssfsi))
+#endif /* L_fixunssfsi */
+
+#ifdef L_fixsfsi
+	! What is a bit unusal about this implementation is that the
+	! sign bit influences the result for NANs: for cleared sign bit, you
+	! get INT_MAX, for set sign bit, you get INT_MIN.
+	! However, since the result for NANs is undefined, this should be no
+	! problem.
+	! N.B. This is scheduled both for SH4-200 and SH4-300
+	.balign 4
+	.global GLOBAL(fixsfsi)
+	FUNC(GLOBAL(fixsfsi))
+	.balign	4
+GLOBAL(fixsfsi):
+	mov	r4,r0
+	shll	r4
+	mov	#-24,r1
+	bt	LOCAL(neg)
+	mov.l	LOCAL(max),r2
+	shld	r1,r4
+	mov.l	LOCAL(mask),r1
+	add	#-127,r4
+	cmp/pz	r4
+	add	#-23,r4
+	bf	LOCAL(ret0)
+	cmp/gt	r0,r2
+	bf	LOCAL(retmax)
+	and	r1,r0
+	addc	r1,r0
+	rts
+	shld	r4,r0
+
+	.balign	4
+LOCAL(neg):
+	mov.l	LOCAL(min),r2
+	shld	r1,r4
+	mov.l	LOCAL(mask),r1
+	add	#-127,r4
+	cmp/pz	r4
+	add	#-23,r4
+	bf	LOCAL(ret0)
+	cmp/gt	r0,r2
+	bf	LOCAL(retmin)
+	and	r1,r0
+	addc	r1,r0
+	shld	r4,r0	! SH4-200 will start this insn on a new cycle
+	rts
+	neg	r0,r0
+
+	.balign	4
+LOCAL(ret0):
+	rts
+	mov	#0,r0
+
+LOCAL(retmax):
+	mov	#-1,r0
+	rts
+	shlr	r0
+
+LOCAL(retmin):
+	mov	#1,r0
+	rts
+	rotr	r0
+
+	.balign 4
+LOCAL(mask):
+	.long	0x007fffff
+LOCAL(max):
+	.long	0x4f000000
+LOCAL(min):
+	.long	0xcf000000
+	ENDFUNC(GLOBAL(fixsfsi))
+#endif /* L_fixsfsi */
+
+#ifdef L_floatunssisf
+#include "IEEE-754/m3/floatunssisf.S"
+#endif /* L_floatunssisf */
+
+#ifdef L_floatsisf
+#include "IEEE-754/m3/floatsisf.S"
+#endif /* L_floatsisf */
+
+#ifdef L_divsf3
+#include "IEEE-754/m3/divsf3.S"
+#endif /* L_divsf3 */
+
+#ifdef L_hypotf
+	.balign 4
+	.global GLOBAL(hypotf)
+	FUNC(GLOBAL(hypotf))
+GLOBAL(hypotf):
+/* This integer implementation takes 71 to 72 cycles in the main path.
+   This is a bit slower than the SH4 can do this computation using double
+   precision hardware floating point - 57 cycles, or 69 with mode switches.  */
+ /* First, calculate x (r4) as the sum of the square of the fractions -
+    the exponent is calculated separately in r3.
+    Then, calculate sqrt(x) for the fraction by reciproot iteration.
+    We get an 7.5 bit inital value using linear approximation with two slopes
+    that are powers of two.
+    x (- [1. .. 2.)  y0 := 1.25 - x/4 - tab(x)   y (- (0.8 .. 1.0)
+    x (- [2. .. 4.)  y0 := 1.   - x/8 - tab(x)   y (- (0.5 .. 0.8)
+ x is represented with two bits before the point,
+ y with 0 bits before the binary point.
+ Thus, to calculate y0 := 1. - x/8 - tab(x), all you have to do is to shift x
+ right by 1, negate it, and subtract tab(x).  */
+
+ /* y1 := 1.5*y0 - 0.5 * (x * y0) * (y0 * y0)
+    z0 := x * y1
+    z1 := z0 + 0.5 * (y1 - (y1*y1) * z0) */
+
+	mov.l	LOCAL(xff000000),r1
+	add	r4,r4
+	mov	r4,r0
+	add	r5,r5
+	cmp/hs	r5,r4
+	sub	r5,r0
+	mov	#-24,r2
+	bf/s	LOCAL(r5_large)
+	shad	r2,r0
+	mov	r4,r3
+	shll8	r4
+	rotcr	r4
+	tst	#0xe0,r0
+	neg	r0,r0
+	bt	LOCAL(ret_abs_r3)
+	tst	r1,r5
+	shll8	r5
+	bt/s	LOCAL(denorm_r5)
+	cmp/hi	r3,r1
+	dmulu.l	r4,r4
+	bf	LOCAL(inf_nan)
+	rotcr	r5
+	shld	r0,r5
+LOCAL(denorm_r5_done):
+	sts	mach,r4
+	dmulu.l	r5,r5
+	mov.l	r6,@-r15
+	mov	#20,r6
+
+	sts	mach,r5
+LOCAL(add_frac):
+	mova	LOCAL(tab)-32,r0
+	mov.l	r7,@-r15
+	mov.w	LOCAL(x1380),r7
+	and	r1,r3
+	addc	r5,r4
+	mov.w	LOCAL(m25),r2	! -25
+	bf	LOCAL(frac_ok)
+	sub	r1,r3
+	rotcr	r4
+	cmp/eq	r1,r3	! did we generate infinity ?
+	bt	LOCAL(inf_nan)
+	shlr	r4
+	mov	r4,r1
+	shld	r2,r1
+	mov.b	@(r0,r1),r0
+	mov	r4,r1
+	shld	r6,r1
+	bra	LOCAL(frac_low2)
+	sub	r1,r7
+
+LOCAL(frac_ok):
+	mov	r4,r1
+	shld	r2,r1
+	mov.b	@(r0,r1),r1
+	cmp/pz	r4
+	mov	r4,r0
+	bt/s	LOCAL(frac_low)
+	shld	r6,r0
+	mov.w	LOCAL(xf80),r7
+	shlr	r0
+LOCAL(frac_low):
+	sub	r0,r7
+LOCAL(frac_low2):
+	mov.l	LOCAL(x40000080),r0 ! avoid denorm results near 1. << r3
+	sub	r1,r7	! {0.12}
+	mov.l	LOCAL(xfffe0000),r5 ! avoid rounding overflow near 4. << r3
+	swap.w	r7,r1	! {0.28}
+	dmulu.l	r1,r4 /* two issue cycles */
+	mulu.w	r7,r7  /* two issue cycles */
+	sts	mach,r2	! {0.26}
+	mov	r1,r7
+	shlr	r1
+	sts	macl,r6	! {0.24}
+	cmp/hi	r0,r4
+	shlr2	r2
+	bf	LOCAL(near_one)
+	shlr	r2	! {0.23} systemic error of linear approximation keeps y1 < 1
+	dmulu.l	r2,r6
+	cmp/hs	r5,r4
+	add	r7,r1	! {1.28}
+	bt	LOCAL(near_four)
+	shlr2	r1	! {1.26}
+	sts	mach,r0	! {0.15} x*y0^3 == {0.16} 0.5*x*y0^3
+	shlr2	r1	! {1.24}
+	shlr8	r1	! {1.16}
+	sett		! compensate for truncation of subtrahend, keep y1 < 1
+	subc	r0,r1   ! {0.16} y1;  max error about 3.5 ulp
+	swap.w	r1,r0
+	dmulu.l	r0,r4	! { 1.30 }
+	mulu.w	r1,r1
+	sts	mach,r2
+	shlr2	r0
+	sts	macl,r1
+	add	r2,r0
+	mov.l	LOCAL(xff000000),r6
+	add	r2,r0
+	dmulu.l	r1,r2
+	add	#127,r0
+	add	r6,r3	! precompensation for adding leading 1
+	sts	mach,r1
+	shlr	r3
+	mov.l	@r15+,r7
+	sub	r1,r0	! {0.31} max error about 50 ulp (+127)
+	mov.l	@r15+,r6
+	shlr8	r0	! {0.23} max error about 0.7 ulp
+	rts
+	add	r3,r0
+	
+LOCAL(r5_large):
+	mov	r5,r3
+	mov	#-31,r2
+	cmp/ge	r2,r0
+	shll8	r5
+	bf	LOCAL(ret_abs_r3)
+	rotcr	r5
+	tst	r1,r4
+	shll8	r4
+	bt/s	LOCAL(denorm_r4)
+	cmp/hi	r3,r1
+	dmulu.l	r5,r5
+	bf	LOCAL(inf_nan)
+	rotcr	r4
+LOCAL(denorm_r4_done):
+	shld	r0,r4
+	sts	mach,r5
+	dmulu.l	r4,r4
+	mov.l	r6,@-r15
+	mov	#20,r6
+	bra	LOCAL(add_frac)
+	sts	mach,r4
+
+LOCAL(near_one):
+	bra	LOCAL(assemble_sqrt)
+	mov	#0,r0
+LOCAL(near_four):
+	! exact round-to-nearest would add 255.  We add 256 for speed & compactness.
+	mov	r4,r0
+	shlr8	r0
+	add	#1,r0
+	tst	r0,r0
+	addc	r0,r3	! might generate infinity.
+LOCAL(assemble_sqrt):
+	mov.l	@r15+,r7
+	shlr	r3
+	mov.l	@r15+,r6
+	rts
+	add	r3,r0
+LOCAL(inf_nan):
+LOCAL(ret_abs_r3):
+	mov	r3,r0
+	rts
+	shlr	r0
+LOCAL(denorm_r5):
+	bf	LOCAL(inf_nan)
+	tst	r1,r4
+	bt	LOCAL(denorm_both)
+	dmulu.l	r4,r4
+	bra	LOCAL(denorm_r5_done)
+	shld	r0,r5
+LOCAL(denorm_r4):
+	bf	LOCAL(inf_nan)
+	tst	r1,r5
+	dmulu.l	r5,r5
+	bf	LOCAL(denorm_r4_done)
+LOCAL(denorm_both):	! normalize according to r3.
+	extu.w	r3,r2
+	mov.l	LOCAL(c__clz_tab),r0
+	cmp/eq	r3,r2
+	mov	#-8,r2
+	bt	0f
+	tst	r1,r3
+	mov	#-16,r2
+	bt	0f
+	mov	#-24,r2
+0:
+	shld	r2,r3
+	mov.l	r7,@-r15
+#ifdef __pic__
+	add	r0,r3
+	mova	 LOCAL(c__clz_tab),r0
+#endif
+	mov.b	@(r0,r3),r0
+	add	#32,r2
+	sub	r0,r2
+	shld	r2,r4
+	mov	r2,r7
+	dmulu.l	r4,r4
+	sts.l	pr,@-r15
+	mov	#1,r3
+	bsr	LOCAL(denorm_r5_done)
+	shld	r2,r5
+	mov.l	LOCAL(x01000000),r1
+	neg	r7,r2
+	lds.l	@r15+,pr
+	tst	r1,r0
+	mov.l	@r15+,r7
+	bt	0f
+	add	#1,r2
+	sub	r1,r0
+0:
+	rts
+	shld	r2,r0
+
+LOCAL(m25):
+	.word	-25
+LOCAL(x1380):
+	.word	0x1380
+LOCAL(xf80):
+	.word	0xf80
+	.balign	4
+LOCAL(xff000000):
+	.long	0xff000000
+LOCAL(x40000080):
+	.long	0x40000080
+LOCAL(xfffe0000):
+	.long	0xfffe0000
+LOCAL(x01000000):
+	.long	0x01000000
+LOCAL(c__clz_tab):
+#ifdef __pic__
+	.long	GLOBAL(clz_tab) - .
+#else
+	.long	GLOBAL(clz_tab)
+#endif
+
+/*
+double err(double x)
+{
+  return (x < 2. ? 1.25 - x/4. : 1. - x/8.) - 1./sqrt(x);
+}
+
+int
+main ()
+{
+  int i = 0;
+  double x, s, v;
+  double lx, hx;
+
+  s = 1./32.;
+  for (x = 1.; x < 4; x += s, i++)
+    {
+      lx = x;
+      hx = x + s - 1. / (1 << 30);
+      v = 0.5 * (err (lx) + err (hx));
+      printf ("%s% 4d%c",
+              (i & 7) == 0 ? "\t.byte\t" : "",
+              (int)(v * 4096 + 0.5) - 128,
+              (i & 7) == 7 ? '\n' : ',');
+    }
+  return 0;
+} */
+
+	.balign	4
+LOCAL(tab):
+	.byte	-113, -84, -57, -33, -11,   8,  26,  41
+	.byte	  55,  67,  78,  87,  94, 101, 106, 110
+	.byte	 113, 115, 115, 115, 114, 112, 109, 106
+	.byte	 101,  96,  91,  84,  77,  69,  61,  52
+	.byte	  51,  57,  63,  68,  72,  77,  80,  84
+	.byte	  87,  89,  91,  93,  95,  96,  97,  97
+	.byte	  97,  97,  97,  96,  95,  94,  93,  91
+	.byte	  89,  87,  84,  82,  79,  76,  72,  69
+	.byte	  65,  61,  57,  53,  49,  44,  39,  34
+	.byte	  29,  24,  19,  13,   8,   2,  -4, -10
+	.byte	 -17, -23, -29, -36, -43, -50, -57, -64
+	.byte	 -71, -78, -85, -93,-101,-108,-116,-124
+	ENDFUNC(GLOBAL(hypotf))
+#endif /* L_hypotf */
+#endif /* DYN_SHIFT */
+
+#endif /* __SH_FPU_ANY__ */
diff -Naur gcc-4.2.1/gcc/config/sh/lib1funcs-4-300.asm gcc-4.2.1-stm/gcc/config/sh/lib1funcs-4-300.asm
--- gcc-4.2.1/gcc/config/sh/lib1funcs-4-300.asm	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/lib1funcs-4-300.asm	2007-08-10 13:08:59.000000000 +0100
@@ -0,0 +1,939 @@
+/* Copyright (C) 2004, 2006 Free Software Foundation, Inc.
+   Copyright (C) 2006 STMicroelectronics
+
+This file is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 2, or (at your option) any
+later version.
+
+In addition to the permissions in the GNU General Public License, the
+Free Software Foundation gives you unlimited permission to link the
+compiled version of this file into combinations with other programs,
+and to distribute those combinations without any restriction coming
+from the use of this file.  (The General Public License restrictions
+do apply in other respects; for example, they cover modification of
+the file, and distribution when not linked into a combine
+executable.)
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; see the file COPYING.  If not, write to
+the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+Boston, MA 02110-1301, USA.  */
+
+/* libgcc routines for the STMicroelectronics ST40-300 CPU.
+   Contributed by J"orn Rennecke joern.rennecke@st.com.  */
+
+#include "lib1funcs.h"
+
+#ifdef L_div_table
+#if defined (__SH3__) || defined (__SH3E__) || defined (__SH4__) || defined (__SH4_SINGLE__) || defined (__SH4_SINGLE_ONLY__) || defined (__SH4_NOFPU__)
+/* This code used shld, thus is not suitable for SH1 / SH2.  */
+
+/* Signed / unsigned division without use of FPU, optimized for SH4-300.
+   Uses a lookup table for divisors in the range -128 .. +127, and
+   div1 with case distinction for larger divisors in three more ranges.
+   The code is lumped together with the table to allow the use of mova.  */
+#ifdef __LITTLE_ENDIAN__
+#define L_LSB 0
+#define L_LSWMSB 1
+#define L_MSWLSB 2
+#else
+#define L_LSB 3
+#define L_LSWMSB 2
+#define L_MSWLSB 1
+#endif
+
+	.global	GLOBAL(udivsi3_i4i)
+	.global	GLOBAL(sdivsi3_i4i)
+	FUNC(GLOBAL(udivsi3_i4i))
+	FUNC(GLOBAL(sdivsi3_i4i))
+
+	.balign 4
+LOCAL(div_ge8m): ! 10 cycles up to here
+	rotcr r1 ! signed shift must use original sign from r4
+	div0s r5,r4
+	mov #24,r7
+	shld r7,r6
+	shad r0,r1
+	rotcl r6
+	div1 r5,r1
+	swap.w r5,r0 ! detect -0x80000000 : 0x800000
+	rotcl r6
+	swap.w r4,r7
+	div1 r5,r1
+	swap.b r7,r7
+	rotcl r6
+	or r7,r0
+	div1 r5,r1
+	swap.w r0,r7
+	rotcl r6
+	or r7,r0
+	div1 r5,r1
+	add #-0x80,r0
+	rotcl r6
+	extu.w r0,r0
+	div1 r5,r1
+	neg r0,r0
+	rotcl r6
+	swap.w r0,r0
+	div1 r5,r1
+	mov.l @r15+,r7
+	and r6,r0
+	rotcl r6
+	div1 r5,r1
+	shll2 r0
+	rotcl r6
+	exts.b r0,r0
+	div1 r5,r1
+	swap.w r0,r0
+	exts.w r0,r1
+	exts.b r6,r0
+	mov.l @r15+,r6
+	rotcl r0
+	rts
+	sub r1,r0
+	! 31 cycles up to here
+
+	.balign 4
+LOCAL(udiv_ge64k): ! 3 cycles up to here
+	mov r4,r0
+	shlr8 r0
+	div0u
+	cmp/hi r0,r5
+	bt LOCAL(udiv_r8)
+	mov.l r5,@-r15
+	shll8 r5
+	! 7 cycles up to here
+	.rept 8
+	div1 r5,r0
+	.endr
+	extu.b r4,r1 ! 15 cycles up to here
+	extu.b r0,r6
+	xor r1,r0
+	xor r6,r0
+	swap.b r6,r6
+	.rept 8
+	div1 r5,r0
+	.endr ! 25 cycles up to here
+	extu.b r0,r0
+	mov.l @r15+,r5
+	or r6,r0
+	mov.l @r15+,r6
+	rts
+	rotcl r0 ! 28 cycles up to here
+
+	.balign 4
+LOCAL(udiv_r8): ! 6 cycles up to here
+	mov.l r4,@-r15
+	shll16 r4
+	shll8 r4
+	!
+	shll r4
+	mov r0,r1
+	div1 r5,r1
+	mov r4,r0
+	rotcl r0
+	mov.l @r15+,r4
+	div1 r5,r1
+	! 12 cycles up to here
+	.rept 6
+	rotcl r0; div1 r5,r1
+	.endr
+	mov.l @r15+,r6 ! 24 cycles up to here
+	rts
+	rotcl r0
+
+	.balign 4
+LOCAL(div_ge32k): ! 6 cycles up to here
+	mov.l r7,@-r15
+	swap.w r5,r6
+	exts.b r6,r7
+	exts.w r6,r6
+	cmp/eq r6,r7
+	extu.b r1,r6
+	bf/s LOCAL(div_ge8m)
+	cmp/hi r1,r4 ! copy sign bit of r4 into T
+	rotcr r1 ! signed shift must use original sign from r4
+	div0s r5,r4
+	shad r0,r1
+	shll8 r5
+	div1 r5,r1
+	mov r5,r7 ! detect r4 == 0x80000000 && r5 == 0x8000(00)
+	div1 r5,r1
+	shlr8 r7
+	div1 r5,r1
+	swap.w r4,r0
+	div1 r5,r1
+	swap.b r0,r0
+	div1 r5,r1
+	or r0,r7
+	div1 r5,r1
+	add #-80,r7
+	div1 r5,r1
+	swap.w r7,r0
+	div1 r5,r1
+	or r0,r7
+	extu.b r1,r0
+	xor r6,r1
+	xor r0,r1
+	exts.b r0,r0
+	div1 r5,r1
+	extu.w r7,r7
+	div1 r5,r1
+	neg r7,r7 ! upper 16 bit of r7 == 0 if r4 == 0x80000000 && r5 == 0x8000
+	div1 r5,r1
+	and r0,r7
+	div1 r5,r1
+	swap.w r7,r7 ! 26 cycles up to here.
+	div1 r5,r1
+	shll8 r0
+	div1 r5,r1
+	exts.w r7,r7
+	div1 r5,r1
+	add r0,r0
+	div1 r5,r1
+	sub r7,r0
+	extu.b r1,r1
+	mov.l @r15+,r7
+	rotcl r1
+	mov.l @r15+,r6
+	add r1,r0
+	mov #-8,r1
+	rts
+	shad r1,r5 ! 34 cycles up to here
+
+	.balign 4
+GLOBAL(udivsi3_i4i):
+	mov.l r6,@-r15
+	extu.w r5,r6
+	cmp/eq r5,r6
+	mov #0x7f,r0
+	bf LOCAL(udiv_ge64k)
+	cmp/hi r0,r5
+	bf LOCAL(udiv_le128)
+	mov r4,r1
+	shlr8 r1
+	div0u
+	shlr r1
+	shll16 r6
+	div1 r6,r1
+	extu.b r4,r0 ! 7 cycles up to here
+	.rept 8
+	div1 r6,r1
+	.endr     ! 15 cycles up to here
+	xor r1,r0 ! xor dividend with result lsb
+	.rept 6
+	div1 r6,r1
+	.endr
+	mov.l r7,@-r15 ! 21 cycles up to here
+	div1 r6,r1
+	extu.b r0,r7
+	div1 r6,r1
+	shll8 r7
+	extu.w r1,r0
+	xor r7,r1 ! replace lsb of result with lsb of dividend
+	div1 r6,r1
+	mov #0,r7
+	div1 r6,r1
+	!
+	div1 r6,r1
+	bra LOCAL(div_end)
+	div1 r6,r1 ! 28 cycles up to here
+
+	/* This is link-compatible with a GLOBAL(sdivsi3) call,
+	   but we effectively clobber only r1, macl and mach  */
+        /* Because negative quotients are calculated as one's complements,
+	   -0x80000000 divided by the smallest positive number of a number
+	   range (0x80, 0x8000, 0x800000) causes saturation in the one's
+           complement representation, and we have to suppress the
+	   one's -> two's complement adjustment.  Since positive numbers
+	   don't get such an adjustment, it's OK to also compute one's -> two's
+	   complement adjustment suppression for a dividend of 0.  */
+	.balign 4
+GLOBAL(sdivsi3_i4i):
+	mov.l r6,@-r15
+	exts.b r5,r6
+	cmp/eq r5,r6
+	mov #-1,r1
+	bt/s LOCAL(div_le128)
+	cmp/pz r4
+	addc r4,r1
+	exts.w r5,r6
+	cmp/eq r5,r6
+	mov #-7,r0
+	bf/s LOCAL(div_ge32k)
+	cmp/hi r1,r4 ! copy sign bit of r4 into T
+	rotcr r1
+	shll16 r6  ! 7 cycles up to here
+	shad r0,r1
+	div0s r5,r4
+	div1 r6,r1
+	mov.l r7,@-r15
+	div1 r6,r1
+	mov r4,r0 ! re-compute adjusted dividend
+	div1 r6,r1
+	mov #-31,r7
+	div1 r6,r1
+	shad r7,r0
+	div1 r6,r1
+	add r4,r0 ! adjusted dividend
+	div1 r6,r1
+	mov.l r8,@-r15
+	div1 r6,r1
+	swap.w r4,r8 ! detect special case r4 = 0x80000000, r5 = 0x80
+	div1 r6,r1
+	swap.b r8,r8
+	xor r1,r0 ! xor dividend with result lsb
+	div1 r6,r1
+	div1 r6,r1
+	or r5,r8
+	div1 r6,r1
+	add #-0x80,r8 ! r8 is 0 iff there is a match
+	div1 r6,r1
+	swap.w r8,r7 ! or upper 16 bits...
+	div1 r6,r1
+	or r7,r8 !...into lower 16 bits
+	div1 r6,r1
+	extu.w r8,r8
+	div1 r6,r1
+	extu.b r0,r7
+	div1 r6,r1
+	shll8 r7
+	exts.w r1,r0
+	xor r7,r1 ! replace lsb of result with lsb of dividend
+	div1 r6,r1
+	neg r8,r8 ! upper 16 bits of r8 are now 0xffff iff we want end adjm.
+	div1 r6,r1
+	and r0,r8
+	div1 r6,r1
+	swap.w r8,r7
+	div1 r6,r1
+	mov.l @r15+,r8 ! 58 insns, 29 cycles up to here
+LOCAL(div_end):
+	div1 r6,r1
+	shll8 r0
+	div1 r6,r1
+	exts.w r7,r7
+	div1 r6,r1
+	add r0,r0
+	div1 r6,r1
+	sub r7,r0
+	extu.b r1,r1
+	mov.l @r15+,r7
+	rotcl r1
+	mov.l @r15+,r6
+	rts
+	add r1,r0
+
+	.balign 4
+LOCAL(udiv_le128): ! 4 cycles up to here (or 7 for mispredict)
+	mova LOCAL(div_table_inv),r0
+	shll2 r6
+	mov.l @(r0,r6),r1
+	mova LOCAL(div_table_clz),r0
+	lds r4,mach
+	!
+	!
+	!
+	tst r1,r1
+	!
+	bt 0f
+	dmulu.l r1,r4
+0:	mov.b @(r0,r5),r1
+	clrt
+	!
+	!
+	sts mach,r0
+	addc r4,r0
+	rotcr r0
+	mov.l @r15+,r6
+	rts
+	shld r1,r0
+
+	.balign 4
+LOCAL(div_le128): ! 3 cycles up to here (or 6 for mispredict)
+	mova LOCAL(div_table_inv),r0
+	shll2 r6
+	mov.l @(r0,r6),r1
+	mova LOCAL(div_table_clz),r0
+	neg r4,r6
+	bf 0f
+	mov r4,r6
+0:	lds r6,mach
+	tst r1,r1
+	bt 0f
+	dmulu.l r1,r6
+0:	div0s r4,r5
+	mov.b @(r0,r5),r1
+	bt/s LOCAL(le128_neg)
+	clrt
+	!
+	sts mach,r0
+	addc r6,r0
+	rotcr r0
+	mov.l @r15+,r6
+	rts
+	shld r1,r0
+
+/* Could trap divide by zero for the cost of one cycle more mispredict penalty:
+...
+	dmulu.l r1,r6
+0:	div0s r4,r5
+	bt/s LOCAL(le128_neg)
+	tst r5,r5
+	bt LOCAL(div_by_zero)
+	mov.b @(r0,r5),r1
+	sts mach,r0
+	addc r6,r0
+...
+LOCAL(div_by_zero):
+	trapa #
+	.balign 4
+LOCAL(le128_neg):
+	bt LOCAL(div_by_zero)
+	mov.b @(r0,r5),r1
+	sts mach,r0
+	addc r6,r0
+...  */
+
+	.balign 4
+LOCAL(le128_neg):
+	sts mach,r0
+	addc r6,r0
+	rotcr r0
+	mov.l @r15+,r6
+	shad r1,r0
+	rts
+	neg r0,r0
+	ENDFUNC(GLOBAL(udivsi3_i4i))
+	ENDFUNC(GLOBAL(sdivsi3_i4i))
+
+/* This table has been generated by divtab-sh4.c.  */
+	.balign 4
+	.byte	-7
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-3
+	.byte	-3
+	.byte	-3
+	.byte	-3
+	.byte	-3
+	.byte	-3
+	.byte	-3
+	.byte	-3
+	.byte	-2
+	.byte	-2
+	.byte	-2
+	.byte	-2
+	.byte	-1
+	.byte	-1
+	.byte	0
+LOCAL(div_table_clz):
+	.byte	0
+	.byte	0
+	.byte	-1
+	.byte	-1
+	.byte	-2
+	.byte	-2
+	.byte	-2
+	.byte	-2
+	.byte	-3
+	.byte	-3
+	.byte	-3
+	.byte	-3
+	.byte	-3
+	.byte	-3
+	.byte	-3
+	.byte	-3
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-4
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-5
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+	.byte	-6
+/* 1/-128 .. 1/127, normalized.  There is an implicit leading 1 in bit 32,
+   or in bit 33 for powers of two.  */
+	.balign 4
+	.long   0x0
+	.long	0x2040811
+	.long	0x4104105
+	.long	0x624DD30
+	.long	0x8421085
+	.long	0xA6810A7
+	.long	0xC9714FC
+	.long	0xECF56BF
+	.long	0x11111112
+	.long	0x135C8114
+	.long	0x15B1E5F8
+	.long	0x18118119
+	.long	0x1A7B9612
+	.long	0x1CF06ADB
+	.long	0x1F7047DD
+	.long	0x21FB7813
+	.long	0x24924925
+	.long	0x27350B89
+	.long	0x29E4129F
+	.long	0x2C9FB4D9
+	.long	0x2F684BDB
+	.long	0x323E34A3
+	.long	0x3521CFB3
+	.long	0x38138139
+	.long	0x3B13B13C
+	.long	0x3E22CBCF
+	.long	0x41414142
+	.long	0x446F8657
+	.long	0x47AE147B
+	.long	0x4AFD6A06
+	.long	0x4E5E0A73
+	.long	0x51D07EAF
+	.long	0x55555556
+	.long	0x58ED2309
+	.long	0x5C9882BA
+	.long	0x60581606
+	.long	0x642C8591
+	.long	0x68168169
+	.long	0x6C16C16D
+	.long	0x702E05C1
+	.long	0x745D1746
+	.long	0x78A4C818
+	.long	0x7D05F418
+	.long	0x81818182
+	.long	0x86186187
+	.long	0x8ACB90F7
+	.long	0x8F9C18FA
+	.long	0x948B0FCE
+	.long	0x9999999A
+	.long	0x9EC8E952
+	.long	0xA41A41A5
+	.long	0xA98EF607
+	.long	0xAF286BCB
+	.long	0xB4E81B4F
+	.long	0xBACF914D
+	.long	0xC0E07039
+	.long	0xC71C71C8
+	.long	0xCD856891
+	.long	0xD41D41D5
+	.long	0xDAE6076C
+	.long	0xE1E1E1E2
+	.long	0xE9131AC0
+	.long	0xF07C1F08
+	.long	0xF81F81F9
+	.long	0x0
+	.long	0x4104105
+	.long	0x8421085
+	.long	0xC9714FC
+	.long	0x11111112
+	.long	0x15B1E5F8
+	.long	0x1A7B9612
+	.long	0x1F7047DD
+	.long	0x24924925
+	.long	0x29E4129F
+	.long	0x2F684BDB
+	.long	0x3521CFB3
+	.long	0x3B13B13C
+	.long	0x41414142
+	.long	0x47AE147B
+	.long	0x4E5E0A73
+	.long	0x55555556
+	.long	0x5C9882BA
+	.long	0x642C8591
+	.long	0x6C16C16D
+	.long	0x745D1746
+	.long	0x7D05F418
+	.long	0x86186187
+	.long	0x8F9C18FA
+	.long	0x9999999A
+	.long	0xA41A41A5
+	.long	0xAF286BCB
+	.long	0xBACF914D
+	.long	0xC71C71C8
+	.long	0xD41D41D5
+	.long	0xE1E1E1E2
+	.long	0xF07C1F08
+	.long	0x0
+	.long	0x8421085
+	.long	0x11111112
+	.long	0x1A7B9612
+	.long	0x24924925
+	.long	0x2F684BDB
+	.long	0x3B13B13C
+	.long	0x47AE147B
+	.long	0x55555556
+	.long	0x642C8591
+	.long	0x745D1746
+	.long	0x86186187
+	.long	0x9999999A
+	.long	0xAF286BCB
+	.long	0xC71C71C8
+	.long	0xE1E1E1E2
+	.long	0x0
+	.long	0x11111112
+	.long	0x24924925
+	.long	0x3B13B13C
+	.long	0x55555556
+	.long	0x745D1746
+	.long	0x9999999A
+	.long	0xC71C71C8
+	.long	0x0
+	.long	0x24924925
+	.long	0x55555556
+	.long	0x9999999A
+	.long	0x0
+	.long	0x55555556
+	.long	0x0
+	.long	0x0
+LOCAL(div_table_inv):
+	.long	0x0
+	.long	0x0
+	.long	0x0
+	.long	0x55555556
+	.long	0x0
+	.long	0x9999999A
+	.long	0x55555556
+	.long	0x24924925
+	.long	0x0
+	.long	0xC71C71C8
+	.long	0x9999999A
+	.long	0x745D1746
+	.long	0x55555556
+	.long	0x3B13B13C
+	.long	0x24924925
+	.long	0x11111112
+	.long	0x0
+	.long	0xE1E1E1E2
+	.long	0xC71C71C8
+	.long	0xAF286BCB
+	.long	0x9999999A
+	.long	0x86186187
+	.long	0x745D1746
+	.long	0x642C8591
+	.long	0x55555556
+	.long	0x47AE147B
+	.long	0x3B13B13C
+	.long	0x2F684BDB
+	.long	0x24924925
+	.long	0x1A7B9612
+	.long	0x11111112
+	.long	0x8421085
+	.long	0x0
+	.long	0xF07C1F08
+	.long	0xE1E1E1E2
+	.long	0xD41D41D5
+	.long	0xC71C71C8
+	.long	0xBACF914D
+	.long	0xAF286BCB
+	.long	0xA41A41A5
+	.long	0x9999999A
+	.long	0x8F9C18FA
+	.long	0x86186187
+	.long	0x7D05F418
+	.long	0x745D1746
+	.long	0x6C16C16D
+	.long	0x642C8591
+	.long	0x5C9882BA
+	.long	0x55555556
+	.long	0x4E5E0A73
+	.long	0x47AE147B
+	.long	0x41414142
+	.long	0x3B13B13C
+	.long	0x3521CFB3
+	.long	0x2F684BDB
+	.long	0x29E4129F
+	.long	0x24924925
+	.long	0x1F7047DD
+	.long	0x1A7B9612
+	.long	0x15B1E5F8
+	.long	0x11111112
+	.long	0xC9714FC
+	.long	0x8421085
+	.long	0x4104105
+	.long	0x0
+	.long	0xF81F81F9
+	.long	0xF07C1F08
+	.long	0xE9131AC0
+	.long	0xE1E1E1E2
+	.long	0xDAE6076C
+	.long	0xD41D41D5
+	.long	0xCD856891
+	.long	0xC71C71C8
+	.long	0xC0E07039
+	.long	0xBACF914D
+	.long	0xB4E81B4F
+	.long	0xAF286BCB
+	.long	0xA98EF607
+	.long	0xA41A41A5
+	.long	0x9EC8E952
+	.long	0x9999999A
+	.long	0x948B0FCE
+	.long	0x8F9C18FA
+	.long	0x8ACB90F7
+	.long	0x86186187
+	.long	0x81818182
+	.long	0x7D05F418
+	.long	0x78A4C818
+	.long	0x745D1746
+	.long	0x702E05C1
+	.long	0x6C16C16D
+	.long	0x68168169
+	.long	0x642C8591
+	.long	0x60581606
+	.long	0x5C9882BA
+	.long	0x58ED2309
+	.long	0x55555556
+	.long	0x51D07EAF
+	.long	0x4E5E0A73
+	.long	0x4AFD6A06
+	.long	0x47AE147B
+	.long	0x446F8657
+	.long	0x41414142
+	.long	0x3E22CBCF
+	.long	0x3B13B13C
+	.long	0x38138139
+	.long	0x3521CFB3
+	.long	0x323E34A3
+	.long	0x2F684BDB
+	.long	0x2C9FB4D9
+	.long	0x29E4129F
+	.long	0x27350B89
+	.long	0x24924925
+	.long	0x21FB7813
+	.long	0x1F7047DD
+	.long	0x1CF06ADB
+	.long	0x1A7B9612
+	.long	0x18118119
+	.long	0x15B1E5F8
+	.long	0x135C8114
+	.long	0x11111112
+	.long	0xECF56BF
+	.long	0xC9714FC
+	.long	0xA6810A7
+	.long	0x8421085
+	.long	0x624DD30
+	.long	0x4104105
+	.long	0x2040811
+	/* maximum error: 0.987342 scaled: 0.921875*/
+
+#endif /* SH3 / SH4 */
+
+#endif /* L_div_table */
diff -Naur gcc-4.2.1/gcc/config/sh/lib1funcs.asm gcc-4.2.1-stm/gcc/config/sh/lib1funcs.asm
--- gcc-4.2.1/gcc/config/sh/lib1funcs.asm	2006-06-13 18:44:56.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/lib1funcs.asm	2007-08-10 13:08:59.000000000 +0100
@@ -1,6 +1,7 @@
 /* Copyright (C) 1994, 1995, 1997, 1998, 1999, 2000, 2001, 2002, 2003,
    2004, 2005, 2006
    Free Software Foundation, Inc.
+   Copyright (c) 2006  STMicroelectronics.
 
 This file is free software; you can redistribute it and/or modify it
 under the terms of the GNU General Public License as published by the
@@ -2108,7 +2109,6 @@
 
 #ifdef L_ic_invalidate_array
 #if defined(__SH4A__) || (defined (__FORCE_SH4A__) && (defined(__SH4_SINGLE__) || defined(__SH4__) || defined(__SH4_SINGLE_ONLY__) || (defined(__SH4_NOFPU__) && !defined(__SH5__))))
-	.global GLOBAL(ic_invalidate_array)
 	/* This is needed when an SH4 dso with trampolines is used on SH4A.  */
 	.global GLOBAL(ic_invalidate_array)
 	FUNC(GLOBAL(ic_invalidate_array))
@@ -3891,3 +3891,7 @@
 	ENDFUNC(GLOBAL(udiv_qrnnd_16))
 #endif /* !__SHMEDIA__ */
 #endif /* L_udiv_qrnnd_16 */
+
+#include "ieee-754-sf.S"
+#include "ieee-754-df.S"
+
diff -Naur gcc-4.2.1/gcc/config/sh/lib1funcs.h gcc-4.2.1-stm/gcc/config/sh/lib1funcs.h
--- gcc-4.2.1/gcc/config/sh/lib1funcs.h	2006-06-13 18:44:56.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/lib1funcs.h	2007-08-10 13:08:59.000000000 +0100
@@ -1,6 +1,7 @@
 /* Copyright (C) 1994, 1995, 1997, 1998, 1999, 2000, 2001, 2002, 2003,
    2004, 2005, 2006
    Free Software Foundation, Inc.
+   Copyright (c) 2006  STMicroelectronics.
 
 This file is free software; you can redistribute it and/or modify it
 under the terms of the GNU General Public License as published by the
@@ -69,13 +70,152 @@
 #endif /* !__LITTLE_ENDIAN__ */
 
 #ifdef __sh1__
+/* branch with two-argument delay slot insn */
 #define SL(branch, dest, in_slot, in_slot_arg2) \
 	in_slot, in_slot_arg2; branch dest
+/* branch with one-argument delay slot insn */
 #define SL1(branch, dest, in_slot) \
 	in_slot; branch dest
+/* branch with comparison in delay slot */
+#define SLC(branch, dest, in_slot, in_slot_arg2) \
+        branch dest; in_slot, in_slot_arg2
+/* comparison in a delay slot, at branch destination */
+#define SLI(in_slot, in_slot_arg2) in_slot, in_slot_arg2
+#define SLCMP(branch, cmp1, cmp1arg2, cmp2, cmp2arg2) \
+	branch .+6; bra .+6; cmp2, cmp2arg2; cmp1, cmp1arg2
+#define DMULU_SAVE \
+ mov.l r10,@-r15; \
+ mov.l r11,@-r15; \
+ mov.l r12,@-r15; \
+ mov.l r13,@-r15
+#define DMULUL(m1, m2, rl) \
+ swap.w m1,r12; \
+ mulu.w r12,m2; \
+ swap.w m2,r13; \
+ sts macl,r10; \
+ mulu.w r13,m1; \
+ clrt; \
+ sts macl,r11; \
+ mulu.w r12,r13; \
+ addc r11,r10; \
+ sts macl,r12; \
+ mulu.w m1,m2; \
+ movt r11; \
+ sts macl,rl; \
+ mov r10,r13; \
+ shll16 r13; \
+ addc r13,rl; \
+ xtrct r11,r10; \
+ addc r10,r12 \
+/* N.B. the carry is cleared here.  */
+#define DMULUH(rh) mov r12,rh
+#define DMULU_RESTORE \
+ mov.l @r15+,r13; \
+ mov.l @r15+,r12; \
+ mov.l @r15+,r11; \
+ mov.l @r15+,r10
 #else /* ! __sh1__ */
+/* branch with two-argument delay slot insn */
 #define SL(branch, dest, in_slot, in_slot_arg2) \
-	branch##.s dest; in_slot, in_slot_arg2
+	branch##/s dest; in_slot, in_slot_arg2
+/* branch with one-argument delay slot insn */
 #define SL1(branch, dest, in_slot) \
 	branch##/s dest; in_slot
+/* branch with comparison in delay slot */
+#define SLC(branch, dest, in_slot, in_slot_arg2) \
+        branch##/s dest; in_slot, in_slot_arg2
+/* comparison in a delay slot, at branch destination */
+#define SLI(in_slot, in_slot_arg)
+#define SLCMP(branch, cmp1, cmp1arg2, cmp2, cmp2arg2) \
+	branch##/s .+6; cmp1, cmp1arg2; cmp2, cmp2arg2
+#define DMULU_SAVE
+#define DMULUL(m1, m2, rl) dmulu.l m1,m2; sts macl,rl
+#define DMULUH(rh) sts mach,rh
+#define DMULU_RESTORE
 #endif /* !__sh1__ */
+
+#if defined (__sh1__) || defined (__sh2__) || defined (__SH2E__)
+/* don't #define DYN_SHIFT */
+  #define SHLL4(REG)	\
+	shll2	REG;	\
+	shll2	REG
+
+  #define SHLR4(REG)	\
+	shlr2	REG;	\
+	shlr2	REG
+
+  #define SHLL6(REG)	\
+	shll2	REG;	\
+	shll2	REG;	\
+	shll2	REG
+
+  #define SHLR6(REG)	\
+	shlr2	REG;	\
+	shlr2	REG;	\
+	shlr2	REG
+
+  #define SHLL12(REG)	\
+	shll8	REG;	\
+	SHLL4 (REG)
+
+  #define SHLR12(REG)	\
+	shlr8	REG;	\
+	SHLR4 (REG)
+
+  #define SHLR19(REG)	\
+	shlr16	REG;	\
+	shlr2	REG;	\
+	shlr	REG
+
+  #define SHLL23(REG)	\
+	shll16	REG;	\
+	shlr	REG;	\
+	shll8	REG
+
+  #define SHLR24(REG)	\
+	shlr16	REG;	\
+	shlr8	REG
+
+  #define SHLR21(REG)	\
+	shlr16	REG;	\
+	shll2	REG;	\
+	add	REG,REG;\
+	shlr8	REG
+
+  #define SHLL21(REG)	\
+	shll16	REG;	\
+	SHLL4 (REG);	\
+	add	REG,REG
+
+  #define SHLR11(REG)	\
+	shlr8	REG;	\
+	shlr2	REG;	\
+	shlr	REG
+
+  #define SHLR22(REG)	\
+	shlr16	REG;	\
+	shll2	REG;	\
+	shlr8	REG
+
+  #define SHLR23(REG)	\
+	shlr16	REG;	\
+	add	REG,REG;\
+	shlr8	REG
+
+  #define SHLR20(REG)	\
+	shlr16	REG;	\
+	SHLR4 (REG)
+
+  #define SHLL20(REG)	\
+	shll16	REG;	\
+	SHLL4 (REG)
+#define SHLD_COUNT(N,COUNT)
+#define SHLRN(N,COUNT,REG) SHLR##N(REG)
+#define SHLLN(N,COUNT,REG) SHLL##N(REG)
+#else
+#define SHLD_COUNT(N,COUNT) mov #N,COUNT
+#define SHLRN(N,COUNT,REG) shld COUNT,REG
+#define SHLLN(N,COUNT,REG) shld COUNT,REG
+#define DYN_SHIFT 1
+#endif
+
diff -Naur gcc-4.2.1/gcc/config/sh/lib1funcs-Os-4-200.asm gcc-4.2.1-stm/gcc/config/sh/lib1funcs-Os-4-200.asm
--- gcc-4.2.1/gcc/config/sh/lib1funcs-Os-4-200.asm	2006-08-21 19:07:00.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/lib1funcs-Os-4-200.asm	2007-08-10 13:08:59.000000000 +0100
@@ -1,4 +1,5 @@
 /* Copyright (C) 2006 Free Software Foundation, Inc.
+   Copyright (c) 2006  STMicroelectronics.
 
 This file is free software; you can redistribute it and/or modify it
 under the terms of the GNU General Public License as published by the
@@ -30,7 +31,6 @@
 
 #include "lib1funcs.h"
 
-#if !__SHMEDIA__
 #ifdef L_udivsi3_i4i
 
 /* 88 bytes; sh4-200 cycle counts:
@@ -149,7 +149,7 @@
 	mov.l @r15+,r2
 #endif /* 0 */
 
-/* Size: 186 bytes jointly for udivsi3_i4i and sdivsi3_i4i
+/* Size: 188 bytes jointly for udivsi3_i4i and sdivsi3_i4i
    sh4-200 run times:
    udiv small divisor: 55 cycles
    udiv large divisor: 52 cycles
@@ -324,4 +324,3 @@
 	ENDFUNC(GLOBAL(sdivsi3_i4i))
 #endif /* __SH_FPU_DOUBLE__ */
 #endif /* L_sdivsi3_i4i */
-#endif /* !__SHMEDIA__ */
diff -Naur gcc-4.2.1/gcc/config/sh/linux-unwind.h gcc-4.2.1-stm/gcc/config/sh/linux-unwind.h
--- gcc-4.2.1/gcc/config/sh/linux-unwind.h	2006-03-16 02:44:10.000000000 +0000
+++ gcc-4.2.1-stm/gcc/config/sh/linux-unwind.h	2007-08-10 13:08:59.000000000 +0100
@@ -230,8 +230,8 @@
   r = SH_DWARF_FRAME_XD0;
   for (i = 0; i < 8; i++)
     {
-      fs->regs.reg[i].how = REG_SAVED_OFFSET;
-      fs->regs.reg[i].loc.offset
+      fs->regs.reg[r+i].how = REG_SAVED_OFFSET;
+      fs->regs.reg[r+i].loc.offset
 	= (long)&(sc->sc_xfpregs[2*i]) - new_cfa;
     }
 
diff -Naur gcc-4.2.1/gcc/config/sh/predicates.md gcc-4.2.1-stm/gcc/config/sh/predicates.md
--- gcc-4.2.1/gcc/config/sh/predicates.md	2006-03-15 15:52:34.000000000 +0000
+++ gcc-4.2.1-stm/gcc/config/sh/predicates.md	2007-08-10 13:08:59.000000000 +0100
@@ -1,5 +1,6 @@
 ;; Predicate definitions for Renesas / SuperH SH.
 ;; Copyright (C) 2005, 2006 Free Software Foundation, Inc.
+;; Copyright (c) 2006  STMicroelectronics.
 ;;
 ;; This file is part of GCC.
 ;;
@@ -695,6 +696,33 @@
 (define_predicate "symbol_ref_operand"
   (match_code "symbol_ref"))
 
+(define_special_predicate "soft_fp_comparison_operand"
+  (match_code "subreg,reg")
+{
+  switch (GET_MODE (op))
+    {
+    default:
+      return 0;
+    case CC_FP_NEmode: case CC_FP_GTmode: case CC_FP_UNLTmode:
+      break;
+    }
+  return register_operand (op, mode);
+})
+
+(define_predicate "soft_fp_comparison_operator"
+  (match_code "eq, unle, ge")
+{
+  switch (GET_CODE (op))
+    {
+    default:
+      return 0;
+    case EQ:  mode = CC_FP_NEmode;    break;
+    case UNLE:        mode = CC_FP_GTmode;    break;
+    case GE:  mode = CC_FP_UNLTmode;  break;
+    }
+  return register_operand (XEXP (op, 0), mode);
+})
+
 ;; Same as target_reg_operand, except that label_refs and symbol_refs
 ;; are accepted before reload.
 
diff -Naur gcc-4.2.1/gcc/config/sh/sh1.md gcc-4.2.1-stm/gcc/config/sh/sh1.md
--- gcc-4.2.1/gcc/config/sh/sh1.md	2005-06-25 02:22:41.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/sh1.md	2007-08-10 13:08:59.000000000 +0100
@@ -1,5 +1,6 @@
 ;; DFA scheduling description for Renesas / SuperH SH.
 ;; Copyright (C) 2004 Free Software Foundation, Inc.
+;; Copyright (c) 2006  STMicroelectronics.
 
 ;; This file is part of GCC.
 
@@ -45,7 +46,7 @@
 
 (define_insn_reservation "sh1_load_store" 2
   (and (eq_attr "pipe_model" "sh1")
-       (eq_attr "type" "load,pcload,pload,store,pstore"))
+       (eq_attr "type" "load,pcload,pload,mem_mac,store,fstore,pstore,mac_mem"))
   "sh1memory*2")
 
 (define_insn_reservation "sh1_arith3" 3
@@ -76,7 +77,7 @@
 
 (define_insn_reservation "sh1_fp" 2
   (and (eq_attr "pipe_model" "sh1")
-       (eq_attr "type" "fp,fmove"))
+       (eq_attr "type" "fp,fpscr_toggle,fp_cmp,fmove"))
   "sh1fp")
 
 (define_insn_reservation "sh1_fdiv" 13
diff -Naur gcc-4.2.1/gcc/config/sh/sh4-300.md gcc-4.2.1-stm/gcc/config/sh/sh4-300.md
--- gcc-4.2.1/gcc/config/sh/sh4-300.md	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/sh4-300.md	2007-08-10 13:08:59.000000000 +0100
@@ -0,0 +1,290 @@
+;; DFA scheduling description for ST40-300.
+;; Copyright (C) 2004 Free Software Foundation, Inc.
+;; Copyright (C) 2006 STMicroelectronics (Will be assigned to FSF when
+;; patch is contributed.)
+
+;; This file is part of GCC.
+
+;; GCC is free software; you can redistribute it and/or modify
+;; it under the terms of the GNU General Public License as published by
+;; the Free Software Foundation; either version 2, or (at your option)
+;; any later version.
+
+;; GCC is distributed in the hope that it will be useful,
+;; but WITHOUT ANY WARRANTY; without even the implied warranty of
+;; MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+;; GNU General Public License for more details.
+
+;; You should have received a copy of the GNU General Public License
+;; along with GCC; see the file COPYING.  If not, write to
+;; the Free Software Foundation, 51 Franklin Street, Fifth Floor,
+;; Boston, MA 02110-1301, USA.
+
+;; Load and store instructions save a cycle if they are aligned on a
+;; four byte boundary.  Using a function unit for stores encourages
+;; gcc to separate load and store instructions by one instruction,
+;; which makes it more likely that the linker will be able to word
+;; align them when relaxing.
+
+;; The following description models the ST40-300 pipeline using the DFA based
+;; scheduler.
+
+;; Two automata are defined to reduce number of states
+;; which a single large automaton will have. (Factoring)
+
+(define_automaton "sh4_300_inst_pipeline,sh4_300_fpu_pipe")
+
+;; This unit is basically the decode unit of the processor.
+;; Since SH4 is a dual issue machine,it is as if there are two
+;; units so that any insn can be processed by either one
+;; of the decoding unit.
+
+(define_cpu_unit "sh4_300_pipe_01,sh4_300_pipe_02" "sh4_300_inst_pipeline")
+
+;; The floating point units.
+
+(define_cpu_unit "sh4_300_fpt,sh4_300_fpu,sh4_300_fds" "sh4_300_fpu_pipe")
+
+;; integer multiplier unit
+
+(define_cpu_unit "sh4_300_mul" "sh4_300_inst_pipeline")
+
+;; LS unit
+
+(define_cpu_unit "sh4_300_ls" "sh4_300_inst_pipeline")
+
+;; The address calculator used for branch instructions.
+;; This will be reserved after "issue" of branch instructions
+;; and this is to make sure that no two branch instructions
+;; can be issued in parallel.
+
+(define_cpu_unit "sh4_300_br" "sh4_300_inst_pipeline")
+
+;; ----------------------------------------------------
+;; This reservation is to simplify the dual issue description.
+
+(define_reservation  "sh4_300_issue"  "sh4_300_pipe_01|sh4_300_pipe_02")
+
+(define_reservation "all" "sh4_300_pipe_01+sh4_300_pipe_02")
+
+;;(define_insn_reservation "nil" 0 (eq_attr "type" "nil") "nothing")
+
+;; MOV RM,RN / MOV #imm8,RN / STS PR,RN
+(define_insn_reservation "sh4_300_mov" 0
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "move,movi8,prget"))
+  "sh4_300_issue")
+
+;; Fixed STS from MACL / MACH
+(define_insn_reservation "sh4_300_mac_gp" 0
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "mac_gp"))
+  "sh4_300_issue+sh4_300_mul")
+
+;; Fixed LDS to MACL / MACH
+(define_insn_reservation "sh4_300_gp_mac" 1
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "gp_mac"))
+  "sh4_300_issue+sh4_300_mul")
+
+;; Instructions without specific resource requirements with latency 1.
+
+(define_insn_reservation "sh4_300_simple_arith" 1
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "mt_group,arith,dyn_shift,prset"))
+  "sh4_300_issue")
+
+;; Load and store instructions have no alignment peculiarities for the ST40-300,
+;; but they use the load-store unit, which they share with the fmove type
+;; insns (fldi[01]; fmov frn,frm; flds; fsts; fabs; fneg) .
+;; Loads have a latency of three.
+
+;; Load Store instructions.
+(define_insn_reservation "sh4_300_load" 3
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "load,pcload,load_si,pcload_si,pload"))
+  "sh4_300_issue+sh4_300_ls")
+
+(define_insn_reservation "sh4_300_mac_load" 3
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "mem_mac"))
+  "sh4_300_issue+sh4_300_ls+sh4_300_mul")
+
+(define_insn_reservation "sh4_300_fload" 4
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "fload,pcfload"))
+  "sh4_300_issue+sh4_300_ls+sh4_300_fpt")
+
+;; sh_adjust_cost describes the reduced latency of the feeding insns of a store.
+;; The latency of an auto-increment register is 1; the latency of the memory
+;; output is not actually considered here anyway.
+(define_insn_reservation "sh4_300_store" 1
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "store,pstore"))
+  "sh4_300_issue+sh4_300_ls")
+
+(define_insn_reservation "sh4_300_fstore" 1
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "fstore"))
+  "sh4_300_issue+sh4_300_ls+sh4_300_fpt")
+
+;; Fixed STS.L from MACL / MACH
+(define_insn_reservation "sh4_300_mac_store" 1
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "mac_mem"))
+  "sh4_300_issue+sh4_300_mul+sh4_300_ls")
+
+(define_insn_reservation "sh4_300_gp_fpul" 2
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "gp_fpul"))
+  "sh4_300_issue+sh4_300_fpt")
+
+(define_insn_reservation "sh4_300_fpul_gp" 1
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "fpul_gp"))
+  "sh4_300_issue+sh4_300_fpt")
+
+;; Branch (BF,BF/S,BT,BT/S,BRA)
+;; Branch Far (JMP,RTS,BRAF)
+;; Group:	BR
+;; When displacement is 0 for BF / BT, we have effectively conditional
+;; execution of one instruction, without pipeline disruption.
+;; Otherwise, the latency depends on prediction success.
+;; We can't really do much with the latency, even if we could express it,
+;; but the pairing restrictions are useful to take into account.
+;; ??? If the branch is likely, and not paired with a preceding insn,
+;; or likely and likely not predicted, we might want to fill the delay slot.
+;; However, there appears to be no machinery to make the compiler
+;; recognize these scenarios.
+
+(define_insn_reservation "sh4_300_branch"  1
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "cbranch,jump,return,jump_ind"))
+  "sh4_300_issue+sh4_300_br")
+
+;; RTE
+(define_insn_reservation "sh4_300_return_from_exp" 9
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "rte"))
+  "sh4_300_pipe_01+sh4_300_pipe_02*9")
+
+;; OCBP, OCBWB
+;; Group:	CO
+;; Latency: 	1-5
+;; Issue Rate: 	1
+
+;; cwb is used for the sequence ocbwb @%0; extu.w %0,%2; or %1,%2; mov.l %0,@%2
+;; This description is likely inexact, but this pattern should not actually
+;; appear when compiling for sh4-300; we should use isbi instead.
+;; If a -mtune option is added later, we should use the icache array
+;; dispatch method instead.
+(define_insn_reservation "sh4_300_ocbwb"  3
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "cwb"))
+  "all*3")
+
+;; JSR,BSR,BSRF
+;; Calls have a mandatory delay slot, which we'd like to fill with an insn
+;; that can be paired with the call itself.
+;; Scheduling runs before reorg, so we approximate this by saying that we
+;; want the call to be paired with a preceding insn.
+;; In most cases, the insn that loads the address of the call should have
+;; a non-zero latency (mov rn,rm doesn't make sense since we could use rn
+;; for the address then).  Thus, a preceding insn that can be paired with
+;; a call should be elegible for the delay slot.
+;;
+;; calls introduce a longisch delay that is likely to flush the pipelines
+;; of the caller's instructions.  Ordinary functions tend to end with a
+;; load to restore a register (in the delay slot of rts), while sfuncs
+;; tend to end with an EX or MT insn.  But that is not actually relevant,
+;; since there are no instructions that contend for memory access early.
+;; We could, of course, provide exact scheduling information for specific
+;; sfuncs, if that should prove useful.
+
+(define_insn_reservation "sh4_300_call" 16
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "call,sfunc"))
+  "sh4_300_issue+sh4_300_br,all*15")
+
+;; FMOV.S / FMOV.D
+(define_insn_reservation "sh4_300_fmov" 1
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "fmove"))
+  "sh4_300_issue+sh4_300_fpt")
+
+;; LDS to FPSCR
+(define_insn_reservation "sh4_300_fpscr_load" 8
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "gp_fpscr"))
+  "sh4_300_issue+sh4_300_fpu+sh4_300_fpt")
+
+;; LDS.L to FPSCR
+(define_insn_reservation "sh4_300_fpscr_load_mem" 8
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type"  "mem_fpscr"))
+  "sh4_300_issue+sh4_300_fpu+sh4_300_fpt+sh4_300_ls")
+
+
+;; Fixed point multiplication (DMULS.L DMULU.L MUL.L MULS.W,MULU.W)
+(define_insn_reservation "multi" 2
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "smpy,dmpy"))
+  "sh4_300_issue+sh4_300_mul")
+
+;; FPCHG, FRCHG, FSCHG
+(define_insn_reservation "fpscr_toggle"  1
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "fpscr_toggle"))
+  "sh4_300_issue+sh4_300_fpu+sh4_300_fpt")
+
+;; FCMP/EQ, FCMP/GT
+(define_insn_reservation "fp_cmp"  3
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "fp_cmp,dfp_cmp"))
+  "sh4_300_issue+sh4_300_fpu")
+
+;; Single precision floating point (FADD,FLOAT,FMAC,FMUL,FSUB,FTRC)
+;; Double-precision floating-point (FADD,FCNVDS,FCNVSD,FLOAT,FSUB,FTRC)
+(define_insn_reservation "fp_arith"  6
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "fp,ftrc_s,dfp_arith,dfp_conv"))
+  "sh4_300_issue+sh4_300_fpu")
+
+;; Single Precision FDIV/SQRT
+(define_insn_reservation "fp_div" 19
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "fdiv"))
+  "sh4_300_issue+sh4_300_fpu+sh4_300_fds,sh4_300_fds*15")
+
+;; Double-precision floating-point FMUL
+(define_insn_reservation "dfp_mul" 9
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "dfp_mul"))
+  "sh4_300_issue+sh4_300_fpu,sh4_300_fpu*3")
+
+;; Double precision FDIV/SQRT
+(define_insn_reservation "dp_div" 35
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "dfdiv"))
+  "sh4_300_issue+sh4_300_fpu+sh4_300_fds,sh4_300_fds*31")
+
+
+;; ??? We don't really want these for sh4-300.
+;; this pattern itself is likely to finish in 3 cycles, but also
+;; to disrupt branch prediction for taken branches for the following
+;; condbranch.
+(define_insn_reservation "sh4_300_arith3" 5
+  (and (eq_attr "pipe_model" "sh4_300")
+       (eq_attr "type" "arith3"))
+  "sh4_300_issue,all*4")
+
+;; arith3b insns without brach redirection make use of the 0-offset 0-latency
+;; branch feature, and thus schedule the same no matter if the branch is taken
+;; or not.  If the branch is redirected, the taken branch might take longer,
+;; but then, we don't have to take the next branch.
+;; ??? should we suppress branch redirection for sh4-300 to improve branch
+;; target hit rates?
+(define_insn_reservation "arith3b" 2
+  (and (eq_attr "pipe_model" "sh4")
+       (eq_attr "type" "arith3"))
+  "issue,all")
diff -Naur gcc-4.2.1/gcc/config/sh/sh4a.md gcc-4.2.1-stm/gcc/config/sh/sh4a.md
--- gcc-4.2.1/gcc/config/sh/sh4a.md	2005-06-25 02:22:41.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/sh4a.md	2007-08-10 13:08:59.000000000 +0100
@@ -1,5 +1,6 @@
 ;; Scheduling description for Renesas SH4a
 ;; Copyright (C) 2003, 2004 Free Software Foundation, Inc.
+;; Copyright (c) 2006  STMicroelectronics.
 ;;
 ;; This file is part of GCC.
 ;;
@@ -98,9 +99,11 @@
 ;; MOV
 ;; Group: MT
 ;; Latency: 0
+;; ??? not sure if movi8 belongs here, but that's where it was
+;; effectively before.
 (define_insn_reservation "sh4a_mov" 0
   (and (eq_attr "cpu" "sh4a")
-       (eq_attr "type" "move"))
+       (eq_attr "type" "move,movi8,gp_mac"))
   "ID_or")
 
 ;; Load
@@ -108,7 +111,7 @@
 ;; Latency: 3
 (define_insn_reservation "sh4a_load" 3
   (and (eq_attr "cpu" "sh4a")
-       (eq_attr "type" "load,pcload"))
+       (eq_attr "type" "load,pcload,mem_mac"))
   "sh4a_ls+sh4a_memory")
 
 (define_insn_reservation "sh4a_load_si" 3
@@ -121,7 +124,7 @@
 ;; Latency: 0
 (define_insn_reservation "sh4a_store" 0
   (and (eq_attr "cpu" "sh4a")
-       (eq_attr "type" "store"))
+       (eq_attr "type" "store,fstore,mac_mem"))
   "sh4a_ls+sh4a_memory")
 
 ;; CWB TYPE
@@ -177,7 +180,7 @@
 ;; Latency: 	3
 (define_insn_reservation "sh4a_fp_arith"  3
   (and (eq_attr "cpu" "sh4a")
-       (eq_attr "type" "fp"))
+       (eq_attr "type" "fp,fp_cmp,fpscr_toggle"))
   "ID_or,sh4a_fex")
 
 (define_insn_reservation "sh4a_fp_arith_ftrc"  3
@@ -207,7 +210,7 @@
 ;; Latency: 	5
 (define_insn_reservation "sh4a_fp_double_arith" 5
   (and (eq_attr "cpu" "sh4a")
-       (eq_attr "type" "dfp_arith"))
+       (eq_attr "type" "dfp_arith,dfp_mul"))
   "ID_or,sh4a_fex*3")
 
 ;; Double precision FDIV/SQRT
diff -Naur gcc-4.2.1/gcc/config/sh/sh4.md gcc-4.2.1-stm/gcc/config/sh/sh4.md
--- gcc-4.2.1/gcc/config/sh/sh4.md	2005-06-25 02:22:41.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/sh4.md	2007-08-10 13:08:59.000000000 +0100
@@ -1,5 +1,6 @@
 ;; DFA scheduling description for SH4.
 ;; Copyright (C) 2004 Free Software Foundation, Inc.
+;; Copyright (c) 2006  STMicroelectronics.
 
 ;; This file is part of GCC.
 
@@ -209,9 +210,14 @@
 
 (define_insn_reservation "sh4_store" 1
   (and (eq_attr "pipe_model" "sh4")
-       (eq_attr "type" "store"))
+       (eq_attr "type" "store,fstore"))
   "issue+load_store,nothing,memory")
 
+(define_insn_reservation "mac_mem" 1
+  (and (eq_attr "pipe_model" "sh4")
+       (eq_attr "type" "mac_mem"))
+  "d_lock,nothing,memory")
+
 ;; Load Store instructions.
 ;; Group:	LS
 ;; Latency: 	1
@@ -372,35 +378,42 @@
 ;; Fixed point multiplication (DMULS.L DMULU.L MUL.L MULS.W,MULU.W)
 ;; Group:	CO
 ;; Latency: 	4 / 4
-;; Issue Rate: 	1
+;; Issue Rate: 	2
 
 (define_insn_reservation "multi" 4
   (and (eq_attr "pipe_model" "sh4")
        (eq_attr "type" "smpy,dmpy"))
   "d_lock,(d_lock+f1_1),(f1_1|f1_2)*3,F2")
 
-;; Fixed STS from MACL / MACH
+;; Fixed STS from, and LDS to MACL / MACH
 ;; Group:	CO
 ;; Latency: 	3
 ;; Issue Rate: 	1
 
 (define_insn_reservation "sh4_mac_gp" 3
   (and (eq_attr "pipe_model" "sh4")
-       (eq_attr "type" "mac_gp"))
+       (eq_attr "type" "mac_gp,gp_mac,mem_mac"))
   "d_lock")
 
 
 ;; Single precision floating point computation FCMP/EQ,
-;; FCMP/GT, FADD, FLOAT, FMAC, FMUL, FSUB, FTRC, FRVHG, FSCHG
+;; FCMP/GT, FADD, FLOAT, FMAC, FMUL, FSUB, FTRC, FRCHG, FSCHG
 ;; Group:	FE
 ;; Latency: 	3/4
 ;; Issue Rate: 	1
 
 (define_insn_reservation "fp_arith"  3
   (and (eq_attr "pipe_model" "sh4")
-       (eq_attr "type" "fp"))
+       (eq_attr "type" "fp,fp_cmp"))
   "issue,F01,F2")
 
+;; We don't model the resource usage of this exactly because that would
+;; introduce a bogus latency.
+(define_insn_reservation "sh4_fpscr_toggle"  1
+  (and (eq_attr "pipe_model" "sh4")
+       (eq_attr "type" "fpscr_toggle"))
+  "issue")
+
 (define_insn_reservation "fp_arith_ftrc"  3
   (and (eq_attr "pipe_model" "sh4")
        (eq_attr "type" "ftrc_s"))
@@ -437,7 +450,7 @@
 
 (define_insn_reservation "fp_double_arith" 8
   (and (eq_attr "pipe_model" "sh4")
-       (eq_attr "type" "dfp_arith"))
+       (eq_attr "type" "dfp_arith,dfp_mul"))
   "issue,F01,F1+F2,fpu*4,F2")
 
 ;; Double-precision FCMP (FCMP/EQ,FCMP/GT)
diff -Naur gcc-4.2.1/gcc/config/sh/sh.c gcc-4.2.1-stm/gcc/config/sh/sh.c
--- gcc-4.2.1/gcc/config/sh/sh.c	2007-05-21 00:27:22.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/sh.c	2007-08-10 13:08:59.000000000 +0100
@@ -3,6 +3,7 @@
    2003, 2004, 2005, 2006 Free Software Foundation, Inc.
    Contributed by Steve Chamberlain (sac@cygnus.com).
    Improved by Jim Wilson (wilson@cygnus.com).
+   Copyright (c) 2006  STMicroelectronics.
 
 This file is part of GCC.
 
@@ -271,6 +272,7 @@
 			         tree, bool);
 static int sh_dwarf_calling_convention (tree);
 static int hard_regs_intersect_p (HARD_REG_SET *, HARD_REG_SET *);
+static void sh_expand_float_condop (rtx operands[4], rtx (*[2]) (rtx));
 
 
 /* Initialize the GCC target structure.  */
@@ -477,6 +479,9 @@
 #undef TARGET_SECONDARY_RELOAD
 #define TARGET_SECONDARY_RELOAD sh_secondary_reload
 
+#undef TARGET_MATCH_ADJUST
+#define TARGET_MATCH_ADJUST sh_match_adjust
+
 struct gcc_target targetm = TARGET_INITIALIZER;
 
 /* Implement TARGET_HANDLE_OPTION.  */
@@ -526,10 +531,15 @@
     case OPT_m4:
     case OPT_m4_100:
     case OPT_m4_200:
+    case OPT_m4_300:
       target_flags = (target_flags & ~MASK_ARCH) | SELECT_SH4;
       return true;
 
     case OPT_m4_nofpu:
+    case OPT_m4_100_nofpu:
+    case OPT_m4_200_nofpu:
+    case OPT_m4_300_nofpu:
+    case OPT_m4_340:
     case OPT_m4_400:
     case OPT_m4_500:
       target_flags = (target_flags & ~MASK_ARCH) | SELECT_SH4_NOFPU;
@@ -538,12 +548,14 @@
     case OPT_m4_single:
     case OPT_m4_100_single:
     case OPT_m4_200_single:
+    case OPT_m4_300_single:
       target_flags = (target_flags & ~MASK_ARCH) | SELECT_SH4_SINGLE;
       return true;
 
     case OPT_m4_single_only:
     case OPT_m4_100_single_only:
     case OPT_m4_200_single_only:
+    case OPT_m4_300_single_only:
       target_flags = (target_flags & ~MASK_ARCH) | SELECT_SH4_SINGLE_ONLY;
       return true;
 
@@ -1339,6 +1351,301 @@
   return 0;
 }
 
+enum rtx_code
+prepare_cbranch_operands (rtx *operands, enum machine_mode mode,
+			  enum rtx_code comparison)
+{
+  rtx op1;
+  rtx scratch = NULL_RTX;
+
+  if (comparison == CODE_FOR_nothing)
+    comparison = GET_CODE (operands[0]);
+  else
+    scratch = operands[4];
+  if (GET_CODE (operands[1]) == CONST_INT
+      && GET_CODE (operands[2]) != CONST_INT)
+    {
+      rtx tmp = operands[1];
+
+      operands[1] = operands[2];
+      operands[2] = tmp;
+      comparison = swap_condition (comparison);
+    }
+  if (GET_CODE (operands[2]) == CONST_INT)
+    {
+      HOST_WIDE_INT val = INTVAL (operands[2]);
+      if ((val == -1 || val == -0x81)
+	  && (comparison == GT || comparison == LE))
+	{
+	  comparison = (comparison == GT) ? GE : LT;
+	  operands[2] = gen_int_mode (val + 1, mode);
+	}
+      else if ((val == 1 || val == 0x80)
+	       && (comparison == GE || comparison == LT))
+	{
+	  comparison = (comparison == GE) ? GT : LE;
+	  operands[2] = gen_int_mode (val - 1, mode);
+	}
+      else if (val == 1 && (comparison == GEU || comparison == LTU))
+	{
+	  comparison = (comparison == GEU) ? NE : EQ;
+	  operands[2] = CONST0_RTX (mode);
+	}
+      else if (val == 0x80 && (comparison == GEU || comparison == LTU))
+	{
+	  comparison = (comparison == GEU) ? GTU : LEU;
+	  operands[2] = gen_int_mode (val - 1, mode);
+	}
+      else if (val == 0 && (comparison == GTU || comparison == LEU))
+	comparison = (comparison == GTU) ? NE : EQ;
+      else if (mode == SImode
+	       && ((val == 0x7fffffff
+		    && (comparison == GTU || comparison == LEU))
+		   || ((unsigned HOST_WIDE_INT) val
+			== (unsigned HOST_WIDE_INT) 0x7fffffff + 1
+		       && (comparison == GEU || comparison == LTU))))
+	{
+	  comparison = (comparison == GTU || comparison == GEU) ? LT : GE;
+	  operands[2] = CONST0_RTX (mode);
+	}
+    }
+  op1 = operands[1];
+  if (!no_new_pseudos)
+    operands[1] = force_reg (mode, op1);
+  /* When we are handling DImode comparisons, we want to keep constants so
+     that we can optimize the component comparisons; however, memory loads
+     are better issued as a whole so that they can be scheduled well.
+     SImode equality comparisons allow I08 constants, but only when they
+     compare r0.  Hence, if operands[1] has to be loaded from somewhere else
+     into a register, that register might as well be r0, and we allow the
+     constant.  If it is already in a register, this is likely to be
+     allocatated to a different hard register, thus we load the constant into
+     a register unless it is zero.  */
+  if (!REG_P (operands[2])
+      && (GET_CODE (operands[2]) != CONST_INT
+	  || (mode == SImode && operands[2] != CONST0_RTX (SImode)
+	      && ((comparison != EQ && comparison != NE)
+		  || (REG_P (op1) && REGNO (op1) != R0_REG)
+		  || !CONST_OK_FOR_I08 (INTVAL (operands[2]))))))
+    {
+      if (scratch && GET_MODE (scratch) == mode)
+	{
+	  emit_move_insn (scratch, operands[2]);
+	  operands[2] = scratch;
+	}
+      else if (!no_new_pseudos)
+	operands[2] = force_reg (mode, operands[2]);
+    }
+  return comparison;
+}
+
+void
+expand_cbranchsi4 (rtx *operands, enum rtx_code comparison, int probability)
+{
+  rtx (*branch_expander) (rtx) = gen_branch_true;
+  rtx jump;
+
+  comparison = prepare_cbranch_operands (operands, SImode, comparison);
+  switch (comparison)
+    {
+    case NE: case LT: case LE: case LTU: case LEU:
+      comparison = reverse_condition (comparison);
+      branch_expander = gen_branch_false;
+    default: ;
+    }
+  emit_insn (gen_rtx_SET (VOIDmode, gen_rtx_REG (SImode, T_REG),
+                          gen_rtx_fmt_ee (comparison, SImode,
+                                          operands[1], operands[2])));
+  jump = emit_jump_insn (branch_expander (operands[3]));
+  if (probability >= 0)
+    REG_NOTES (jump)
+      = gen_rtx_EXPR_LIST (REG_BR_PROB, GEN_INT (probability),
+                           REG_NOTES (jump));
+
+}
+
+/* ??? How should we distribute probabilities when more than one branch
+   is generated.  So far we only have soem ad-hoc observations:
+   - If the operands are random, they are likely to differ in both parts.
+   - If comparing items in a hash chain, the operands are random or equal;
+     operation should be EQ or NE.
+   - If items are searched in an ordered tree from the root, we can expect
+     the highpart to be unequal about half of the time; operation should be
+     an unequality comparison, operands non-constant, and overall probability
+     about 50%.  Likewise for quicksort.
+   - Range checks will be often made against constants.  Even if we assume for
+     simplicity an even distribution of the non-constant operand over a
+     sub-range here, the same probability could be generated with differently
+     wide sub-ranges - as long as the ratio of the part of the subrange that
+     is before the threshold to the part that comes after the threshold stays
+     the same.  Thus, we can't really tell anything here;
+     assuming random distribution is at least simple.
+ */
+
+bool
+expand_cbranchdi4 (rtx *operands, enum rtx_code comparison)
+{
+  enum rtx_code msw_taken, msw_skip, lsw_taken;
+  rtx skip_label;
+  rtx op1h, op1l, op2h, op2l;
+  int num_branches;
+  int prob, rev_prob;
+  int msw_taken_prob = -1, msw_skip_prob = -1, lsw_taken_prob = -1;
+  rtx scratch = operands[4];
+
+  comparison = prepare_cbranch_operands (operands, DImode, comparison);
+  op1h = gen_highpart_mode (SImode, DImode, operands[1]);
+  op2h = gen_highpart_mode (SImode, DImode, operands[2]);
+  op1l = gen_lowpart (SImode, operands[1]);
+  op2l = gen_lowpart (SImode, operands[2]);
+  msw_taken = msw_skip = lsw_taken = CODE_FOR_nothing;
+  prob = split_branch_probability;
+  rev_prob = REG_BR_PROB_BASE - prob;
+  switch (comparison)
+    {
+    /* ??? Should we use the cmpeqdi_t pattern for equality comparisons?
+       That costs 1 cycle more when the first branch can be predicted taken,
+       but saves us mispredicts because only one branch needs prediction.
+       It also enables generating the cmpeqdi_t-1 pattern.  */
+    case EQ:
+      if (TARGET_CMPEQDI_T)
+	{
+	  emit_insn (gen_cmpeqdi_t (operands[1], operands[2]));
+	  emit_jump_insn (gen_branch_true (operands[3]));
+	  return true;
+	}
+      msw_skip = NE;
+      lsw_taken = EQ;
+      if (prob >= 0)
+	{
+	  /* If we had more precision, we'd use rev_prob - (rev_prob >> 32) .
+	   */
+	  msw_skip_prob = rev_prob;
+	  if (REG_BR_PROB_BASE <= 65535)
+	    lsw_taken_prob = prob ? REG_BR_PROB_BASE : 0;
+	  else
+	    {
+	      gcc_assert (HOST_BITS_PER_WIDEST_INT >= 64);
+	      lsw_taken_prob
+		= (prob
+		   ? (REG_BR_PROB_BASE
+		      - ((HOST_WIDEST_INT) REG_BR_PROB_BASE * rev_prob
+			 / ((HOST_WIDEST_INT) prob << 32)))
+		   : 0);
+	    }
+	}
+      break;
+    case NE:
+      if (TARGET_CMPEQDI_T)
+	{
+	  emit_insn (gen_cmpeqdi_t (operands[1], operands[2]));
+	  emit_jump_insn (gen_branch_false (operands[3]));
+	  return true;
+	}
+      msw_taken = NE;
+      msw_taken_prob = prob;
+      lsw_taken = NE;
+      lsw_taken_prob = 0;
+      break;
+    case GTU: case GT:
+      msw_taken = comparison;
+      if (GET_CODE (op2l) == CONST_INT && INTVAL (op2l) == -1)
+	break;
+      if (comparison != GTU || op2h != CONST0_RTX (SImode))
+	msw_skip = swap_condition (msw_taken);
+      lsw_taken = GTU;
+      break;
+    case GEU: case GE:
+      if (op2l == CONST0_RTX (SImode))
+	msw_taken = comparison;
+      else
+	{
+	  msw_taken = comparison == GE ? GT : GTU;
+	  msw_skip = swap_condition (msw_taken);
+	  lsw_taken = GEU;
+	}
+      break;
+    case LTU: case LT:
+      msw_taken = comparison;
+      if (op2l == CONST0_RTX (SImode))
+	break;
+      msw_skip = swap_condition (msw_taken);
+      lsw_taken = LTU;
+      break;
+    case LEU: case LE:
+      if (GET_CODE (op2l) == CONST_INT && INTVAL (op2l) == -1)
+	msw_taken = comparison;
+      else
+	{
+	  lsw_taken = LEU;
+	  if (comparison == LE)
+	    msw_taken = LT;
+	  else if (op2h != CONST0_RTX (SImode))
+	    msw_taken = LTU;
+	  else
+	    break;
+	  msw_skip = swap_condition (msw_taken);
+	}
+      break;
+    default: return false;
+    }
+  num_branches = ((msw_taken != CODE_FOR_nothing)
+		  + (msw_skip != CODE_FOR_nothing)
+		  + (lsw_taken != CODE_FOR_nothing));
+  if (comparison != EQ && comparison != NE && num_branches > 1)
+    {
+      if (!CONSTANT_P (operands[2])
+	  && prob >= (int) (REG_BR_PROB_BASE * 3 / 8U)
+	  && prob <= (int) (REG_BR_PROB_BASE * 5 / 8U))
+	{
+	  msw_taken_prob = prob / 2U;
+	  msw_skip_prob
+	    = REG_BR_PROB_BASE * rev_prob / (REG_BR_PROB_BASE + rev_prob);
+	  lsw_taken_prob = prob;
+	}
+      else
+	{
+	  msw_taken_prob = prob;
+	  msw_skip_prob = REG_BR_PROB_BASE;
+	  /* ??? If we have a constant op2h, should we use that when
+	     calculating lsw_taken_prob?  */
+	  lsw_taken_prob = prob;
+	}
+    }
+  operands[1] = op1h;
+  operands[2] = op2h;
+  operands[4] = NULL_RTX;
+  if (reload_completed
+      && ! arith_reg_or_0_operand (op2h, SImode) && true_regnum (op1h)
+      && (msw_taken != CODE_FOR_nothing || msw_skip != CODE_FOR_nothing))
+    {
+      emit_move_insn (scratch, operands[2]);
+      operands[2] = scratch;
+    }
+  if (msw_taken != CODE_FOR_nothing)
+    expand_cbranchsi4 (operands, msw_taken, msw_taken_prob);
+  if (msw_skip != CODE_FOR_nothing)
+    {
+      rtx taken_label = operands[3];
+
+      operands[3] = skip_label = gen_label_rtx ();
+      expand_cbranchsi4 (operands, msw_skip, msw_skip_prob);
+      operands[3] = taken_label;
+    }
+  operands[1] = op1l;
+  operands[2] = op2l;
+  if (lsw_taken != CODE_FOR_nothing)
+    {
+      if (reload_completed
+	  && ! arith_reg_or_0_operand (op2l, SImode) && true_regnum (op1l))
+	operands[4] = scratch;
+      expand_cbranchsi4 (operands, lsw_taken, lsw_taken_prob);
+    }
+  if (msw_skip != CODE_FOR_nothing)
+    emit_label (skip_label);
+  return true;
+}
+
 /* Prepare the operands for an scc instruction; make sure that the
    compare has been done.  */
 rtx
@@ -1403,6 +1710,76 @@
   return t_reg;
 }
 
+static rtx
+sh_soft_fp_cmp (int code, enum machine_mode op_mode, rtx op0, rtx op1)
+{
+  const char *name = NULL;
+  rtx (*fun) (rtx, rtx), addr, tmp, first, last, equiv;
+  int df = op_mode == DFmode;
+  enum machine_mode mode = CODE_FOR_nothing; /* shut up warning.  */
+
+  switch (code)
+    {
+    case EQ:
+      if (!flag_finite_math_only)
+	{
+	  name = df ? "__nedf2f" : "__nesf2f";
+	  fun = df ? gen_cmpnedf_i1 : gen_cmpnesf_i1;
+	  mode = CC_FP_NEmode;
+	  break;
+	} /* Fall through.  */
+    case UNEQ:
+      fun = gen_cmpuneq_sdf;
+      break;
+    case UNLE:
+      if (flag_finite_math_only && !df)
+	{
+	  fun = gen_cmplesf_i1_finite;
+	  break;
+	}
+      name = df ? "__gtdf2t" : "__gtsf2t";
+      fun = df ? gen_cmpgtdf_i1 : gen_cmpgtsf_i1;
+      mode = CC_FP_GTmode;
+      break;
+    case GE:
+      if (flag_finite_math_only && !df)
+	{
+	  tmp = op0; op0 = op1; op1 = tmp;
+	  fun = gen_cmplesf_i1_finite;
+	  break;
+	}
+      name = df ? "__gedf2f" : "__gesf2f";
+      fun = df ? gen_cmpunltdf_i1 : gen_cmpunltsf_i1;
+      mode = CC_FP_UNLTmode;
+      break;
+    case UNORDERED:
+      fun = gen_cmpun_sdf;
+      break;
+    default: gcc_unreachable ();
+    }
+
+  if (!name)
+    return fun (force_reg (op_mode, op0), force_reg (op_mode, op1));
+
+  tmp = gen_reg_rtx (mode);
+  addr = gen_reg_rtx (Pmode);
+  function_symbol (addr, name, SFUNC_STATIC);
+  first = emit_move_insn (gen_rtx_REG (op_mode, R4_REG), op0);
+  emit_move_insn (gen_rtx_REG (op_mode, R5_REG + df), op1);
+  last = emit_insn (fun (tmp, addr));
+  equiv = gen_rtx_fmt_ee (COMPARE, mode, op0, op1);
+  REG_NOTES (last) = gen_rtx_EXPR_LIST (REG_EQUAL, equiv, REG_NOTES (last));
+  /* Wrap the sequence in REG_LIBCALL / REG_RETVAL notes so that loop
+     invariant code motion can move it.  */
+  REG_NOTES (first) = gen_rtx_INSN_LIST (REG_LIBCALL, last, REG_NOTES (first));
+  REG_NOTES (last) = gen_rtx_INSN_LIST (REG_RETVAL, first, REG_NOTES (last));
+  /* Use fpcmp_i1 rather than cmpeqsi_t, so that the optimizers can grok
+     the computation.  */
+  return gen_rtx_SET (VOIDmode,
+		      gen_rtx_REG (SImode, T_REG),
+		      gen_rtx_fmt_ee (code, SImode, tmp, CONST0_RTX (mode)));
+}
+
 /* Called from the md file, set up the operands of a compare instruction.  */
 
 void
@@ -1412,22 +1789,35 @@
   rtx insn;
   if (mode == VOIDmode)
     mode = GET_MODE (sh_compare_op1);
-  if (code != EQ
-      || mode == DImode
-      || (TARGET_SH2E && GET_MODE_CLASS (mode) == MODE_FLOAT))
+  if (GET_MODE_CLASS (mode) == MODE_FLOAT
+      ? !TARGET_SH1_SOFTFP_MODE (mode)
+      : (code != EQ || mode == DImode))
     {
       /* Force args into regs, since we can't use constants here.  */
       sh_compare_op0 = force_reg (mode, sh_compare_op0);
       if (sh_compare_op1 != const0_rtx
 	  || code == GTU  || code == GEU
-	  || (TARGET_SH2E && GET_MODE_CLASS (mode) == MODE_FLOAT))
+	  || GET_MODE_CLASS (mode) == MODE_FLOAT)
 	sh_compare_op1 = force_reg (mode, sh_compare_op1);
     }
-  if (TARGET_SH2E && GET_MODE_CLASS (mode) == MODE_FLOAT && code == GE)
+  if (GET_MODE_CLASS (mode) == MODE_FLOAT
+      && flag_finite_math_only
+      && !TARGET_SH1_SOFTFP_MODE (mode))
+    switch (code)
+      {
+      case UNGT: code = GT; break;
+      case UNEQ: code = EQ; break;
+      }
+  if (GET_MODE_CLASS (mode) == MODE_FLOAT && code == GE
+      && !TARGET_SH1_SOFTFP_MODE (mode))
     {
       from_compare (operands, GT);
-      insn = gen_ieee_ccmpeqsf_t (sh_compare_op0, sh_compare_op1);
+      insn = gen_ieee_ccmpeqsf_t (copy_rtx (sh_compare_op0),
+				  copy_rtx (sh_compare_op1));
     }
+  else if (GET_MODE_CLASS (mode) == MODE_FLOAT
+	   && TARGET_SH1_SOFTFP_MODE (mode))
+    insn = sh_soft_fp_cmp (code, mode, sh_compare_op0, sh_compare_op1);
   else
     insn = gen_rtx_SET (VOIDmode,
 			gen_rtx_REG (SImode, T_REG),
@@ -1721,6 +2111,12 @@
     }
 }
 
+/* Output a code sequence for INSN using TEMPLATE with OPERANDS; but before,
+   fill in operands 9 as a label to the successor insn.
+   We try to use jump threading where possible.
+   IF CODE matches the comparison in the IF_THEN_ELSE of a following jump,
+   we assume the jump is taken.  I.e. EQ means follow jmp and bf, NE means
+   follow jmp and bt, if the address is in range.  */
 const char *
 output_branchy_insn (enum rtx_code code, const char *template,
 		     rtx insn, rtx *operands)
@@ -2115,6 +2511,15 @@
       else if ((outer_code == AND || outer_code == IOR || outer_code == XOR)
 	       && CONST_OK_FOR_K08 (INTVAL (x)))
         *total = 1;
+      /* prepare_cmp_insn will force costly constants int registers before
+	 the cbrach[sd]i4 pattterns can see them, so preserve potentially
+	 interesting ones not covered by I08 above.  */
+      else if (outer_code == COMPARE
+	       && ((unsigned HOST_WIDE_INT) INTVAL (x)
+		    == (unsigned HOST_WIDE_INT) 0x7fffffff + 1
+		    || INTVAL (x) == 0x7fffffff
+		   || INTVAL (x) == 0x80 || INTVAL (x) == -0x81))
+        *total = 1;
       else
         *total = 8;
       return true;
@@ -2133,6 +2538,11 @@
     case CONST_DOUBLE:
       if (TARGET_SHMEDIA)
         *total = COSTS_N_INSNS (4);
+      /* prepare_cmp_insn will force costly constants int registers before
+	 the cbrachdi4 patttern can see them, so preserve potentially
+	 interesting ones.  */
+      else if (outer_code == COMPARE && GET_MODE (x) == DImode)
+        *total = 1;
       else
         *total = 10;
       return true;
@@ -3590,7 +4000,6 @@
 
       if (GET_CODE (from) == BARRIER)
 	{
-
 	  found_barrier = from;
 
 	  /* If we are at the end of the function, or in front of an alignment
@@ -4240,6 +4649,27 @@
     }
 }
 
+int sh_jump_align (rtx label)
+{
+  if (sh_align_small_blocks && TARGET_CACHE32)
+    {
+      rtx insn;
+      int size = 0;
+
+      for (insn = NEXT_INSN (label);
+	   insn && GET_CODE (insn) != BARRIER &&
+	     GET_CODE (insn) != CODE_LABEL;
+	   insn = NEXT_INSN (insn))
+	if (INSN_P (insn))
+	  size += get_attr_length (insn);
+
+      if (size <= sh_align_small_blocks)
+	return 0;
+    }
+
+  return align_jumps_log;
+}
+
 /* BARRIER_OR_LABEL is either a BARRIER or a CODE_LABEL immediately following
    a barrier.  Return the base 2 logarithm of the desired alignment.  */
 int
@@ -4247,6 +4677,7 @@
 {
   rtx next = next_real_insn (barrier_or_label), pat, prev;
   int slot, credit, jump_to_next = 0;
+  rtx label;
 
   if (! next)
     return 0;
@@ -4353,6 +4784,26 @@
 	}
     }
 
+  if (sh_align_small_blocks && TARGET_CACHE32) 
+    {
+      label = NEXT_INSN (barrier_or_label);
+      if (label && GET_CODE (label) == CODE_LABEL)
+	{
+	  rtx insn;
+	  int size = 0;
+
+	  for (insn = NEXT_INSN (label);
+	       insn && GET_CODE (insn) != BARRIER && 
+		 GET_CODE (insn) != CODE_LABEL;
+	       insn = NEXT_INSN (insn))
+	    if (INSN_P (insn))
+	      size += get_attr_length (insn);
+
+	  if (size <= sh_align_small_blocks)
+	    return 0;
+	}
+    }
+
   return align_jumps_log;
 }
 
@@ -5451,13 +5902,13 @@
 static void
 push_regs (HARD_REG_SET *mask, int interrupt_handler)
 {
-  int i;
+  int i = interrupt_handler ? LAST_BANKED_REG + 1 : 0;
   int skip_fpscr = 0;
 
   /* Push PR last; this gives better latencies after the prologue, and
      candidates for the return delay slot when there are no general
      registers pushed.  */
-  for (i = 0; i < FIRST_PSEUDO_REGISTER; i++)
+  for (; i < FIRST_PSEUDO_REGISTER; i++)
     {
       /* If this is an interrupt handler, and the SZ bit varies,
 	 and we have to push any floating point register, we need
@@ -5477,6 +5928,13 @@
 	  && TEST_HARD_REG_BIT (*mask, i))
 	push (i);
     }
+
+  /* Push banked registers last to improve delay slot opportunities.  */
+  if (interrupt_handler)
+    for (i = FIRST_BANKED_REG; i <= LAST_BANKED_REG; i++)
+      if (TEST_HARD_REG_BIT (*mask, i))
+	push (i);
+
   if (TEST_HARD_REG_BIT (*mask, PR_REG))
     push (PR_REG);
 }
@@ -6444,10 +6902,26 @@
     }
   else /* ! TARGET_SH5 */
     {
+      int last_reg;
+
       save_size = 0;
       if (TEST_HARD_REG_BIT (live_regs_mask, PR_REG))
 	pop (PR_REG);
-      for (i = 0; i < FIRST_PSEUDO_REGISTER; i++)
+
+      /* Banked registers are poped first to avoid being scheduled in the
+	 delay slot. RTE switches banks before the ds instruction.  */
+      if (current_function_interrupt)
+	{
+	  for (i = FIRST_BANKED_REG; i <= LAST_BANKED_REG; i++)
+	    if (TEST_HARD_REG_BIT (live_regs_mask, i)) 
+	      pop (LAST_BANKED_REG - i);
+
+	  last_reg = FIRST_PSEUDO_REGISTER - LAST_BANKED_REG - 1;
+	}
+      else
+	last_reg = FIRST_PSEUDO_REGISTER;
+
+      for (i = 0; i < last_reg; i++)
 	{
 	  int j = (FIRST_PSEUDO_REGISTER - 1) - i;
 
@@ -6457,9 +6931,9 @@
 	    fpscr_deferred = 1;
 	  else if (j != PR_REG && TEST_HARD_REG_BIT (live_regs_mask, j))
 	    pop (j);
+
 	  if (j == FIRST_FP_REG && fpscr_deferred)
 	    pop (FPSCR_REG);
-
 	}
     }
   if (target_flags != save_flags && ! current_function_interrupt)
@@ -8111,6 +8585,69 @@
   emit_df_insn ((*fun) (operands[0], operands[1], operands[2],
 			get_fpscr_rtx ()));
 }
+
+/* Expand an sfunc operation taking NARGS MODE arguments, using generator
+   function FUN, which needs symbol NAME loaded int a register first.
+   Add a REG_EQUAL note using EQUIV.  */
+static void
+expand_sfunc_op (int nargs, enum machine_mode mode, rtx (*fun) (rtx, rtx),
+		 const char *name, rtx equiv, rtx *operands)
+{
+  int i;
+  rtx addr, first = NULL_RTX, last, insn;
+  /* For now keep all variants ABI compatibilities. 
+     Check for TARGET_OSFP: ARG_TO_R4 (see ieee-754-df.S) and _SH_FPU_ANY_.  */
+  int next_reg = FIRST_PARM_REG;
+
+  addr = gen_reg_rtx (Pmode);
+  function_symbol (addr, name, SFUNC_FREQUENT);
+
+  for (i = 1; i <= nargs; i++)
+    {
+      insn = emit_move_insn (gen_rtx_REG (mode, next_reg), operands[i]);
+      if (!first)
+	first = insn;
+      next_reg += GET_MODE_SIZE (mode) / UNITS_PER_WORD;
+    }
+  last = emit_insn ((*fun) (operands[0], addr));
+  REG_NOTES (last) = gen_rtx_EXPR_LIST (REG_EQUAL, equiv, REG_NOTES (last));
+
+  /* If flag_non_call_exceptions is in effect, it will stipulate BB boundaries
+     where we don't want them; we must not have a LIBCALL block spanning
+     multiple basic blocks.  */
+  if (flag_non_call_exceptions)
+    {
+      for (insn = first; insn != last; insn = NEXT_INSN (insn))
+	if (may_trap_p (insn))
+	  return;
+    }
+  /* Wrap the sequence in REG_LIBCALL / REG_RETVAL notes so that loop
+     invariant code motion can move it.  */
+  REG_NOTES (first) = gen_rtx_INSN_LIST (REG_LIBCALL, last, REG_NOTES (first));
+  REG_NOTES (last) = gen_rtx_INSN_LIST (REG_RETVAL, first, REG_NOTES (last));
+}
+
+/* Expand an sfunc unary operation taking an MODE argument, using generator
+   function FUN, which needs symbol NAME loaded int a register first.
+   Add a REG_EQUAL note using CODE.  */
+void
+expand_sfunc_unop (enum machine_mode mode, rtx (*fun) (rtx, rtx),
+		   const char *name, enum rtx_code code, rtx *operands)
+{
+  rtx equiv = gen_rtx_fmt_e (code, GET_MODE (operands[0]), operands[1]);
+  expand_sfunc_op (1, mode, fun, name, equiv, operands);
+}
+
+/* Expand an sfunc binary operation in MODE, using generator function FUN,
+   which needs symbol NAME loaded int a register first.
+   Add a REG_EQUAL note using CODE.  */
+void
+expand_sfunc_binop (enum machine_mode mode, rtx (*fun) (rtx, rtx),
+		    const char *name, enum rtx_code code, rtx *operands)
+{
+  rtx equiv = gen_rtx_fmt_ee (code, mode, operands[1], operands[2]);
+  expand_sfunc_op (2, mode, fun, name, equiv, operands);
+}
 
 /* ??? gcc does flow analysis strictly after common subexpression
    elimination.  As a result, common subexpression elimination fails
@@ -8220,6 +8757,13 @@
   return gen_rtx_REG (Pmode, 7);
 }
 
+/* This function switches the fpscr.  */
+void
+emit_fpu_flip (void)
+{
+  emit_insn (gen_toggle_pr ());
+}
+
 /* This function will set the fpscr from memory.
    MODE is the mode we are setting it to.  */
 void
@@ -8578,23 +9122,32 @@
     }
   else if (REG_NOTE_KIND (link) == 0)
     {
-      enum attr_type dep_type, type;
+      enum attr_type type;
+      rtx dep_set;
 
       if (recog_memoized (insn) < 0
 	  || recog_memoized (dep_insn) < 0)
 	return cost;
 
-      dep_type = get_attr_type (dep_insn);
-      if (dep_type == TYPE_FLOAD || dep_type == TYPE_PCFLOAD)
-	cost--;
-      if ((dep_type == TYPE_LOAD_SI || dep_type == TYPE_PCLOAD_SI)
-	  && (type = get_attr_type (insn)) != TYPE_CALL
-	  && type != TYPE_SFUNC)
-	cost--;
+      dep_set = single_set (dep_insn);
 
+      /* The latency that we specify in the scheduling description refers
+	 to the actual output, not to an auto-increment register; for that,
+	 the latency is one.  */
+      if (dep_set && MEM_P (SET_SRC (dep_set)) && cost > 1)
+	{
+	  rtx set = single_set (insn);
+
+	  if (set
+	      && !reg_mentioned_p (SET_DEST (dep_set), SET_SRC (set))
+	      && (!MEM_P (SET_DEST (set))
+		  || !reg_mentioned_p (SET_DEST (dep_set),
+				       XEXP (SET_DEST (set), 0))))
+	    cost = 1;
+	}
       /* The only input for a call that is timing-critical is the
 	 function's address.  */
-      if (GET_CODE(insn) == CALL_INSN)
+      if (GET_CODE (insn) == CALL_INSN)
 	{
 	  rtx call = PATTERN (insn);
 
@@ -8606,12 +9159,16 @@
 		  /* sibcalli_thunk uses a symbol_ref in an unspec.  */
 	      && (GET_CODE (XEXP (XEXP (call, 0), 0)) == UNSPEC
 		  || ! reg_set_p (XEXP (XEXP (call, 0), 0), dep_insn)))
-	    cost = 0;
+	    cost -= TARGET_SH4_300 ? 3 : 6;
 	}
       /* Likewise, the most timing critical input for an sfuncs call
 	 is the function address.  However, sfuncs typically start
 	 using their arguments pretty quickly.
-	 Assume a four cycle delay before they are needed.  */
+	 Assume a four cycle delay for SH4 before they are needed.
+	 Cached ST40-300 calls are quicker, so assume only a one
+	 cycle delay there.
+	 ??? Maybe we should encode the delays till input registers
+	 are needed by sfuncs into the sfunc call insn.  */
       /* All sfunc calls are parallels with at least four components.
 	 Exploit this to avoid unnecessary calls to sfunc_uses_reg.  */
       else if (GET_CODE (PATTERN (insn)) == PARALLEL
@@ -8619,50 +9176,79 @@
 	       && (reg = sfunc_uses_reg (insn)))
 	{
 	  if (! reg_set_p (reg, dep_insn))
-	    cost -= 4;
+	    cost -= TARGET_SH4_300 ? 1 : 4;
 	}
-      /* When the preceding instruction loads the shift amount of
-	 the following SHAD/SHLD, the latency of the load is increased
-	 by 1 cycle.  */
-      else if (TARGET_SH4
-	       && get_attr_type (insn) == TYPE_DYN_SHIFT
-	       && get_attr_any_int_load (dep_insn) == ANY_INT_LOAD_YES
-	       && reg_overlap_mentioned_p (SET_DEST (single_set (dep_insn)),
-					   XEXP (SET_SRC (single_set (insn)),
+      if (TARGET_HARD_SH4 && !TARGET_SH4_300)
+	{
+	  enum attr_type dep_type = get_attr_type (dep_insn);
+
+	  if (dep_type == TYPE_FLOAD || dep_type == TYPE_PCFLOAD)
+	    cost--;
+	  else if ((dep_type == TYPE_LOAD_SI || dep_type == TYPE_PCLOAD_SI)
+		   && (type = get_attr_type (insn)) != TYPE_CALL
+		   && type != TYPE_SFUNC)
+	    cost--;
+	  /* When the preceding instruction loads the shift amount of
+	     the following SHAD/SHLD, the latency of the load is increased
+	     by 1 cycle.  */
+	  if (get_attr_type (insn) == TYPE_DYN_SHIFT
+	      && get_attr_any_int_load (dep_insn) == ANY_INT_LOAD_YES
+	      && reg_overlap_mentioned_p (SET_DEST (single_set (dep_insn)),
+					  XEXP (SET_SRC (single_set (insn)),
 						 1)))
-	cost++;
-      /* When an LS group instruction with a latency of less than
-	 3 cycles is followed by a double-precision floating-point
-	 instruction, FIPR, or FTRV, the latency of the first
-	 instruction is increased to 3 cycles.  */
-      else if (cost < 3
-	       && get_attr_insn_class (dep_insn) == INSN_CLASS_LS_GROUP
-	       && get_attr_dfp_comp (insn) == DFP_COMP_YES)
-	cost = 3;
-      /* The lsw register of a double-precision computation is ready one
-	 cycle earlier.  */
-      else if (reload_completed
-	       && get_attr_dfp_comp (dep_insn) == DFP_COMP_YES
-	       && (use_pat = single_set (insn))
-	       && ! regno_use_in (REGNO (SET_DEST (single_set (dep_insn))),
-				  SET_SRC (use_pat)))
-	cost -= 1;
-
-      if (get_attr_any_fp_comp (dep_insn) == ANY_FP_COMP_YES
-	  && get_attr_late_fp_use (insn) == LATE_FP_USE_YES)
-	cost -= 1;
+	    cost++;
+	  /* When an LS group instruction with a latency of less than
+	     3 cycles is followed by a double-precision floating-point
+	     instruction, FIPR, or FTRV, the latency of the first
+	     instruction is increased to 3 cycles.  */
+	  else if (cost < 3
+		   && get_attr_insn_class (dep_insn) == INSN_CLASS_LS_GROUP
+		   && get_attr_dfp_comp (insn) == DFP_COMP_YES)
+	    cost = 3;
+	  /* The lsw register of a double-precision computation is ready one
+	     cycle earlier.  */
+	  else if (reload_completed
+		   && get_attr_dfp_comp (dep_insn) == DFP_COMP_YES
+		   && (use_pat = single_set (insn))
+		   && ! regno_use_in (REGNO (SET_DEST (single_set (dep_insn))),
+				      SET_SRC (use_pat)))
+	    cost -= 1;
+	}
+      else if (TARGET_SH4_300)
+	{
+	  /* Stores need their input register two cycles later.  */
+	  if (dep_set && cost >= 1
+	      && ((type = get_attr_type (insn)) == TYPE_STORE
+		  || type == TYPE_PSTORE
+		  || type == TYPE_FSTORE || type == TYPE_MAC_MEM))
+	    {
+	      rtx set = single_set (insn);
+
+	      if (!reg_mentioned_p (SET_SRC (set), XEXP (SET_DEST (set), 0))
+		  && rtx_equal_p (SET_SRC (set), SET_DEST (dep_set)))
+		{
+		  cost -= 2;
+		  /* But don't reduce the cost below 1 if the address depends
+		     on a side effect of dep_insn.  */
+		  if (cost < 1
+		      && modified_in_p (XEXP (SET_DEST (set), 0), dep_insn))
+		    cost = 1;
+		}
+	    }
+	}
     }
   /* An anti-dependence penalty of two applies if the first insn is a double
      precision fadd / fsub / fmul.  */
-  else if (REG_NOTE_KIND (link) == REG_DEP_ANTI
+  else if (!TARGET_SH4_300
+	   && REG_NOTE_KIND (link) == REG_DEP_ANTI
 	   && recog_memoized (dep_insn) >= 0
-	   && get_attr_type (dep_insn) == TYPE_DFP_ARITH
+	   && (get_attr_type (dep_insn) == TYPE_DFP_ARITH
+	       || get_attr_type (dep_insn) == TYPE_DFP_MUL)
 	   /* A lot of alleged anti-flow dependences are fake,
 	      so check this one is real.  */
 	   && flow_dependent_p (dep_insn, insn))
     cost = 2;
 
-
   return cost;
 }
 
@@ -9245,7 +9831,8 @@
   emit_move_insn (adjust_address (tramp_mem, SImode, 12), fnaddr);
   if (TARGET_HARVARD)
     {
-      if (TARGET_USERMODE)
+      if (!TARGET_INLINE_IC_INVALIDATE
+	  || !(TARGET_SH4A_ARCH || TARGET_SH4_300) && TARGET_USERMODE)
 	emit_library_call (function_symbol (NULL, "__ic_invalidate",
 					    FUNCTION_ORDINARY),
 			   0, VOIDmode, 1, tramp, SImode);
@@ -10009,11 +10596,10 @@
 {
   rtx sym;
 
-  /* If this is not an ordinary function, the name usually comes from a
-     string literal or an sprintf buffer.  Make sure we use the same
+  /* The name usually comes from a string literal or an sprintf buffer.
+     Make sure we use the same
      string consistently, so that cse will be able to unify address loads.  */
-  if (kind != FUNCTION_ORDINARY)
-    name = IDENTIFIER_POINTER (get_identifier (name));
+  name = IDENTIFIER_POINTER (get_identifier (name));
   sym = gen_rtx_SYMBOL_REF (Pmode, name);
   SYMBOL_REF_FLAGS (sym) = SYMBOL_FLAG_FUNCTION;
   if (flag_pic)
@@ -10021,6 +10607,10 @@
       {
       case FUNCTION_ORDINARY:
 	break;
+      case SFUNC_FREQUENT:
+	if (!optimize || optimize_size)
+	  break;
+	/* Fall through.  */
       case SFUNC_GOT:
 	{
 	  rtx reg = target ? target : gen_reg_rtx (Pmode);
@@ -10123,6 +10713,141 @@
   return 1;
 }
 
+void
+sh_expand_float_cbranch (rtx operands[4])
+{
+  static rtx (*branches[]) (rtx) = { gen_branch_true, gen_branch_false };
+
+  sh_expand_float_condop (operands, branches);
+}
+
+void
+sh_expand_float_scc (rtx operands[4])
+{
+  static rtx (*movts[]) (rtx) = { gen_movt, gen_movnegt };
+
+  operands[3] = NULL_RTX;
+  sh_expand_float_condop (operands, movts);
+}
+
+/* The first element of USER is for positive logic, the second one for
+   negative logic.  */
+static void
+sh_expand_float_condop (rtx operands[4], rtx (*user[2]) (rtx))
+{
+  enum machine_mode mode = GET_MODE (operands[1]);
+  enum rtx_code comparison = GET_CODE (operands[0]);
+  int swap_operands = 0;
+
+  if (TARGET_SH1_SOFTFP_MODE (mode))
+    {
+      switch (comparison)
+	{
+	case NE:
+	  comparison = EQ;
+	  user++;
+	  break;
+	case LT:
+	  swap_operands = 1;	/* Fall through.  */
+	case GT:
+	  comparison = UNLE;
+	  user++;
+	  break;
+	case UNGT:
+	  swap_operands = 1;	/* Fall through.  */
+	case UNLT:
+	  comparison = GE;
+	  user++;
+	  break;
+	case UNGE:
+	  swap_operands = 1;
+	  comparison = UNLE;
+	  break;
+	case LE:
+	  swap_operands = 1;
+	  comparison = GE;	/* Fall through.  */
+	case EQ:
+	case UNEQ:
+	case GE:
+	case UNLE:
+	case UNORDERED:
+	  break;
+	case LTGT:
+	  comparison = UNEQ;
+	  user++;
+	  break;
+	case ORDERED:
+	  comparison = UNORDERED;
+	  user++;
+	  break;
+	  
+	default: gcc_unreachable ();
+	}
+    }
+  else /* SH2E .. SH4 Hardware floating point */
+    {
+      switch (comparison)
+	{
+	case NE:
+	  comparison = EQ;
+	  user++;
+	  break;
+	case LT:
+	  swap_operands = 1;	/* Fall through.  */
+	  comparison = GT;
+	case GT:
+	case EQ:
+	case LTGT:
+	case ORDERED:
+	  break;
+	case LE:
+	  if (flag_finite_math_only)
+	    {
+	      comparison = GT;
+	      user++;
+	      break;
+	    }
+	  swap_operands = 1;
+	  comparison = GE;	/* Fall through.  */
+	case GE:
+	  if (flag_finite_math_only)
+	    {
+	      swap_operands = 1;
+	      comparison = GT;
+	      user++;
+	      break;
+	    }
+	  break;
+	case UNGE:
+	  swap_operands = 1;	/* Fall through.  */
+	case UNLE:
+	  comparison = GT;
+	  user++;
+	  break;
+	case UNGT:
+	  swap_operands = 1;	/* Fall through.  */
+	case UNLT:
+	  comparison = GE;
+	  user++;
+	  break;
+	case UNEQ:
+	  comparison = LTGT;
+	  user++;
+	  break;
+	case UNORDERED:
+	  comparison = ORDERED;
+	  user++;
+	  break;
+
+	default: gcc_unreachable ();
+	}
+    }
+  sh_compare_op0 = operands[1+swap_operands];
+  sh_compare_op1 = operands[2-swap_operands];
+  from_compare (&operands[3], comparison);
+  emit_jump_insn ((*user) (operands[3]));
+}
+
 /* INSN is an sfunc; return the rtx that describes the address used.  */
 static rtx
 extract_sfunc_addr (rtx insn)
@@ -10792,6 +11517,20 @@
   return for_each_rtx (&PATTERN (insn), &sh_contains_memref_p_1, NULL);
 }
 
+/* Return nonzero iff INSN loads a banked register.  */
+int
+sh_loads_bankedreg_p (rtx insn)
+{
+  if (GET_CODE (PATTERN (insn)) == SET)
+    {
+      rtx op = SET_DEST (PATTERN(insn));
+      if (REG_P (op) && BANKED_REGISTER_P (REGNO (op)))
+	return 1;
+    }
+
+  return 0;  
+}
+
 /* FNADDR is the MEM expression from a call expander.  Return an address
    to use in an SHmedia insn pattern.  */
 rtx
@@ -10940,6 +11679,19 @@
   return NO_REGS;
 }
 
+int
+sh_match_adjust (rtx x, int regno)
+{
+  /* On a WORDS_BIG_ENDIAN machine, point to the last register of a
+     multiple hard register group of scalar integer registers, so that
+     for example (reg:DI 0) and (reg:SI 1) will be considered the same
+     register.  */
+  if (WORDS_BIG_ENDIAN && GET_MODE_SIZE (GET_MODE (x)) > UNITS_PER_WORD
+      && regno < FIRST_PSEUDO_REGISTER)
+    regno += hard_regno_nregs[regno][GET_MODE (x)] - 1;
+  return regno;
+}
+
 enum sh_divide_strategy_e sh_div_strategy = SH_DIV_STRATEGY_DEFAULT;
 
 #include "gt-sh.h"
diff -Naur gcc-4.2.1/gcc/config/sh/sh.h gcc-4.2.1-stm/gcc/config/sh/sh.h
--- gcc-4.2.1/gcc/config/sh/sh.h	2006-10-06 15:44:18.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/sh.h	2007-08-10 13:08:59.000000000 +0100
@@ -3,6 +3,7 @@
    2003, 2004, 2005, 2006 Free Software Foundation, Inc.
    Contributed by Steve Chamberlain (sac@cygnus.com).
    Improved by Jim Wilson (wilson@cygnus.com).
+   Copyright (c) 2006  STMicroelectronics.
 
 This file is part of GCC.
 
@@ -104,6 +105,10 @@
     if (! VALID_REGISTER_P (regno))					\
       fixed_regs[regno] = call_used_regs[regno] = 1;			\
   /* R8 and R9 are call-clobbered on SH5, but not on earlier SH ABIs.  */ \
+  if (TARGET_SH4A_FP || TARGET_SH4_300)					\
+    {                                                                   \
+      global_regs[FPSCR_REG] = 1;                                       \
+    }									\
   if (TARGET_SH5)							\
     {									\
       call_used_regs[FIRST_GENERAL_REG + 8]				\
@@ -117,6 +122,10 @@
       CLEAR_HARD_REG_SET (reg_class_contents[FP0_REGS]);		\
       regno_reg_class[FIRST_FP_REG] = FP_REGS;				\
     }									\
+  if (TARGET_R0R3_TO_REG_MUL < 2)					\
+    regno_reg_class[R1_REG] = regno_reg_class[R2_REG]			\
+      = regno_reg_class[R3_REG] = GENERAL_REGS;				\
+    /* The peephole2s needs reg_class_contents[R0R3_REGS].  */		\
   if (flag_pic)								\
     {									\
       fixed_regs[PIC_OFFSET_TABLE_REGNUM] = 1;				\
@@ -172,6 +181,11 @@
 #define TARGET_FPU_DOUBLE \
   ((target_flags & MASK_SH4) != 0 || TARGET_SH2A_DOUBLE)
 
+#define TARGET_SH1_SOFTFP (TARGET_SH1 && !TARGET_FPU_DOUBLE)
+
+#define TARGET_SH1_SOFTFP_MODE(MODE) \
+  (TARGET_SH1_SOFTFP && (!TARGET_SH2E || (MODE) == DFmode))
+
 /* Nonzero if an FPU is available.  */
 #define TARGET_FPU_ANY (TARGET_SH2E || TARGET_FPU_DOUBLE)
 
@@ -269,11 +283,48 @@
 #define SELECT_SH5_COMPACT       (MASK_SH5 | MASK_SH4 | SELECT_SH3E)
 #define SELECT_SH5_COMPACT_NOFPU (MASK_SH5 | SELECT_SH3)
 
+/* Check if we have support for optimized software floating point using
+   dynamic shifts - then some function calls clobber fewer registers.  */
+#ifdef SUPPORT_SH3
+#define SUPPORT_SH3_OSFP 1
+#else
+#define SUPPORT_SH3_OSFP 0
+#endif
+
+#ifdef SUPPORT_SH3E
+#define SUPPORT_SH3E_OSFP 1
+#else
+#define SUPPORT_SH3E_OSFP 0
+#endif
+
+#if defined(SUPPORT_SH4_NOFPU) || defined(SUPPORT_SH3_OSFP)
+#define SUPPORT_SH4_NOFPU_OSFP 1
+#else
+#define SUPPORT_SH4_NOFPU_OSFP 0
+#endif
+
+#if defined(SUPPORT_SH4_SINGLE_ONLY) || defined (SUPPORT_SH3E_OSFP)
+#define SUPPORT_SH4_SINGLE_ONLY_OSFP 1
+#else
+#define SUPPORT_SH4_SINGLE_ONLY_OSFP 0
+#endif
+
+#ifdef notyet
+#define TARGET_OSFP (0 \
+ || (TARGET_SH3 && !TARGET_SH2E && SUPPORT_SH3_OSFP) \
+ || (TARGET_SH3E && SUPPORT_SH3E_OSFP) \
+ || (TARGET_HARD_SH4 && !TARGET_SH2E && SUPPORT_SH4_NOFPU_OSFP) \
+ || (TARGET_HARD_SH4 && TARGET_SH2E && SUPPORT_SH4_SINGLE_ONLY_OSFP))
+#else
+#define TARGET_OSFP (0)
+#endif
+
 #if SUPPORT_SH1
 #define SUPPORT_SH2 1
 #endif
 #if SUPPORT_SH2
 #define SUPPORT_SH3 1
+#define SUPPORT_SH2A_NOFPU 1
 #endif
 #if SUPPORT_SH3
 #define SUPPORT_SH4_NOFPU 1
@@ -281,16 +332,17 @@
 #if SUPPORT_SH4_NOFPU
 #define SUPPORT_SH4A_NOFPU 1
 #define SUPPORT_SH4AL 1
-#define SUPPORT_SH2A_NOFPU 1
 #endif
 
 #if SUPPORT_SH2E
 #define SUPPORT_SH3E 1
+#define SUPPORT_SH2A_SINGLE_ONLY 1
 #endif
 #if SUPPORT_SH3E
 #define SUPPORT_SH4_SINGLE_ONLY 1
+#endif
+#if SUPPORT_SH4_SINGLE_ONLY
 #define SUPPORT_SH4A_SINGLE_ONLY 1
-#define SUPPORT_SH2A_SINGLE_ONLY 1
 #endif
 
 #if SUPPORT_SH4
@@ -458,6 +510,8 @@
 #define DRIVER_SELF_SPECS "%{m2a:%{ml:%eSH2a does not support little-endian}}"
 #define OPTIMIZATION_OPTIONS(LEVEL,SIZE)				\
 do {									\
+  TARGET_CBRANCHDI4 = 1;						\
+  TARGET_EXPAND_CBRANCHDI4 = 1;						\
   if (LEVEL)								\
     {									\
       flag_omit_frame_pointer = -1;					\
@@ -488,6 +542,8 @@
      the user explicitly requested this to be on or off.  */		\
   if (flag_schedule_insns > 0)						\
     flag_schedule_insns = 2;						\
+  if (flag_trapping_math > 0) /* Likewise for flag_trapping_math.  */	\
+    flag_trapping_math = 2;						\
 } while (0)
 
 #define ASSEMBLER_DIALECT assembler_dialect
@@ -528,10 +584,15 @@
   if (flag_finite_math_only == 2)					\
     flag_finite_math_only						\
       = !flag_signaling_nans && TARGET_SH2E && ! TARGET_IEEE;		\
+  if (flag_trapping_math == 2)						\
+    flag_trapping_math = (!TARGET_SH2E && TARGET_OSFP);			\
   if (TARGET_SH2E && !flag_finite_math_only)				\
     target_flags |= MASK_IEEE;						\
   sh_cpu = CPU_SH1;							\
-  assembler_dialect = 0;						\
+  if (TARGET_SH1)							\
+    assembler_dialect = 1;						\
+  else                                                                  \
+    assembler_dialect = 0;						\
   if (TARGET_SH2)							\
     sh_cpu = CPU_SH2;							\
   if (TARGET_SH2E)							\
@@ -608,6 +669,7 @@
 	      else							\
 		sh_div_strategy = SH_DIV_INV;				\
 	    }								\
+	  TARGET_CBRANCHDI4 = 0;					\
 	}								\
       /* -fprofile-arcs needs a working libgcov .  In unified tree	\
 	 configurations with newlib, this requires to configure with	\
@@ -622,6 +684,8 @@
        targetm.asm_out.aligned_op.di = NULL;				\
        targetm.asm_out.unaligned_op.di = NULL;				\
     }									\
+  if (!TARGET_SH1)							\
+    TARGET_PRETEND_CMOVE = 0;						\
   if (TARGET_SH1)							\
     {									\
       if (! strcmp (sh_div_str, "call-div1"))				\
@@ -651,13 +715,9 @@
 	  sh_div_strategy = SH_DIV_CALL_FP;				\
         /* SH1 .. SH3 cores often go into small-footprint systems, so	\
 	   default to the smallest implementation available.  */	\
-	else if (TARGET_SH2)	/* ??? EXPERIMENTAL */			\
-	  sh_div_strategy = SH_DIV_CALL_TABLE;				\
 	else								\
 	  sh_div_strategy = SH_DIV_CALL_DIV1;				\
     }									\
-  if (!TARGET_SH1)							\
-    TARGET_PRETEND_CMOVE = 0;						\
   if (sh_divsi3_libfunc[0])						\
     ; /* User supplied - leave it alone.  */				\
   else if (TARGET_DIVIDE_CALL_FP)					\
@@ -668,6 +728,11 @@
     sh_divsi3_libfunc = "__sdivsi3_1";					\
   else									\
     sh_divsi3_libfunc = "__sdivsi3";					\
+  if (sh_branch_cost == -1)						\
+    sh_branch_cost							\
+      = TARGET_SH5 ? 1 : ! TARGET_SH2 || TARGET_HARD_SH4 ? 2 : 1;	\
+  if (TARGET_R0R3_TO_REG_MUL == -1)					\
+    TARGET_R0R3_TO_REG_MUL = /* TARGET_SH4_300 ? 2 : */ 0;		\
   if (TARGET_FMOVD)							\
     reg_class_from_letter['e' - 'a'] = NO_REGS;				\
 									\
@@ -745,6 +810,8 @@
       if (align_functions < min_align)					\
 	align_functions = min_align;					\
     }									\
+  if (!TARGET_SH2)							\
+      TARGET_EXPAND_CBRANCHDI4 = 1;					\
 } while (0)
 
 /* Target machine storage layout.  */
@@ -844,7 +911,7 @@
   ((GET_MODE_CLASS (TYPE_MODE (TYPE)) == MODE_COMPLEX_INT \
     || GET_MODE_CLASS (TYPE_MODE (TYPE)) == MODE_COMPLEX_FLOAT) \
    ? (unsigned) MIN (BIGGEST_ALIGNMENT, GET_MODE_BITSIZE (TYPE_MODE (TYPE))) \
-   : (unsigned) ALIGN)
+   : (unsigned) DATA_ALIGNMENT(TYPE, ALIGN))
 
 /* Make arrays of chars word-aligned for the same reasons.  */
 #define DATA_ALIGNMENT(TYPE, ALIGN)		\
@@ -865,6 +932,9 @@
 #define LABEL_ALIGN_AFTER_BARRIER(LABEL_AFTER_BARRIER) \
   barrier_align (LABEL_AFTER_BARRIER)
 
+#define JUMP_ALIGN(LABEL) \
+   sh_jump_align (LABEL)
+
 #define LOOP_ALIGN(A_LABEL) \
   ((! optimize || TARGET_HARD_SH4 || TARGET_SMALLCODE) \
    ? 0 : sh_loop_align (A_LABEL))
@@ -1017,6 +1087,16 @@
 #define FIRST_TARGET_REG TR0_REG
 #define LAST_TARGET_REG  (FIRST_TARGET_REG + (TARGET_SHMEDIA ? 7 : -1))
 
+/* Registers that can be accessed through bank0 or bank1 depending on sr.md.  */
+
+#define FIRST_BANKED_REG R0_REG
+#define LAST_BANKED_REG R7_REG
+
+#define BANKED_REGISTER_P(REGNO)                       \
+  IN_RANGE ((REGNO),                                   \
+	    (unsigned HOST_WIDE_INT) FIRST_BANKED_REG, \
+	    (unsigned HOST_WIDE_INT) LAST_BANKED_REG)
+
 #define GENERAL_REGISTER_P(REGNO) \
   IN_RANGE ((REGNO), \
 	    (unsigned HOST_WIDE_INT) FIRST_GENERAL_REG, \
@@ -1383,6 +1463,7 @@
 {
   NO_REGS,
   R0_REGS,
+  R0R3_REGS,
   PR_REGS,
   T_REGS,
   MAC_REGS,
@@ -1408,6 +1489,7 @@
 {			\
   "NO_REGS",		\
   "R0_REGS",		\
+  "R0R3_REGS",		\
   "PR_REGS",		\
   "T_REGS",		\
   "MAC_REGS",		\
@@ -1435,6 +1517,8 @@
   { 0x00000000, 0x00000000, 0x00000000, 0x00000000, 0x00000000 },	\
 /* R0_REGS:  */								\
   { 0x00000001, 0x00000000, 0x00000000, 0x00000000, 0x00000000 },	\
+/* R0R3_REGS:  */							\
+  { 0x0000000f, 0x00000000, 0x00000000, 0x00000000, 0x00000000 },	\
 /* PR_REGS:  */								\
   { 0x00000000, 0x00000000, 0x00000000, 0x00000000, 0x00040000 },	\
 /* T_REGS:  */								\
@@ -1525,9 +1609,11 @@
    description.  */
 extern enum reg_class reg_class_from_letter[];
 
-/* We might use 'Rxx' constraints in the future for exotic reg classes.*/
+/* Use 'Rxx' constraints for exotic reg classes.  */
 #define REG_CLASS_FROM_CONSTRAINT(C, STR) \
-  (ISLOWER (C) ? reg_class_from_letter[(C)-'a'] : NO_REGS )
+  (ISLOWER (C) ? reg_class_from_letter[(C)-'a'] \
+   : (C) == 'R' && (STR)[1] == '0' && (STR)[2] == '3' ? R0R3_REGS \
+   : NO_REGS )
 
 /* Overview of uppercase letter constraints:
    A: Addresses (constraint len == 3)
@@ -2288,6 +2374,7 @@
 #define CONSTANT_ADDRESS_P(X)	(GET_CODE (X) == LABEL_REF)
 
 /* Nonzero if the constant value X is a legitimate general operand.  */
+/* can_store_by_pieces constructs VOIDmode CONST_DOUBLEs.  */
 
 #define LEGITIMATE_CONSTANT_P(X) \
   (TARGET_SHMEDIA							\
@@ -2298,7 +2385,7 @@
       || TARGET_SHMEDIA64)						\
    : (GET_CODE (X) != CONST_DOUBLE					\
       || GET_MODE (X) == DFmode || GET_MODE (X) == SFmode		\
-      || (TARGET_SH2E && (fp_zero_operand (X) || fp_one_operand (X)))))
+      || GET_MODE (X) == DImode || GET_MODE (X) == VOIDmode))
 
 /* The macros REG_OK_FOR..._P assume that the arg is a REG rtx
    and check its validity for a certain class.
@@ -2851,7 +2938,7 @@
 
 /* Max number of bytes we want move_by_pieces to be able to copy
    efficiently.  */
-#define MOVE_MAX_PIECES (TARGET_SH4 || TARGET_SHMEDIA ? 8 : 4)
+#define MOVE_MAX_PIECES (TARGET_SH1 || TARGET_SH5 ? 8 : 4)
 
 /* Define if operations between registers always perform the operation
    on the full register even if a narrower mode is specified.  */
@@ -3406,8 +3493,10 @@
 #define MODE_PRIORITY_TO_MODE(ENTITY, N) \
   ((TARGET_FPU_SINGLE != 0) ^ (N) ? FP_MODE_SINGLE : FP_MODE_DOUBLE)
 
-#define EMIT_MODE_SET(ENTITY, MODE, HARD_REGS_LIVE) \
-  fpscr_set_from_mem ((MODE), (HARD_REGS_LIVE))
+#define EMIT_MODE_SET(ENTITY, MODE, FLIP, HARD_REGS_LIVE) \
+  ((TARGET_SH4A_FP || TARGET_SH4_300)                     \
+   && (FLIP) ? emit_fpu_flip ()                           \
+   : fpscr_set_from_mem ((MODE), (HARD_REGS_LIVE)))
 
 #define MD_CAN_REDIRECT_BRANCH(INSN, SEQ) \
   sh_can_redirect_branch ((INSN), (SEQ))
diff -Naur gcc-4.2.1/gcc/config/sh/sh.md gcc-4.2.1-stm/gcc/config/sh/sh.md
--- gcc-4.2.1/gcc/config/sh/sh.md	2007-07-04 05:57:59.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/sh.md	2007-08-10 13:08:59.000000000 +0100
@@ -3,6 +3,7 @@
 ;;  2003, 2004, 2005, 2006 Free Software Foundation, Inc.
 ;;  Contributed by Steve Chamberlain (sac@cygnus.com).
 ;;  Improved by Jim Wilson (wilson@cygnus.com).
+;;  Copyright (c) 2006  STMicroelectronics.
 
 ;; This file is part of GCC.
 
@@ -47,6 +48,8 @@
 ;;    l -- pr
 ;;    z -- r0
 ;;
+;;    R03 -- r0, r1, r2 or r3  - experimental constraint for SH4-300
+;;
 ;; Special formats used for outputting SH instructions:
 ;;
 ;;   %.  --  print a .s if insn needs delay slot
@@ -107,6 +110,7 @@
   (DR0_REG	64)
   (DR2_REG	66)
   (DR4_REG	68)
+  (FR4_REG	68)
   (FR23_REG	87)
 
   (TR0_REG	128)
@@ -162,6 +166,16 @@
   (UNSPECV_CONST8	6)
   (UNSPECV_WINDOW_END	10)
   (UNSPECV_CONST_END	11)
+
+  ;; NaN handling for software floating point:
+  ;; We require one bit specific for a precision to be set in all NaNs,
+  ;; so that we can test them with a not / tst sequence.
+  ;; ??? Ironically, this is the quiet bit for now, because that is the
+  ;; only bit set by __builtin_nan ("").
+  ;; ??? Should really use one bit lower and force it set by using
+  ;; a custom encoding function.
+  (SF_NAN_MASK		0x7fc00000)
+  (DF_NAN_MASK		0x7ff80000)
 ])
 
 ;; -------------------------------------------------------------------------
@@ -203,7 +217,9 @@
 ;; load_si	Likewise, SImode variant for general register.
 ;; fload	Likewise, but load to fp register.
 ;; store	to memory
+;; fstore	floating point register to memory
 ;; move		general purpose register to register
+;; movi8	8 bit immediate to general purpose register
 ;; mt_group	other sh4 mt instructions
 ;; fmove	register to register, floating point
 ;; smpy		word precision integer multiply
@@ -220,11 +236,15 @@
 ;; sfunc	special function call with known used registers
 ;; call		function call
 ;; fp		floating point
+;; fpscr_toggle	toggle a bit in the fpscr
 ;; fdiv		floating point divide (or square root)
 ;; gp_fpul	move from general purpose register to fpul
 ;; fpul_gp	move from fpul to general purpose register
 ;; mac_gp	move from mac[lh] to general purpose register
-;; dfp_arith, dfp_cmp,dfp_conv
+;; gp_mac	move from general purpose register to mac[lh]
+;; mac_mem	move from mac[lh] to memory
+;; mem_mac	move from memory to mac[lh]
+;; dfp_arith,dfp_mul, fp_cmp,dfp_cmp,dfp_conv
 ;; ftrc_s	fix_truncsfsi2_i4
 ;; dfdiv	double precision floating point divide (or square root)
 ;; cwb		ic_invalidate_line_i
@@ -262,7 +282,7 @@
 ;; nil		no-op move, will be deleted.
 
 (define_attr "type"
- "mt_group,cbranch,jump,jump_ind,arith,arith3,arith3b,dyn_shift,load,load_si,fload,store,move,fmove,smpy,dmpy,return,pload,prset,pstore,prget,pcload,pcload_si,pcfload,rte,sfunc,call,fp,fdiv,ftrc_s,dfp_arith,dfp_cmp,dfp_conv,dfdiv,gp_fpul,fpul_gp,mac_gp,mem_fpscr,gp_fpscr,cwb,movua,fsrra,fsca,tls_load,arith_media,cbranch_media,cmp_media,dfdiv_media,dfmul_media,dfparith_media,dfpconv_media,dmpy_media,fcmp_media,fdiv_media,fload_media,fmove_media,fparith_media,fpconv_media,fstore_media,gettr_media,invalidate_line_media,jump_media,load_media,pt_media,ptabs_media,store_media,mcmp_media,mac_media,d2mpy_media,atrans_media,ustore_media,nil,other"
+ "mt_group,cbranch,jump,jump_ind,arith,arith3,arith3b,dyn_shift,load,load_si,fload,store,fstore,move,movi8,fmove,smpy,dmpy,return,pload,prset,pstore,prget,pcload,pcload_si,pcfload,rte,sfunc,call,fp,fpscr_toggle,fdiv,ftrc_s,dfp_arith,dfp_mul,fp_cmp,dfp_cmp,dfp_conv,dfdiv,gp_fpul,fpul_gp,mac_gp,gp_mac,mac_mem,mem_mac,mem_fpscr,gp_fpscr,cwb,movua,fsrra,fsca,tls_load,arith_media,cbranch_media,cmp_media,dfdiv_media,dfmul_media,dfparith_media,dfpconv_media,dmpy_media,fcmp_media,fdiv_media,fload_media,fmove_media,fparith_media,fpconv_media,fstore_media,gettr_media,invalidate_line_media,jump_media,load_media,pt_media,ptabs_media,store_media,mcmp_media,mac_media,d2mpy_media,atrans_media,ustore_media,nil,other"
   (const_string "other"))
 
 ;; We define a new attribute namely "insn_class".We use
@@ -278,12 +298,12 @@
 (define_attr "insn_class"
   "mt_group,ex_group,ls_group,br_group,fe_group,co_group,none"
   (cond [(eq_attr "type" "move,mt_group") (const_string "mt_group")
-         (eq_attr "type" "arith,dyn_shift") (const_string "ex_group")
-	 (eq_attr "type" "fmove,load,pcload,load_si,pcload_si,fload,pcfload,store,gp_fpul,fpul_gp") (const_string "ls_group")
+         (eq_attr "type" "movi8,arith,dyn_shift") (const_string "ex_group")
+	 (eq_attr "type" "fmove,load,pcload,load_si,pcload_si,fload,pcfload,store,fstore,gp_fpul,fpul_gp") (const_string "ls_group")
 	 (eq_attr "type" "cbranch,jump") (const_string "br_group")
-	 (eq_attr "type" "fp,fdiv,ftrc_s,dfp_arith,dfp_conv,dfdiv")
+	 (eq_attr "type" "fp,fp_cmp,fdiv,ftrc_s,dfp_arith,dfp_mul,dfp_conv,dfdiv")
 	   (const_string "fe_group")
-	 (eq_attr "type" "jump_ind,smpy,dmpy,mac_gp,return,pload,prset,pstore,prget,rte,sfunc,call,dfp_cmp,mem_fpscr,gp_fpscr,cwb") (const_string "co_group")]
+	 (eq_attr "type" "jump_ind,smpy,dmpy,mac_gp,return,pload,prset,pstore,prget,rte,sfunc,call,dfp_cmp,mem_fpscr,gp_fpscr,cwb,gp_mac,mac_mem,mem_mac") (const_string "co_group")]
 	(const_string "none")))
 ;; nil are zero instructions, and arith3 / arith3b are multiple instructions,
 ;; so these do not belong in an insn group, although they are modeled
@@ -455,6 +475,12 @@
 
 (define_attr "needs_delay_slot" "yes,no" (const_string "no"))
 
+(define_attr "banked" "yes,no" 
+	(cond [(eq (symbol_ref "sh_loads_bankedreg_p (insn)")
+		   (const_int 1))
+	       (const_string "yes")]
+	      (const_string "no")))
+
 ;; ??? This should be (nil) instead of (const_int 0)
 (define_attr "hit_stack" "yes,no"
 	(cond [(eq (symbol_ref "find_regno_note (insn, REG_INC, SP_REG)")
@@ -495,14 +521,14 @@
 ;; SH4 Double-precision computation with double-precision result -
 ;; the two halves are ready at different times.
 (define_attr "dfp_comp" "yes,no"
-  (cond [(eq_attr "type" "dfp_arith,dfp_conv,dfdiv") (const_string "yes")]
+  (cond [(eq_attr "type" "dfp_arith,dfp_mul,dfp_conv,dfdiv") (const_string "yes")]
 	(const_string "no")))
 
 ;; Insns for which the latency of a preceding fp insn is decreased by one.
 (define_attr "late_fp_use" "yes,no" (const_string "no"))
 ;; And feeding insns for which this relevant.
 (define_attr "any_fp_comp" "yes,no"
-  (cond [(eq_attr "type" "fp,fdiv,ftrc_s,dfp_arith,dfp_conv,dfdiv")
+  (cond [(eq_attr "type" "fp,fdiv,ftrc_s,dfp_arith,dfp_mul,dfp_conv,dfdiv")
 	 (const_string "yes")]
 	(const_string "no")))
 
@@ -533,8 +559,9 @@
 		  (eq_attr "type" "!pload,prset"))
 	     (and (eq_attr "interrupt_function" "yes")
 		  (ior
-		   (ne (symbol_ref "TARGET_SH3") (const_int 0))
-		   (eq_attr "hit_stack" "no"))))) (nil) (nil)])
+		   (eq (symbol_ref "TARGET_SH3") (const_int 0))
+		   (eq_attr "hit_stack" "no")
+		   (eq_attr "banked" "no"))))) (nil) (nil)])
 
 ;; Since a call implicitly uses the PR register, we can't allow
 ;; a PR register store in a jsr delay slot.
@@ -589,6 +616,14 @@
 	cmp/eq	%1,%0"
    [(set_attr "type" "mt_group")])
 
+(define_insn "fpcmp_i1"
+  [(set (reg:SI T_REG)
+	(match_operator:SI 1 "soft_fp_comparison_operator"
+	  [(match_operand 0 "soft_fp_comparison_operand" "r") (const_int 0)]))]
+  "TARGET_SH1_SOFTFP"
+  "tst	%0,%0"
+   [(set_attr "type" "mt_group")])
+
 (define_insn "cmpgtsi_t"
   [(set (reg:SI T_REG)
 	(gt:SI (match_operand:SI 0 "arith_reg_operand" "r,r")
@@ -610,15 +645,37 @@
    [(set_attr "type" "mt_group")])
 
 ;; -------------------------------------------------------------------------
+;; SImode compare and branch
+;; -------------------------------------------------------------------------
+
+(define_expand "cbranchsi4"
+  [(set (pc)
+	(if_then_else (match_operator 0 "comparison_operator"
+			[(match_operand:SI 1 "arith_operand" "")
+			 (match_operand:SI 2 "arith_operand" "")])
+		      (label_ref (match_operand 3 "" ""))
+		      (pc)))
+   (clobber (reg:SI T_REG))]
+  "TARGET_SH1"
+  "expand_cbranchsi4 (operands, CODE_FOR_nothing, -1); DONE;")
+
+;; -------------------------------------------------------------------------
 ;; SImode unsigned integer comparisons
 ;; -------------------------------------------------------------------------
 
-(define_insn "cmpgeusi_t"
+(define_insn_and_split "cmpgeusi_t"
   [(set (reg:SI T_REG)
 	(geu:SI (match_operand:SI 0 "arith_reg_operand" "r")
-		(match_operand:SI 1 "arith_reg_operand" "r")))]
+		(match_operand:SI 1 "arith_reg_or_0_operand" "rN")))]
   "TARGET_SH1"
   "cmp/hs	%1,%0"
+  "&& operands[0] == CONST0_RTX (SImode)"
+  [(pc)]
+  "
+{
+  emit_insn (gen_sett ());
+  DONE;
+}"
    [(set_attr "type" "mt_group")])
 
 (define_insn "cmpgtusi_t"
@@ -648,12 +705,64 @@
 }")
 
 ;; -------------------------------------------------------------------------
-;; DImode signed integer comparisons
+;; DImode compare and branch
 ;; -------------------------------------------------------------------------
 
-;; ??? Could get better scheduling by splitting the initial test from the
-;; rest of the insn after reload.  However, the gain would hardly justify
-;; the sh.md size increase necessary to do that.
+
+;; arith3 patterns don't work well with the sh4-300 branch prediction mechanism.
+;; Therefore, we aim to have a set of three branches that go straight to the
+;; destination, i.e. only one of them is taken at any one time.
+;; This mechanism should also be slightly better for the sh4-200.
+
+(define_expand "cbranchdi4"
+  [(parallel [(set (pc)
+		   (if_then_else (match_operator 0 "comparison_operator"
+				   [(match_operand:DI 1 "arith_operand" "")
+				    (match_operand:DI 2 "arith_operand" "")])
+				 (label_ref (match_operand 3 "" ""))
+				 (pc)))
+	      (clobber (match_dup 4))
+	      (clobber (reg:SI T_REG))])]
+  "TARGET_CBRANCHDI4"
+  "
+{
+  enum rtx_code comparison;
+
+  if (TARGET_EXPAND_CBRANCHDI4)
+    {
+      if (expand_cbranchdi4 (operands, CODE_FOR_nothing))
+	DONE;
+    }
+  comparison = prepare_cbranch_operands (operands, DImode, CODE_FOR_nothing);
+  if (comparison != GET_CODE (operands[0]))
+    operands[0]
+      = gen_rtx_fmt_ee (VOIDmode, comparison, operands[1], operands[2]);
+   operands[4] = gen_rtx_SCRATCH (SImode);
+}")
+
+(define_insn_and_split "cbranchdi4_i"
+  [(set (pc)
+	(if_then_else (match_operator 0 "comparison_operator"
+			[(match_operand:DI 1 "arith_operand" "r,r")
+			 (match_operand:DI 2 "arith_operand" "rN,i")])
+		      (label_ref (match_operand 3 "" ""))
+		      (pc)))
+   (clobber (match_scratch:SI 4 "=X,&r"))
+   (clobber (reg:SI T_REG))]
+  "TARGET_CBRANCHDI4"
+  "#"
+  "&& reload_completed"
+  [(pc)]
+  "
+{
+  if (!expand_cbranchdi4 (operands, GET_CODE (operands[0])))
+    FAIL;
+  DONE;
+}")
+
+;; -------------------------------------------------------------------------
+;; DImode signed integer comparisons
+;; -------------------------------------------------------------------------
 
 (define_insn ""
   [(set (reg:SI T_REG)
@@ -1132,9 +1241,9 @@
 
 (define_insn "*movsicc_t_false"
   [(set (match_operand:SI 0 "arith_reg_dest" "=r,r")
-	(if_then_else (eq (reg:SI T_REG) (const_int 0))
-		      (match_operand:SI 1 "general_movsrc_operand" "r,I08")
-		      (match_operand:SI 2 "arith_reg_operand" "0,0")))]
+	(if_then_else:SI (eq (reg:SI T_REG) (const_int 0))
+			 (match_operand:SI 1 "general_movsrc_operand" "r,I08")
+			 (match_operand:SI 2 "arith_reg_operand" "0,0")))]
   "TARGET_PRETEND_CMOVE
    && (arith_reg_operand (operands[1], SImode)
        || (immediate_operand (operands[1], SImode)
@@ -1145,9 +1254,9 @@
 
 (define_insn "*movsicc_t_true"
   [(set (match_operand:SI 0 "arith_reg_dest" "=r,r")
-	(if_then_else (ne (reg:SI T_REG) (const_int 0))
-		      (match_operand:SI 1 "general_movsrc_operand" "r,I08")
-		      (match_operand:SI 2 "arith_reg_operand" "0,0")))]
+	(if_then_else:SI (ne (reg:SI T_REG) (const_int 0))
+			 (match_operand:SI 1 "general_movsrc_operand" "r,I08")
+			 (match_operand:SI 2 "arith_reg_operand" "0,0")))]
   "TARGET_PRETEND_CMOVE
    && (arith_reg_operand (operands[1], SImode)
        || (immediate_operand (operands[1], SImode)
@@ -2756,6 +2865,64 @@
   "mul.l	%1,%0"
   [(set_attr "type" "dmpy")])
 
+(define_insn "mulr03"
+  [(set (match_operand:SI 0 "arith_reg_operand" "=r")
+	(mult:SI (match_operand:SI 1 "arith_reg_operand" "%0")
+		 (match_operand:SI 2 "arith_reg_operand" "R03")))]
+  "TARGET_R0R3_TO_REG_MUL - !reload_completed >= 1"
+  "mulr	%2,%0"
+  [(set_attr "type" "dmpy")])
+
+(define_peephole2
+  [(set (reg:SI MACL_REG)
+	(mult:SI (match_operand:SI 1 "arith_reg_operand" "r,R03")
+		 (match_operand:SI 2 "arith_reg_operand" "R03,r")))
+   (set (match_operand:SI 0 "arith_reg_dest" "=r,r") (reg:SI MACL_REG))]
+  "TARGET_R0R3_TO_REG_MUL
+   && peep2_regno_dead_p (2, MACL_REG)
+   && ((!reg_overlap_mentioned_p (operands[0], operands[1])
+	&& true_regnum (operands[1]) <= R3_REG)
+       || (!reg_overlap_mentioned_p (operands[0], operands[2])
+	   && true_regnum (operands[2]) <= R3_REG))"
+  [(set (match_dup 0) (match_dup 3))
+   (set (match_dup 0) (mult:SI (match_dup 0) (match_dup 4)))]
+  "
+{
+  if (reg_overlap_mentioned_p (operands[0], operands[1])
+      || true_regnum (operands[1]) > R3_REG)
+    {
+      operands[4] = operands[2];
+      operands[3] = operands[1];
+    }
+  else
+    {
+      operands[4] = operands[1];
+      operands[3] = operands[2];
+    }
+}")
+
+(define_peephole2
+  [(match_scratch:SI 3 "R03")
+   (set (reg:SI MACL_REG)
+	(mult:SI (match_operand:SI 1 "arith_reg_operand" "r,0")
+		 (match_operand:SI 2 "arith_reg_operand" "0,r")))
+   (set (match_operand:SI 0 "arith_reg_dest" "=r,r") (reg:SI MACL_REG))]
+  "TARGET_R0R3_TO_REG_MUL
+   && peep2_regno_dead_p (3, MACL_REG)
+   && (true_regnum (operands[1]) == true_regnum (operands[0])
+       || true_regnum (operands[2]) == true_regnum (operands[0]))"
+  [(set (match_dup 3) (match_dup 4))
+   (set (match_dup 0) (mult:SI (match_dup 0) (match_dup 3)))]
+  "
+{
+  if (true_regnum (operands[1]) == true_regnum (operands[0]))
+    operands[4] = operands[2];
+  else
+    operands[4] = operands[1];
+}")
+
+;; ??? should we also use mulr if we'd need two reg-reg copies?
+
 (define_expand "mulsi3"
   [(set (reg:SI MACL_REG)
 	(mult:SI  (match_operand:SI 1 "arith_reg_operand" "")
@@ -2767,7 +2934,12 @@
 {
   rtx first, last;
 
-  if (!TARGET_SH2)
+  if (TARGET_R0R3_TO_REG_MUL == 2)
+    {
+      emit_insn (gen_mulr03 (operands[0], operands[1], operands[2]));
+      DONE;
+    }
+  else if (!TARGET_SH2)
     {
       /* The address must be set outside the libcall,
 	 since it goes into a pseudo.  */
@@ -4737,7 +4909,7 @@
   [(set (mem:SF (pre_dec:SI (reg:SI SP_REG))) (reg:SF FPUL_REG))]
   "TARGET_SH2E && ! TARGET_SH5"
   "sts.l	fpul,@-r15"
-  [(set_attr "type" "store")
+  [(set_attr "type" "fstore")
    (set_attr "late_fp_use" "yes")
    (set_attr "hit_stack" "yes")])
 
@@ -4819,9 +4991,9 @@
 ;; (made from (set (subreg:SI (reg:QI ###) 0) ) into T.
 (define_insn "movsi_i"
   [(set (match_operand:SI 0 "general_movdst_operand"
-	    "=r,r,t,r,r,r,r,m,<,<,x,l,x,l,r")
+	    "=r,r,r,t,r,r,r,r,m,<,<,x,l,x,l,r")
 	(match_operand:SI 1 "general_movsrc_operand"
-	 "Q,rI08,r,mr,x,l,t,r,x,l,r,r,>,>,i"))]
+	 "Q,r,I08,r,mr,x,l,t,r,x,l,r,r,>,>,i"))]
   "TARGET_SH1
    && ! TARGET_SH2E
    && ! TARGET_SH2A
@@ -4830,6 +5002,7 @@
   "@
 	mov.l	%1,%0
 	mov	%1,%0
+	mov	%1,%0
 	cmp/pl	%1
 	mov.l	%1,%0
 	sts	%1,%0
@@ -4843,8 +5016,8 @@
 	lds.l	%1,%0
 	lds.l	%1,%0
 	fake	%1,%0"
-  [(set_attr "type" "pcload_si,move,mt_group,load_si,mac_gp,prget,move,store,store,pstore,move,prset,load,pload,pcload_si")
-   (set_attr "length" "*,*,*,*,*,*,*,*,*,*,*,*,*,*,*")])
+  [(set_attr "type" "pcload_si,move,movi8,mt_group,load_si,mac_gp,prget,arith,store,mac_mem,pstore,gp_mac,prset,mem_mac,pload,pcload_si")
+   (set_attr "length" "*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*")])
 
 ;; t/r must come after r/r, lest reload will try to reload stuff like
 ;; (subreg:SI (reg:SF FR14_REG) 0) into T (compiling stdlib/strtod.c -m3e -O2)
@@ -4854,15 +5027,16 @@
 ;; TARGET_FMOVD is in effect, and mode switching is done before reload.
 (define_insn "movsi_ie"
   [(set (match_operand:SI 0 "general_movdst_operand"
-	    "=r,r,r,t,r,r,r,r,m,<,<,x,l,x,l,y,<,r,y,r,*f,y,*f,y")
+	    "=r,r,r,r,t,r,r,r,r,m,<,<,x,l,x,l,y,<,r,y,r,*f,y,*f,y")
 	(match_operand:SI 1 "general_movsrc_operand"
-	 "Q,rI08,I20,r,mr,x,l,t,r,x,l,r,r,>,>,>,y,i,r,y,y,*f,*f,y"))]
+	 "Q,r,I08,I20,r,mr,x,l,t,r,x,l,r,r,>,>,>,y,i,r,y,y,*f,*f,y"))]
   "(TARGET_SH2E || TARGET_SH2A)
    && (register_operand (operands[0], SImode)
        || register_operand (operands[1], SImode))"
   "@
 	mov.l	%1,%0
 	mov	%1,%0
+	mov	%1,%0
 	movi20	%1,%0
 	cmp/pl	%1
 	mov.l	%1,%0
@@ -4885,26 +5059,27 @@
 	flds	%1,fpul
 	fmov	%1,%0
 	! move optimized away"
-  [(set_attr "type" "pcload_si,move,move,*,load_si,mac_gp,prget,move,store,store,pstore,move,prset,load,pload,load,store,pcload_si,gp_fpul,fpul_gp,fmove,fmove,fmove,nil")
-   (set_attr "late_fp_use" "*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,yes,*,*,yes,*,*,*,*")
-   (set_attr "length" "*,*,4,*,4,*,*,*,4,*,*,*,*,*,*,*,*,*,*,*,*,*,*,0")])
+  [(set_attr "type" "pcload_si,move,movi8,move,*,load_si,mac_gp,prget,arith,store,mac_mem,pstore,gp_mac,prset,mem_mac,pload,load,fstore,pcload_si,gp_fpul,fpul_gp,fmove,fmove,fmove,nil")
+   (set_attr "late_fp_use" "*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,yes,*,*,yes,*,*,*,*")
+   (set_attr "length" "*,*,*,4,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,0")])
 
 (define_insn "movsi_i_lowpart"
-  [(set (strict_low_part (match_operand:SI 0 "general_movdst_operand" "+r,r,r,r,r,r,m,r"))
-	(match_operand:SI 1 "general_movsrc_operand" "Q,rI08,mr,x,l,t,r,i"))]
+  [(set (strict_low_part (match_operand:SI 0 "general_movdst_operand" "+r,r,r,r,r,r,r,m,r"))
+	(match_operand:SI 1 "general_movsrc_operand" "Q,r,I08,mr,x,l,t,r,i"))]
    "TARGET_SH1
     && (register_operand (operands[0], SImode)
         || register_operand (operands[1], SImode))"
   "@
 	mov.l	%1,%0
 	mov	%1,%0
+	mov	%1,%0
 	mov.l	%1,%0
 	sts	%1,%0
 	sts	%1,%0
 	movt	%0
 	mov.l	%1,%0
 	fake	%1,%0"
-  [(set_attr "type" "pcload,move,load,move,prget,move,store,pcload")])
+  [(set_attr "type" "pcload,move,arith,load,mac_gp,prget,arith,store,pcload")])
 
 (define_insn_and_split "load_ra"
   [(set (match_operand:SI 0 "general_movdst_operand" "")
@@ -5071,7 +5246,7 @@
       emit_insn (gen_ic_invalidate_line_compact (operands[0], operands[1]));
       DONE;
     }
-  else if (TARGET_SH4A_ARCH)
+  else if (TARGET_SH4A_ARCH || TARGET_SH4_300)
     {
       emit_insn (gen_ic_invalidate_line_sh4a (operands[0]));
       DONE;
@@ -5099,7 +5274,7 @@
 (define_insn "ic_invalidate_line_sh4a"
   [(unspec_volatile [(match_operand:SI 0 "register_operand" "r")]
 		    UNSPEC_ICACHE)]
-  "TARGET_SH4A_ARCH"
+  "TARGET_SH4A_ARCH || TARGET_SH4_300"
   "ocbwb\\t@%0\;synco\;icbi\\t@%0"
   [(set_attr "length" "16")
    (set_attr "type" "cwb")])
@@ -5156,19 +5331,20 @@
    (set_attr "needs_delay_slot" "yes")])
 
 (define_insn "movqi_i"
-  [(set (match_operand:QI 0 "general_movdst_operand" "=r,r,m,r,r,l")
-	(match_operand:QI 1 "general_movsrc_operand"  "ri,m,r,t,l,r"))]
+  [(set (match_operand:QI 0 "general_movdst_operand" "=r,r,r,m,r,r,l")
+	(match_operand:QI 1 "general_movsrc_operand"  "r,i,m,r,t,l,r"))]
   "TARGET_SH1
    && (arith_reg_operand (operands[0], QImode)
        || arith_reg_operand (operands[1], QImode))"
   "@
 	mov	%1,%0
+	mov	%1,%0
 	mov.b	%1,%0
 	mov.b	%1,%0
 	movt	%0
 	sts	%1,%0
 	lds	%1,%0"
- [(set_attr "type" "move,load,store,move,move,move")])
+ [(set_attr "type" "move,movi8,load,store,arith,prget,prset")])
 
 (define_insn "*movqi_media"
   [(set (match_operand:QI 0 "general_movdst_operand" "=r,r,r,m")
@@ -5770,7 +5946,7 @@
       (if_then_else
        (ne (symbol_ref "TARGET_SHCOMPACT") (const_int 0))
        (const_int 10) (const_int 8))])
-   (set_attr "type" "fmove,move,pcfload,fload,store,pcload,load,store,load,fload")
+   (set_attr "type" "fmove,move,pcfload,fload,fstore,pcload,load,store,load,fload")
    (set_attr "late_fp_use" "*,*,*,*,yes,*,*,*,*,*")
    (set (attr "fp_mode") (if_then_else (eq_attr "fmovd" "yes")
 					   (const_string "double")
@@ -6487,9 +6663,9 @@
 	sts.l	%1,%0
 	lds.l	%1,%0
 	! move optimized away"
-  [(set_attr "type" "fmove,move,fmove,fmove,pcfload,fload,store,pcload,load,store,fmove,fmove,load,*,fpul_gp,gp_fpul,store,load,nil")
+  [(set_attr "type" "fmove,move,fmove,fmove,pcfload,fload,fstore,pcload,load,store,fmove,fmove,load,*,fpul_gp,gp_fpul,fstore,load,nil")
    (set_attr "late_fp_use" "*,*,*,*,*,*,yes,*,*,*,*,*,*,*,yes,*,yes,*,*")
-   (set_attr "length" "*,*,*,*,4,4,4,*,*,*,2,2,2,4,2,2,2,2,0")
+   (set_attr "length" "*,*,*,*,4,2,2,*,*,*,2,2,2,4,2,2,2,2,0")
    (set (attr "fp_mode") (if_then_else (eq_attr "fmovd" "yes")
 					   (const_string "single")
 					   (const_string "none")))])
@@ -6696,6 +6872,59 @@
 
 ;; Conditional branch insns
 
+(define_expand "cbranchsf4"
+  [(set (pc) (if_then_else
+	      (match_operator 0 "comparison_operator"
+	       [(match_operand:SF 1 "register_operand" "")
+	        (match_operand:SF 2 "nonmemory_operand" "")])
+	      (label_ref (match_operand 3 "" ""))
+	      (pc)))]
+  "TARGET_SH1"
+  "
+{
+  sh_expand_float_cbranch (operands);
+  DONE;
+}")
+
+(define_expand "cbranchdf4"
+  [(set (pc) (if_then_else
+	      (match_operator 0 "comparison_operator"
+	       [(match_operand:DF 1 "register_operand" "")
+	        (match_operand:DF 2 "nonmemory_operand" "")])
+	      (label_ref (match_operand 3 "" ""))
+	      (pc)))]
+  "TARGET_SH1"
+  "
+{
+  sh_expand_float_cbranch (operands);
+  DONE;
+}")
+
+;; ??? cstore<mode> patterns cause compiler crashes.
+;;(define_expand "cstoresf4"
+;;  [(set (match_operand:SI 3 "arith_reg_operand" "")
+;;	      (match_operator:SI 0 "comparison_operator"
+;;	       [(match_operand:SF 1 "register_operand" "")
+;;	        (match_operand:SF 2 "nonmemory_operand" "")]))]
+;;  "TARGET_SH1"
+;;  "
+;;{
+;;  sh_expand_float_scc (operands);
+;;  DONE;
+;;}")
+;;
+;;(define_expand "cstoredf4"
+;;  [(set (match_operand:SI 3 "arith_reg_operand" "")
+;;	      (match_operator:SI 0 "comparison_operator"
+;;	       [(match_operand:DF 1 "register_operand" "")
+;;	        (match_operand:DF 2 "nonmemory_operand" "")]))]
+;;  "TARGET_SH1"
+;;  "
+;;{
+;;  sh_expand_float_scc (operands);
+;;  DONE;
+;;}")
+
 (define_expand "beq_media"
   [(set (pc)
 	(if_then_else (eq (match_operand:DI 1 "arith_reg_operand" "r,r")
@@ -6827,10 +7056,9 @@
 	(if_then_else (ne (reg:SI T_REG) (const_int 0))
 		      (label_ref (match_operand 0 "" ""))
 		      (pc)))]
-  ""
+  "TARGET_SHMEDIA"
   "
 {
-  if (TARGET_SHMEDIA)
     {
       enum machine_mode mode = GET_MODE (sh_compare_op0);
 
@@ -6852,8 +7080,6 @@
 				     sh_compare_op0, sh_compare_op1));
       DONE;
     }
-
-  from_compare (operands, EQ);
 }")
 
 (define_expand "bne"
@@ -6861,10 +7087,9 @@
 	(if_then_else (eq (reg:SI T_REG) (const_int 0))
 		      (label_ref (match_operand 0 "" ""))
 		      (pc)))]
-  ""
+  "TARGET_SHMEDIA"
   "
 {
-  if (TARGET_SHMEDIA)
     {
       enum machine_mode mode = GET_MODE (sh_compare_op0);
 
@@ -6886,8 +7111,6 @@
 				     sh_compare_op0, sh_compare_op1));
       DONE;
     }
-
-  from_compare (operands, EQ);
 }")
 
 (define_expand "bgt"
@@ -6895,10 +7118,9 @@
 	(if_then_else (ne (reg:SI T_REG) (const_int 0))
 		      (label_ref (match_operand 0 "" ""))
 		      (pc)))]
-  ""
+  "TARGET_SHMEDIA"
   "
 {
-  if (TARGET_SHMEDIA)
     {
       enum machine_mode mode = GET_MODE (sh_compare_op0);
 
@@ -6919,8 +7141,6 @@
 				     sh_compare_op0, sh_compare_op1));
       DONE;
     }
-
-  from_compare (operands, GT);
 }")
 
 (define_expand "blt"
@@ -6928,10 +7148,9 @@
 	(if_then_else (eq (reg:SI T_REG) (const_int 0))
 		      (label_ref (match_operand 0 "" ""))
 		      (pc)))]
-  ""
+  "TARGET_SHMEDIA"
   "
 {
-  if (TARGET_SHMEDIA)
     {
       enum machine_mode mode = GET_MODE (sh_compare_op0);
 
@@ -6952,16 +7171,6 @@
 				     sh_compare_op1, sh_compare_op0));
       DONE;
     }
-
-  if (GET_MODE_CLASS (GET_MODE (sh_compare_op0)) == MODE_FLOAT)
-    {
-      rtx tmp = sh_compare_op0;
-      sh_compare_op0 = sh_compare_op1;
-      sh_compare_op1 = tmp;
-      emit_insn (gen_bgt (operands[0]));
-      DONE;
-    }
-  from_compare (operands, GE);
 }")
 
 (define_expand "ble"
@@ -6969,10 +7178,9 @@
 	(if_then_else (eq (reg:SI T_REG) (const_int 0))
 		      (label_ref (match_operand 0 "" ""))
 		      (pc)))]
-  ""
+  "TARGET_SHMEDIA"
   "
 {
-  if (TARGET_SHMEDIA)
     {
       enum machine_mode mode = GET_MODE (sh_compare_op0);
 
@@ -6993,18 +7201,6 @@
 				     sh_compare_op1, sh_compare_op0));
       DONE;
     }
-
-  if (TARGET_SH2E
-      && TARGET_IEEE
-      && GET_MODE_CLASS (GET_MODE (sh_compare_op0)) == MODE_FLOAT)
-    {
-      rtx tmp = sh_compare_op0;
-      sh_compare_op0 = sh_compare_op1;
-      sh_compare_op1 = tmp;
-      emit_insn (gen_bge (operands[0]));
-      DONE;
-    }
-  from_compare (operands, GT);
 }")
 
 (define_expand "bge"
@@ -7012,10 +7208,9 @@
 	(if_then_else (ne (reg:SI T_REG) (const_int 0))
 		      (label_ref (match_operand 0 "" ""))
 		      (pc)))]
-  ""
+  "TARGET_SHMEDIA"
   "
 {
-  if (TARGET_SHMEDIA)
     {
       enum machine_mode mode = GET_MODE (sh_compare_op0);
 
@@ -7036,18 +7231,6 @@
 				     sh_compare_op0, sh_compare_op1));
       DONE;
     }
-
-  if (TARGET_SH2E
-      && ! TARGET_IEEE
-      && GET_MODE_CLASS (GET_MODE (sh_compare_op0)) == MODE_FLOAT)
-    {
-      rtx tmp = sh_compare_op0;
-      sh_compare_op0 = sh_compare_op1;
-      sh_compare_op1 = tmp;
-      emit_insn (gen_ble (operands[0]));
-      DONE;
-    }
-  from_compare (operands, GE);
 }")
 
 (define_expand "bgtu"
@@ -7055,10 +7238,9 @@
 	(if_then_else (ne (reg:SI T_REG) (const_int 0))
 		      (label_ref (match_operand 0 "" ""))
 		      (pc)))]
-  ""
+  "TARGET_SHMEDIA"
   "
 {
-  if (TARGET_SHMEDIA)
     {
       enum machine_mode mode = GET_MODE (sh_compare_op0);
 
@@ -7070,8 +7252,6 @@
 				      sh_compare_op0, sh_compare_op1));
       DONE;
     }
-
-  from_compare (operands, GTU);
 }")
 
 (define_expand "bltu"
@@ -7079,10 +7259,9 @@
 	(if_then_else (eq (reg:SI T_REG) (const_int 0))
 		      (label_ref (match_operand 0 "" ""))
 		      (pc)))]
-  ""
+  "TARGET_SHMEDIA"
   "
 {
-  if (TARGET_SHMEDIA)
     {
       enum machine_mode mode = GET_MODE (sh_compare_op0);
 
@@ -7094,8 +7273,6 @@
 				      sh_compare_op1, sh_compare_op0));
       DONE;
     }
-
-  from_compare (operands, GEU);
 }")
 
 (define_expand "bgeu"
@@ -7103,10 +7280,9 @@
 	(if_then_else (ne (reg:SI T_REG) (const_int 0))
 		      (label_ref (match_operand 0 "" ""))
 		      (pc)))]
-  ""
+  "TARGET_SHMEDIA"
   "
 {
-  if (TARGET_SHMEDIA)
     {
       enum machine_mode mode = GET_MODE (sh_compare_op0);
 
@@ -7118,8 +7294,6 @@
 				      sh_compare_op0, sh_compare_op1));
       DONE;
     }
-
-  from_compare (operands, GEU);
 }")
 
 (define_expand "bleu"
@@ -7127,10 +7301,9 @@
 	(if_then_else (eq (reg:SI T_REG) (const_int 0))
 		      (label_ref (match_operand 0 "" ""))
 		      (pc)))]
-  ""
+  "TARGET_SHMEDIA"
   "
 {
-  if (TARGET_SHMEDIA)
     {
       enum machine_mode mode = GET_MODE (sh_compare_op0);
 
@@ -7142,8 +7315,50 @@
 				      sh_compare_op1, sh_compare_op0));
       DONE;
     }
+}")
+
+(define_expand "cmpun_sdf"
+  [(unordered (match_operand 0 "" "") (match_operand 1 "" ""))]
+  ""
+  "
+{
+  HOST_WIDE_INT mask;
+  switch (GET_MODE (operands[0]))
+    {
+    case SFmode:
+      mask = SF_NAN_MASK;
+      break;
+    case DFmode:
+      mask = DF_NAN_MASK;
+      break;
+    default:
+      FAIL;
+    }
+  emit_insn (gen_cmpunsf_i1 (operands[0], operands[1],
+			     force_reg (SImode, GEN_INT (mask))));
+  DONE;
+}")
 
-  from_compare (operands, GTU);
+(define_expand "cmpuneq_sdf"
+  [(uneq (match_operand 0 "" "") (match_operand 1 "" ""))]
+  ""
+  "
+{
+  HOST_WIDE_INT mask;
+  switch (GET_MODE (operands[0]))
+    {
+    case SFmode:
+      mask = SF_NAN_MASK;
+      break;
+    case DFmode:
+      mask = DF_NAN_MASK;
+      break;
+    default:
+      FAIL;
+    }
+  emit_insn (gen_cmpuneqsf_i1 (operands[0], operands[1],
+			       force_reg (SImode, GEN_INT (mask))));
+  DONE;
 }")
 
 (define_expand "bunordered"
@@ -7155,10 +7370,44 @@
   "TARGET_SHMEDIA"
   "
 {
-  operands[0] = gen_rtx_LABEL_REF (Pmode, operands[0]);
-  operands[1] = gen_reg_rtx (SImode);
   operands[2] = force_reg (GET_MODE (sh_compare_op0), sh_compare_op0);
   operands[3] = force_reg (GET_MODE (sh_compare_op1), sh_compare_op1);
+  operands[0] = gen_rtx_LABEL_REF (Pmode, operands[0]);
+  operands[1] = gen_reg_rtx (SImode);
+}")
+
+;; ??? The entire compare/branch scheme is antiquated, we should define
+;; cbranchsf / cbranchdf instead.
+(define_expand "bunle"
+  [(set (pc)
+	(if_then_else (ne (reg:SI T_REG) (const_int 0))
+		      (match_operand 0 "" "")
+		      (pc)))]
+  "TARGET_SHMEDIA_FPU"
+  "
+{
+  rtx tmp = gen_reg_rtx (SImode);
+
+  operands[0] = gen_rtx_LABEL_REF (Pmode, operands[0]);
+  emit_insn (gen_sgt (tmp));
+  emit_jump_insn (gen_beq_media (operands[0], tmp, const0_rtx));
+  DONE;
+}")
+
+(define_expand "bunlt"
+  [(set (pc)
+	(if_then_else (eq (reg:SI T_REG) (const_int 0))
+		      (match_operand 0 "" "")
+		      (pc)))]
+  "TARGET_SHMEDIA_FPU"
+  "
+{
+  rtx tmp = gen_reg_rtx (SImode);
+
+  operands[0] = gen_rtx_LABEL_REF (Pmode, operands[0]);
+  emit_insn (gen_sge (tmp));
+  emit_jump_insn (gen_beq_media (operands[0], tmp, const0_rtx));
+  DONE;
 }")
 
 ;; combiner splitter for test-and-branch on single bit in register.  This
@@ -7189,6 +7438,52 @@
 		 ? gen_rtx_GE (VOIDmode, operands[4], const0_rtx)
 		 : gen_rtx_GT (VOIDmode, const0_rtx, operands[4]));
 }")
+
+; operand 0 is the loop count pseudo register
+; operand 1 is the number of loop iterations or 0 if it is unknown
+; operand 2 is the maximum number of loop iterations
+; operand 3 is the number of levels of enclosed loops
+; operand 4 is the label to jump to at the top of the loop
+
+(define_expand "doloop_end"
+  [(parallel [(set (pc) (if_then_else
+			  (ne:SI (match_operand:SI 0 "" "")
+			      (const_int 1))
+			  (label_ref (match_operand 4 "" ""))
+			  (pc)))
+	      (set (match_dup 0)
+		   (plus:SI (match_dup 0) (const_int -1)))
+	      (clobber (reg:SI T_REG))])]
+  "TARGET_SH2"
+  "
+{
+  if (GET_MODE (operands[0]) != SImode)
+    FAIL;
+}
+")
+
+(define_insn_and_split "doloop_end_split"
+  [(set (pc)
+	(if_then_else (ne:SI (match_operand:SI 0 "arith_reg_dest" "+r")
+			  (const_int 1))
+		      (label_ref (match_operand 1 "" ""))
+		      (pc)))
+   (set (match_dup 0)
+	(plus (match_dup 0) (const_int -1)))
+   (clobber (reg:SI T_REG))]
+  "TARGET_SH2"
+  "#"
+  ""
+  [(parallel [(set (reg:SI T_REG)
+		   (eq:SI (match_operand:SI 0 "arith_reg_dest" "+r")
+			  (const_int 1)))
+	      (set (match_dup 0) (plus:SI (match_dup 0) (const_int -1)))])
+   (set (pc) (if_then_else (eq (reg:SI T_REG) (const_int 0))
+			   (label_ref (match_operand 1 "" ""))
+			   (pc)))]
+""
+   [(set_attr "type" "cbranch")])
+
 
 ;; ------------------------------------------------------------------------
 ;; Jump and linkage insns
@@ -9162,6 +9457,12 @@
     DONE;
   if (! currently_expanding_to_rtl)
     FAIL;
+  if (GET_MODE_CLASS (GET_MODE (sh_compare_op0)) == MODE_FLOAT)
+   {
+      from_compare (operands, EQ);
+      emit_insn (gen_movt (operands[0]));
+      DONE;
+    }
   operands[1] = prepare_scc_operands (EQ);
 }")
 
@@ -9434,7 +9735,12 @@
     FAIL;
   if (GET_MODE_CLASS (GET_MODE (sh_compare_op0)) == MODE_FLOAT)
     {
-      if (TARGET_IEEE)
+      if (TARGET_SH1_SOFTFP_MODE (GET_MODE (sh_compare_op0)))
+	{
+	  from_compare (operands, GE);
+	  emit_insn (gen_movt (operands[0]));
+	}
+      else if (TARGET_IEEE)
 	{
 	  rtx lab = gen_label_rtx ();
 	  prepare_scc_operands (EQ);
@@ -9444,7 +9750,10 @@
 	  emit_insn (gen_movt (operands[0]));
 	}
       else
-	emit_insn (gen_movnegt (operands[0], prepare_scc_operands (LT)));
+	{
+	  prepare_scc_operands (LT);
+	  emit_insn (gen_movnegt (operands[0]));
+	}
       DONE;
     }
   operands[1] = prepare_scc_operands (GE);
@@ -9587,6 +9896,19 @@
   operands[1] = prepare_scc_operands (GEU);
 }")
 
+(define_expand "sunle"
+  [(set (match_operand:SI 0 "arith_reg_operand" "")
+	(match_dup 1))]
+  "TARGET_SH1_SOFTFP"
+  "
+{
+  if (! currently_expanding_to_rtl)
+    FAIL;
+  from_compare (operands, UNLE);
+  emit_insn (gen_movt (operands[0]));
+  DONE;
+}")
+
 ;; sne moves the complement of the T reg to DEST like this:
 ;;      cmp/eq ...
 ;;      mov    #-1,temp
@@ -9640,7 +9962,14 @@
     DONE;
   if (! currently_expanding_to_rtl)
     FAIL;
-  operands[1] = prepare_scc_operands (EQ);
+  if (GET_MODE_CLASS (GET_MODE (sh_compare_op0)) == MODE_FLOAT
+      && TARGET_SH1_SOFTFP_MODE (GET_MODE (sh_compare_op0) == DFmode))
+    {
+      from_compare (operands, EQ);
+      operands[1] = gen_rtx_REG (SImode, T_REG);
+    }
+  else
+    operands[1] = prepare_scc_operands (EQ);
   operands[2] = gen_reg_rtx (SImode);
 }")
 
@@ -9665,10 +9994,14 @@
 		   (neg:SI (plus:SI (match_dup 1)
 				    (match_dup 2))))
 	      (set (reg:SI T_REG)
-		   (ne:SI (ior:SI (match_operand 1 "" "") (match_dup 2))
+		   (ne:SI (ior:SI (match_dup 1) (match_dup 2))
 			  (const_int 0)))])]
   "TARGET_SH1"
-  "operands[2] = gen_reg_rtx (SImode);")
+  "
+{
+  operands[1] = gen_rtx_REG (SImode, T_REG);
+  operands[2] = gen_reg_rtx (SImode);
+}")
 
 ;; Recognize mov #-1/negc/neg sequence, and change it to movt/add #-1.
 ;; This prevents a regression that occurred when we switched from xor to
@@ -9944,7 +10277,7 @@
 	sts	fpscr,%0
 	sts.l	fpscr,%0"
   [(set_attr "length" "0,2,2,4,2,2,2,2,2")
-   (set_attr "type" "nil,mem_fpscr,load,mem_fpscr,gp_fpscr,move,store,mac_gp,store")])
+   (set_attr "type" "nil,mem_fpscr,load,mem_fpscr,gp_fpscr,move,store,mac_gp,fstore")])
 
 (define_peephole2
   [(set (reg:PSI FPSCR_REG)
@@ -9995,25 +10328,20 @@
 	(xor:PSI (reg:PSI FPSCR_REG) (const_int 1048576)))]
   "(TARGET_SH4 || TARGET_SH2A_DOUBLE)"
   "fschg"
-  [(set_attr "type" "fp") (set_attr "fp_set" "unknown")])
+  [(set_attr "type" "fpscr_toggle") (set_attr "fp_set" "unknown")])
 
-;; There's no way we can use it today, since optimize mode switching
-;; doesn't enable us to know from which mode we're switching to the
-;; mode it requests, to tell whether we can use a relative mode switch
-;; (like toggle_pr) or an absolute switch (like loading fpscr from
-;; memory).
 (define_insn "toggle_pr"
   [(set (reg:PSI FPSCR_REG)
 	(xor:PSI (reg:PSI FPSCR_REG) (const_int 524288)))]
-  "TARGET_SH4A_FP && ! TARGET_FPU_SINGLE"
+  "(TARGET_SH4A_FP || TARGET_SH4_300)"
   "fpchg"
-  [(set_attr "type" "fp")])
+  [(set_attr "type" "fpscr_toggle")])
 
 (define_expand "addsf3"
   [(set (match_operand:SF 0 "arith_reg_operand" "")
 	(plus:SF (match_operand:SF 1 "arith_reg_operand" "")
 		 (match_operand:SF 2 "arith_reg_operand" "")))]
-  "TARGET_SH2E || TARGET_SHMEDIA_FPU"
+  "TARGET_SH2E || (TARGET_SH3 && TARGET_OSFP) || TARGET_SHMEDIA_FPU"
   "
 {
   if (TARGET_SH2E)
@@ -10021,6 +10349,12 @@
       expand_sf_binop (&gen_addsf3_i, operands);
       DONE;
     }
+  else if (TARGET_OSFP)
+    {
+      expand_sfunc_binop (SFmode, &gen_addsf3_i3, \"__addsf3\", PLUS,
+			  operands);
+      DONE;
+    }
 }")
 
 (define_insn "*addsf3_media"
@@ -10097,6 +10431,22 @@
 }"
   [(set_attr "type" "fparith_media")])
 
+(define_insn "addsf3_i3"
+  [(set (match_operand:SF 0 "arith_reg_dest" "=z")
+	(plus:SF (reg:SF R4_REG) (reg:SF R5_REG)))
+   (clobber (reg:SI T_REG))
+   (clobber (reg:SI PR_REG))
+   (clobber (reg:SI R1_REG))
+   (clobber (reg:SI R2_REG))
+   (clobber (reg:SI R3_REG))
+   (clobber (reg:SI R6_REG))
+   (clobber (reg:SI R7_REG))
+   (use (match_operand:SI 1 "arith_reg_operand" "r"))]
+  "TARGET_OSFP && ! TARGET_SH2E"
+  "jsr	@%1%#"
+  [(set_attr "type" "sfunc")
+   (set_attr "needs_delay_slot" "yes")])
+
 (define_insn "addsf3_i"
   [(set (match_operand:SF 0 "fp_arith_reg_operand" "=f")
 	(plus:SF (match_operand:SF 1 "fp_arith_reg_operand" "%0")
@@ -10111,7 +10461,7 @@
   [(set (match_operand:SF 0 "fp_arith_reg_operand" "")
 	(minus:SF (match_operand:SF 1 "fp_arith_reg_operand" "")
 		  (match_operand:SF 2 "fp_arith_reg_operand" "")))]
-  "TARGET_SH2E || TARGET_SHMEDIA_FPU"
+  "TARGET_SH2E || (TARGET_SH3 && TARGET_OSFP) || TARGET_SHMEDIA_FPU"
   "
 {
   if (TARGET_SH2E)
@@ -10119,6 +10469,12 @@
       expand_sf_binop (&gen_subsf3_i, operands);
       DONE;
     }
+  else if (TARGET_OSFP)
+    {
+      expand_sfunc_binop (SFmode, &gen_subsf3_i3, \"__subsf3\", MINUS,
+			  operands);
+      DONE;
+    }
 }")
 
 (define_insn "*subsf3_media"
@@ -10129,6 +10485,23 @@
   "fsub.s	%1, %2, %0"
   [(set_attr "type" "fparith_media")])
 
+(define_insn "subsf3_i3"
+  [(set (match_operand:SF 0 "arith_reg_dest" "=z")
+	(minus:SF (reg:SF R4_REG) (reg:SF R5_REG)))
+   (clobber (reg:SI T_REG))
+   (clobber (reg:SI PR_REG))
+   (clobber (reg:SI R1_REG))
+   (clobber (reg:SI R2_REG))
+   (clobber (reg:SI R3_REG))
+   (clobber (reg:SI R5_REG))
+   (clobber (reg:SI R6_REG))
+   (clobber (reg:SI R7_REG))
+   (use (match_operand:SI 1 "arith_reg_operand" "r"))]
+  "TARGET_OSFP && ! TARGET_SH2E"
+  "jsr	@%1%#"
+  [(set_attr "type" "sfunc")
+   (set_attr "needs_delay_slot" "yes")])
+
 (define_insn "subsf3_i"
   [(set (match_operand:SF 0 "fp_arith_reg_operand" "=f")
 	(minus:SF (match_operand:SF 1 "fp_arith_reg_operand" "0")
@@ -10139,24 +10512,19 @@
   [(set_attr "type" "fp")
    (set_attr "fp_mode" "single")])
 
-;; Unfortunately, the combiner is unable to cope with the USE of the FPSCR
-;; register in feeding fp instructions.  Thus, we cannot generate fmac for
-;; mixed-precision SH4 targets.  To allow it to be still generated for the
-;; SH3E, we use a separate insn for SH3E mulsf3.
-
 (define_expand "mulsf3"
   [(set (match_operand:SF 0 "fp_arith_reg_operand" "")
 	(mult:SF (match_operand:SF 1 "fp_arith_reg_operand" "")
 		 (match_operand:SF 2 "fp_arith_reg_operand" "")))]
-  "TARGET_SH2E || TARGET_SHMEDIA_FPU"
+  "TARGET_SH2E || (TARGET_SH3 && TARGET_OSFP) || TARGET_SHMEDIA_FPU"
   "
 {
-  if (TARGET_SH4 || TARGET_SH2A_SINGLE)
-    expand_sf_binop (&gen_mulsf3_i4, operands);
-  else if (TARGET_SH2E)
-    emit_insn (gen_mulsf3_ie (operands[0], operands[1], operands[2]));
-  if (! TARGET_SHMEDIA)
-    DONE;
+  if (!TARGET_SH2E && TARGET_OSFP)
+    {
+      expand_sfunc_binop (SFmode, &gen_mulsf3_i3, \"__mulsf3\", MULT,
+                         operands);
+      DONE;
+    }
 }")
 
 (define_insn "*mulsf3_media"
@@ -10167,6 +10535,27 @@
   "fmul.s	%1, %2, %0"
   [(set_attr "type" "fparith_media")])
 
+;; Unfortunately, the combiner is unable to cope with the USE of the FPSCR
+;; register in feeding fp instructions.  Thus, in order to generate fmac,
+;; we start out with a mulsf pattern that does not depend on fpscr.
+;; This is split after combine to introduce the dependency, in order to
+;; get mode switching and scheduling right.
+(define_insn_and_split "mulsf3_ie"
+  [(set (match_operand:SF 0 "fp_arith_reg_operand" "=f")
+	(mult:SF (match_operand:SF 1 "fp_arith_reg_operand" "%0")
+		 (match_operand:SF 2 "fp_arith_reg_operand" "f")))]
+  "TARGET_SH2E"
+  "fmul	%2,%0"
+  "TARGET_SH4 || TARGET_SH2A_SINGLE"
+  [(const_int 0)]
+  "
+{
+  emit_insn (gen_mulsf3_i4 (operands[0], operands[1], operands[2],
+	     get_fpscr_rtx ()));
+  DONE;
+}"
+  [(set_attr "type" "fp")])
+
 (define_insn "mulsf3_i4"
   [(set (match_operand:SF 0 "fp_arith_reg_operand" "=f")
 	(mult:SF (match_operand:SF 1 "fp_arith_reg_operand" "%0")
@@ -10177,20 +10566,28 @@
   [(set_attr "type" "fp")
    (set_attr "fp_mode" "single")])
 
-(define_insn "mulsf3_ie"
-  [(set (match_operand:SF 0 "fp_arith_reg_operand" "=f")
-	(mult:SF (match_operand:SF 1 "fp_arith_reg_operand" "%0")
-		 (match_operand:SF 2 "fp_arith_reg_operand" "f")))]
-  "TARGET_SH2E && ! (TARGET_SH4 || TARGET_SH2A_SINGLE)"
-  "fmul	%2,%0"
-  [(set_attr "type" "fp")])
+(define_insn "mulsf3_i3"
+  [(set (match_operand:SF 0 "arith_reg_dest" "=z")
+	(mult:SF (reg:SF R4_REG) (reg:SF R5_REG)))
+   (clobber (reg:SI MACH_REG))
+   (clobber (reg:SI MACL_REG))
+   (clobber (reg:SI T_REG))
+   (clobber (reg:SI PR_REG))
+   (clobber (reg:SI R1_REG))
+   (clobber (reg:SI R2_REG))
+   (clobber (reg:SI R3_REG))
+   (use (match_operand:SI 1 "arith_reg_operand" "r"))]
+  "TARGET_OSFP && ! TARGET_SH2E"
+  "jsr	@%1%#"
+  [(set_attr "type" "sfunc")
+   (set_attr "needs_delay_slot" "yes")])
 
 (define_insn "mac_media"
   [(set (match_operand:SF 0 "fp_arith_reg_operand" "=f")
 	(plus:SF (mult:SF (match_operand:SF 1 "fp_arith_reg_operand" "%f")
 			  (match_operand:SF 2 "fp_arith_reg_operand" "f"))
 		 (match_operand:SF 3 "fp_arith_reg_operand" "0")))]
-  "TARGET_SHMEDIA_FPU"
+  "TARGET_SHMEDIA_FPU && TARGET_FMAC"
   "fmac.s %1, %2, %0"
   [(set_attr "type" "fparith_media")])
 
@@ -10200,7 +10597,7 @@
 			  (match_operand:SF 2 "fp_arith_reg_operand" "f"))
 		 (match_operand:SF 3 "arith_reg_operand" "0")))
    (use (match_operand:PSI 4 "fpscr_operand" "c"))]
-  "TARGET_SH2E && ! TARGET_SH4"
+  "TARGET_SH2E && TARGET_FMAC"
   "fmac	fr0,%2,%0"
   [(set_attr "type" "fp")
    (set_attr "fp_mode" "single")])
@@ -10345,13 +10742,156 @@
   "ftrc	%1,%0"
   [(set_attr "type" "fp")])
 
+(define_insn "cmpnesf_i1"
+  [(set (match_operand:CC_FP_NE 0 "register_operand" "=z")
+	(compare:CC_FP_NE (reg:SF R4_REG) (reg:SF R5_REG)))
+   (clobber (reg:SI T_REG))
+   (clobber (reg:SI PR_REG))
+   (clobber (reg:SI R1_REG))
+   (use (match_operand:SI 1 "arith_reg_operand" "r"))]
+  "TARGET_SH1 && ! TARGET_SH2E"
+  "jsr	@%1%#"
+  [(set_attr "type" "sfunc")
+   (set_attr "needs_delay_slot" "yes")])
+
+(define_insn "cmpgtsf_i1"
+  [(set (match_operand:CC_FP_GT 0 "register_operand" "=z")
+	(compare:CC_FP_GT (reg:SF R4_REG) (reg:SF R5_REG)))
+   (clobber (reg:SI T_REG))
+   (clobber (reg:SI PR_REG))
+   (clobber (reg:SI R1_REG))
+   (use (match_operand:SI 1 "arith_reg_operand" "r"))]
+  "TARGET_SH1 && ! TARGET_SH2E"
+  "jsr	@%1%#"
+  [(set_attr "type" "sfunc")
+   (set_attr "needs_delay_slot" "yes")])
+
+(define_insn "cmpunltsf_i1"
+  [(set (match_operand:CC_FP_UNLT 0 "register_operand" "=z")
+	(compare:CC_FP_UNLT (reg:SF R4_REG) (reg:SF R5_REG)))
+   (clobber (reg:SI T_REG))
+   (clobber (reg:SI PR_REG))
+   (clobber (reg:SI R1_REG))
+   (use (match_operand:SI 1 "arith_reg_operand" "r"))]
+  "TARGET_SH1 && ! TARGET_SH2E"
+  "jsr	@%1%#"
+  [(set_attr "type" "sfunc")
+   (set_attr "needs_delay_slot" "yes")])
+
+(define_insn "cmpeqsf_i1_finite"
+  [(set (reg:SI T_REG)
+	(eq:SI (match_operand:SF 0 "arith_reg_operand" "r,r,r")
+	       (match_operand:SF 1 "arith_reg_operand" "r,r,r")))
+   (clobber (match_scratch:SI 2 "=0,1,?r"))]
+  "TARGET_SH1 && ! TARGET_SH2E && flag_finite_math_only"
+  "*
+{
+  if (which_alternative == 0)
+     output_asm_insn (\"cmp/eq\t%0,%1\;or\t%1,%2\;bt\t0f\", operands);
+  else if (which_alternative == 1)
+     output_asm_insn (\"cmp/eq\t%0,%1\;or\t%0,%2\;bt\t0f\", operands);
+  else
+    output_asm_insn (\"cmp/eq\t%0,%1\;mov\t%0,%2\;bt\t0f\;or\t%1,%2\",
+		     operands);
+  return \"add\t%2,%2\;tst\t%2,%2\\n0:\";
+}"
+  [(set_attr "length" "10,10,12")])
+
+(define_insn "cmplesf_i1_finite"
+  [(set (reg:SI T_REG)
+	(le:SI (match_operand:SF 0 "arith_reg_operand" "r,r,r")
+	       (match_operand:SF 1 "arith_reg_operand" "r,r,r")))
+   (clobber (match_scratch:SI 2 "=0,1,r"))]
+  "TARGET_SH1 && ! TARGET_SH2E && flag_finite_math_only"
+  "*
+{
+  output_asm_insn (\"cmp/pz\t%0\", operands);
+  if (which_alternative == 2)
+    output_asm_insn (\"mov\t%0,%2\", operands);
+  if (TARGET_SH2)
+    output_asm_insn (\"bf/s\t0f\;cmp/hs\t%1,%0\;cmp/ge\t%0,%1\", operands);
+  else
+    output_asm_insn (\"bt\t1f\;bra\t0f\;cmp/hs\t%1,%0\\n1:\tcmp/ge\t%0,%1\",
+		     operands);
+  if (which_alternative == 1)
+    output_asm_insn (\"or\t%0,%2\", operands);
+  else
+    output_asm_insn (\"or\t%1,%2\", operands);
+  return \"bt\t0f\;add\t%2,%2\;tst\t%2,%2\\n0:\";
+}"
+  [(set_attr "length" "18,18,20")])
+
+(define_insn "cmpunsf_i1"
+  [(set (reg:SI T_REG)
+	(unordered:SI (match_operand:SF 0 "arith_reg_operand" "r,r")
+		      (match_operand:SF 1 "arith_reg_operand" "r,r")))
+   (use (match_operand:SI 2 "arith_reg_operand" "r,r"))
+   (clobber (match_scratch:SI 3 "=0,&r"))]
+  "TARGET_SH1 && ! TARGET_SH2E"
+  "not\t%0,%3\;tst\t%2,%3\;not\t%1,%3\;bt\t0f\;tst\t%2,%3\;0:"
+  [(set_attr "length" "10")])
+
+;; ??? This is a lot of code with a lot of branches; a library function
+;; might be better.
+(define_insn "cmpuneqsf_i1"
+  [(set (reg:SI T_REG)
+	(uneq:SI (match_operand:SF 0 "arith_reg_operand" "r")
+		 (match_operand:SF 1 "arith_reg_operand" "r")))
+   (use (match_operand:SI 2 "arith_reg_operand" "r"))
+   (clobber (match_scratch:SI 3 "=&r"))]
+  "TARGET_SH1 && ! TARGET_SH2E"
+  "*
+{
+  output_asm_insn (\"not\t%0,%3\;tst\t%2,%3\;not\t%1,%3\", operands);
+  output_asm_insn (\"bt\t0f\;tst\t%2,%3\;bt\t0f\;cmp/eq\t%0,%1\", operands);
+  output_asm_insn (\"mov\t%0,%3\;bt\t0f\;or\t%1,%3\", operands);
+  return \"add\t%3,%3\;tst\t%3,%3\\n0:\";
+}"
+  [(set_attr "length" "24")])
+
+(define_insn "movcc_fp_ne"
+  [(set (match_operand:CC_FP_NE 0 "general_movdst_operand"
+	    "=r,r,m")
+	(match_operand:CC_FP_NE 1 "general_movsrc_operand"
+	 "rI08,mr,r"))]
+  "TARGET_SH1"
+  "@
+	mov	%1,%0
+	mov.l	%1,%0
+	mov.l	%1,%0"
+  [(set_attr "type" "move,load,store")])
+
+(define_insn "movcc_fp_gt"
+  [(set (match_operand:CC_FP_GT 0 "general_movdst_operand"
+	    "=r,r,m")
+	(match_operand:CC_FP_GT 1 "general_movsrc_operand"
+	 "rI08,mr,r"))]
+  "TARGET_SH1"
+  "@
+	mov	%1,%0
+	mov.l	%1,%0
+	mov.l	%1,%0"
+  [(set_attr "type" "move,load,store")])
+
+(define_insn "movcc_fp_unlt"
+  [(set (match_operand:CC_FP_UNLT 0 "general_movdst_operand"
+	    "=r,r,m")
+	(match_operand:CC_FP_UNLT 1 "general_movsrc_operand"
+	 "rI08,mr,r"))]
+  "TARGET_SH1"
+  "@
+	mov	%1,%0
+	mov.l	%1,%0
+	mov.l	%1,%0"
+  [(set_attr "type" "move,load,store")])
+
 (define_insn "cmpgtsf_t"
   [(set (reg:SI T_REG)
 	(gt:SI (match_operand:SF 0 "fp_arith_reg_operand" "f")
 	       (match_operand:SF 1 "fp_arith_reg_operand" "f")))]
   "TARGET_SH2E && ! (TARGET_SH4 || TARGET_SH2A_SINGLE)"
   "fcmp/gt	%1,%0"
-  [(set_attr "type" "fp")
+  [(set_attr "type" "fp_cmp")
    (set_attr "fp_mode" "single")])
 
 (define_insn "cmpeqsf_t"
@@ -10360,7 +10900,7 @@
 	       (match_operand:SF 1 "fp_arith_reg_operand" "f")))]
   "TARGET_SH2E && ! (TARGET_SH4 || TARGET_SH2A_SINGLE)"
   "fcmp/eq	%1,%0"
-  [(set_attr "type" "fp")
+  [(set_attr "type" "fp_cmp")
    (set_attr "fp_mode" "single")])
 
 (define_insn "ieee_ccmpeqsf_t"
@@ -10372,6 +10912,22 @@
   "* return output_ieee_ccmpeq (insn, operands);"
   [(set_attr "length" "4")])
 
+(define_insn "*cmpltgtsf_t"
+  [(set (reg:SI T_REG)
+	(ltgt:SI (match_operand:SF 0 "fp_arith_reg_operand" "f")
+		 (match_operand:SF 1 "fp_arith_reg_operand" "f")))]
+  "TARGET_SH2E && ! (TARGET_SH4 || TARGET_SH2A_SINGLE)"
+  "fcmp/gt\t%1,%0\;bt\t0f\;fcmp/gt\t%0,%1\\n0:"
+  [(set_attr "length" "6")])
+
+(define_insn "*cmporderedsf_t"
+  [(set (reg:SI T_REG)
+	(ordered:SI (match_operand:SF 0 "fp_arith_reg_operand" "f")
+		    (match_operand:SF 1 "fp_arith_reg_operand" "f")))]
+  "TARGET_SH2E && ! (TARGET_SH4 || TARGET_SH2A_SINGLE)"
+  "fcmp/eq\t%0,%0\;bf\t0f\;fcmp/eq\t%1,%1\\n0:"
+  [(set_attr "length" "6")])
+
 
 (define_insn "cmpgtsf_t_i4"
   [(set (reg:SI T_REG)
@@ -10380,7 +10936,7 @@
    (use (match_operand:PSI 2 "fpscr_operand" "c"))]
   "(TARGET_SH4 || TARGET_SH2A_SINGLE)"
   "fcmp/gt	%1,%0"
-  [(set_attr "type" "fp")
+  [(set_attr "type" "fp_cmp")
    (set_attr "fp_mode" "single")])
 
 (define_insn "cmpeqsf_t_i4"
@@ -10390,7 +10946,7 @@
    (use (match_operand:PSI 2 "fpscr_operand" "c"))]
   "(TARGET_SH4 || TARGET_SH2A_SINGLE)"
   "fcmp/eq	%1,%0"
-  [(set_attr "type" "fp")
+  [(set_attr "type" "fp_cmp")
    (set_attr "fp_mode" "single")])
 
 (define_insn "*ieee_ccmpeqsf_t_4"
@@ -10404,6 +10960,26 @@
   [(set_attr "length" "4")
    (set_attr "fp_mode" "single")])
 
+(define_insn "*cmpltgtsf_t_4"
+  [(set (reg:SI T_REG)
+	(ltgt:SI (match_operand:SF 0 "fp_arith_reg_operand" "f")
+		 (match_operand:SF 1 "fp_arith_reg_operand" "f")))
+   (use (match_operand:PSI 2 "fpscr_operand" "c"))]
+  "TARGET_SH4 || TARGET_SH2A_SINGLE"
+  "fcmp/gt\t%1,%0\;bt\t0f\;fcmp/gt\t%0,%1\\n0:"
+  [(set_attr "length" "6")
+   (set_attr "fp_mode" "single")])
+
+(define_insn "*cmporderedsf_t_4"
+  [(set (reg:SI T_REG)
+	(ordered:SI (match_operand:SF 0 "fp_arith_reg_operand" "f")
+		    (match_operand:SF 1 "fp_arith_reg_operand" "f")))
+   (use (match_operand:PSI 2 "fpscr_operand" "c"))]
+  "TARGET_SH4 || TARGET_SH2A_SINGLE"
+  "fcmp/eq\t%0,%0\;bf\t0f\;fcmp/eq\t%1,%1\\n0:"
+  [(set_attr "length" "6")
+   (set_attr "fp_mode" "single")])
+
 (define_insn "cmpeqsf_media"
   [(set (match_operand:SI 0 "register_operand" "=r")
 	(eq:SI (match_operand:SF 1 "fp_arith_reg_operand" "f")
@@ -10646,11 +11222,39 @@
   [(set_attr "type" "fmove")
    (set_attr "fp_mode" "single")])
 
+(define_expand "abssc2"
+  [(set (match_operand:SF 0 "fp_arith_reg_operand" "")
+	(abs:SF (match_operand:SC 1 "fp_arith_reg_operand" "")))]
+  "TARGET_OSFP && ! TARGET_SH2E"
+  "
+{
+  expand_sfunc_unop (SCmode, &gen_abssc2_i3, \"__hypotf\", ABS, operands);
+  DONE;
+}")
+
+(define_insn "abssc2_i3"
+  [(set (match_operand:SF 0 "arith_reg_dest" "=z")
+	(abs:SF (reg:SC R4_REG)))
+   (clobber (reg:SI MACH_REG))
+   (clobber (reg:SI MACL_REG))
+   (clobber (reg:SI T_REG))
+   (clobber (reg:SI PR_REG))
+   (clobber (reg:SI R1_REG))
+   (clobber (reg:SI R2_REG))
+   (clobber (reg:SI R3_REG))
+   (clobber (reg:SI R4_REG))
+   (clobber (reg:SI R5_REG))
+   (use (match_operand:SI 1 "arith_reg_operand" "r"))]
+  "TARGET_OSFP && ! TARGET_SH2E"
+  "jsr	@%1%#"
+  [(set_attr "type" "sfunc")
+   (set_attr "needs_delay_slot" "yes")])
+
 (define_expand "adddf3"
   [(set (match_operand:DF 0 "fp_arith_reg_operand" "")
 	(plus:DF (match_operand:DF 1 "fp_arith_reg_operand" "")
 		 (match_operand:DF 2 "fp_arith_reg_operand" "")))]
-  "(TARGET_SH4 || TARGET_SH2A_DOUBLE) || TARGET_SHMEDIA_FPU"
+  "TARGET_FPU_DOUBLE || (TARGET_SH3 && TARGET_OSFP)"
   "
 {
   if (TARGET_SH4 || TARGET_SH2A_DOUBLE)
@@ -10658,6 +11262,12 @@
       expand_df_binop (&gen_adddf3_i, operands);
       DONE;
     }
+  else if (TARGET_SH3 && TARGET_OSFP)
+    {
+      expand_sfunc_binop (DFmode, &gen_adddf3_i3_wrap, \"__adddf3\", PLUS,
+			  operands);
+      DONE;
+    }
 }")
 
 (define_insn "*adddf3_media"
@@ -10678,6 +11288,30 @@
   [(set_attr "type" "dfp_arith")
    (set_attr "fp_mode" "double")])
 
+(define_expand "adddf3_i3_wrap"
+  [(match_operand:DF 0 "" "") (match_operand:SI 1 "" "")]
+  "TARGET_SH3"
+  "
+{
+  emit_insn (gen_adddf3_i3 (operands[1]));
+  emit_move_insn (operands[0], gen_rtx_REG (DFmode, R0_REG));
+  DONE;
+}")
+
+(define_insn "adddf3_i3"
+  [(set (reg:DF R0_REG)
+	(plus:DF (reg:DF R4_REG) (reg:DF R6_REG)))
+   (clobber (reg:SI T_REG))
+   (clobber (reg:SI PR_REG))
+   (clobber (reg:DI R2_REG))
+   (clobber (reg:DF R4_REG))
+   (clobber (reg:DF R6_REG))
+   (use (match_operand:SI 0 "arith_reg_operand" "r"))]
+  "TARGET_SH3"
+  "jsr	@%0%#"
+  [(set_attr "type" "sfunc")
+   (set_attr "needs_delay_slot" "yes")])
+
 (define_expand "subdf3"
   [(set (match_operand:DF 0 "fp_arith_reg_operand" "")
 	(minus:DF (match_operand:DF 1 "fp_arith_reg_operand" "")
@@ -10714,7 +11348,7 @@
   [(set (match_operand:DF 0 "fp_arith_reg_operand" "")
 	(mult:DF (match_operand:DF 1 "fp_arith_reg_operand" "")
 		 (match_operand:DF 2 "fp_arith_reg_operand" "")))]
-  "(TARGET_SH4 || TARGET_SH2A_DOUBLE) || TARGET_SHMEDIA_FPU"
+  "TARGET_FPU_DOUBLE || (TARGET_SH3 && TARGET_OSFP)"
   "
 {
   if (TARGET_SH4 || TARGET_SH2A_DOUBLE)
@@ -10722,6 +11356,12 @@
       expand_df_binop (&gen_muldf3_i, operands);
       DONE;
     }
+  else if (TARGET_SH3 && TARGET_OSFP)
+    {
+      expand_sfunc_binop (DFmode, &gen_muldf3_i3_wrap, \"__muldf3\", MULT,
+			  operands);
+      DONE;
+    }
 }")
 
 (define_insn "*muldf3_media"
@@ -10739,9 +11379,35 @@
    (use (match_operand:PSI 3 "fpscr_operand" "c"))]
   "(TARGET_SH4 || TARGET_SH2A_DOUBLE)"
   "fmul	%2,%0"
-  [(set_attr "type" "dfp_arith")
+  [(set_attr "type" "dfp_mul")
    (set_attr "fp_mode" "double")])
 
+(define_expand "muldf3_i3_wrap"
+  [(match_operand:DF 0 "" "") (match_operand:SI 1 "" "")]
+  "TARGET_SH3"
+  "
+{
+  emit_insn (gen_muldf3_i3 (operands[1]));
+  emit_move_insn (operands[0], gen_rtx_REG (DFmode, R0_REG));
+  DONE;
+}")
+
+(define_insn "muldf3_i3"
+  [(set (reg:DF R0_REG)
+	(mult:DF (reg:DF R4_REG) (reg:DF R6_REG)))
+   (clobber (reg:SI MACH_REG))
+   (clobber (reg:SI MACL_REG))
+   (clobber (reg:SI T_REG))
+   (clobber (reg:SI PR_REG))
+   (clobber (reg:DI R2_REG))
+   (clobber (reg:DF R4_REG))
+   (clobber (reg:DF R6_REG))
+   (use (match_operand:SI 0 "arith_reg_operand" "r"))]
+  "TARGET_SH3"
+  "jsr	@%0%#"
+  [(set_attr "type" "sfunc")
+   (set_attr "needs_delay_slot" "yes")])
+
 (define_expand "divdf3"
   [(set (match_operand:DF 0 "fp_arith_reg_operand" "")
 	(div:DF (match_operand:DF 1 "fp_arith_reg_operand" "")
@@ -10871,6 +11537,73 @@
 ;; 	      (use (match_dup 2))])
 ;;    (set (match_dup 0) (reg:SI FPUL_REG))])
 
+(define_insn "cmpnedf_i1"
+  [(set (match_operand:CC_FP_NE 0 "register_operand" "=z")
+	(compare:CC_FP_NE (reg:DF R4_REG) (reg:DF R6_REG)))
+   (clobber (reg:SI T_REG))
+   (clobber (reg:SI PR_REG))
+   (clobber (reg:SI R1_REG))
+   (use (match_operand:SI 1 "arith_reg_operand" "r"))]
+  "TARGET_SH1 && ! TARGET_SH2E"
+  "jsr	@%1%#"
+  [(set_attr "type" "sfunc")
+   (set_attr "needs_delay_slot" "yes")])
+
+(define_insn "cmpgtdf_i1"
+  [(set (match_operand:CC_FP_GT 0 "register_operand" "=z")
+	(compare:CC_FP_GT (reg:DF R4_REG) (reg:DF R6_REG)))
+   (clobber (reg:SI T_REG))
+   (clobber (reg:SI PR_REG))
+   (clobber (reg:SI R1_REG))
+   (use (match_operand:SI 1 "arith_reg_operand" "r"))]
+  "TARGET_SH1_SOFTFP"
+  "jsr	@%1%#"
+  [(set_attr "type" "sfunc")
+   (set_attr "needs_delay_slot" "yes")])
+
+(define_insn "cmpunltdf_i1"
+  [(set (match_operand:CC_FP_UNLT 0 "register_operand" "=z")
+	(compare:CC_FP_UNLT (reg:DF R4_REG) (reg:DF R6_REG)))
+   (clobber (reg:SI T_REG))
+   (clobber (reg:SI PR_REG))
+   (clobber (reg:SI R1_REG))
+   (use (match_operand:SI 1 "arith_reg_operand" "r"))]
+  "TARGET_SH1_SOFTFP"
+  "jsr	@%1%#"
+  [(set_attr "type" "sfunc")
+   (set_attr "needs_delay_slot" "yes")])
+
+(define_insn "cmpeqdf_i1_finite"
+  [(set (reg:SI T_REG)
+	(eq:SI (match_operand:DF 0 "arith_reg_operand" "r")
+	       (match_operand:DF 1 "arith_reg_operand" "r")))
+   (clobber (match_scratch:SI 2 "=&r"))]
+  "TARGET_SH1_SOFTFP && flag_finite_math_only"
+  "cmp/eq\t%R0,%R1\;mov\t%S0,%2\;bf\t0f\;cmp/eq\t%S0,%S1\;bt\t0f\;or\t%S1,%2\;add\t%2,%2\;or\t%R0,%2\;tst\t%2,%2\\n0:"
+  [(set_attr "length" "18")])
+
+(define_insn "cmpundf_i1"
+  [(set (reg:SI T_REG)
+	(unordered:SI (match_operand:DF 0 "arith_reg_operand" "r,r")
+		      (match_operand:DF 1 "arith_reg_operand" "r,r")))
+   (use (match_operand:SI 2 "arith_reg_operand" "r,r"))
+   (clobber (match_scratch:SI 3 "=0,&r"))]
+  "TARGET_SH1 && ! TARGET_SH2E"
+  "not\t%S0,%3\;tst\t%2,%3\;not\t%S1,%3\;bt\t0f\;tst\t%2,%3\;0:"
+  [(set_attr "length" "10")])
+
+;; ??? This is a lot of code with a lot of branches; a library function
+;; might be better.
+(define_insn "cmpuneqdf_i1"
+  [(set (reg:SI T_REG)
+	(uneq:SI (match_operand:DF 0 "arith_reg_operand" "r")
+		 (match_operand:DF 1 "arith_reg_operand" "r")))
+   (use (match_operand:SI 2 "arith_reg_operand" "r"))
+   (clobber (match_scratch:SI 3 "=&r"))]
+  "TARGET_SH1_SOFTFP"
+  "not\t%S0,%3\;tst\t%2,%3\;not\t%S1,%3\;bt\t0f\;tst\t%2,%3\;bt\t0f\;cmp/eq\t%R0,%R1\; bf\t0f\;cmp/eq\t%S0,%S1\;bt\t0f\;mov\t%S0,%3\;or\t%S1,%3\;add\t%3,%3\;or\t%R0,%3\;tst\t%3,%3\\n0:"
+  [(set_attr "length" "30")])
+
 (define_insn "cmpgtdf_t"
   [(set (reg:SI T_REG)
 	(gt:SI (match_operand:DF 0 "arith_reg_operand" "f")
@@ -10902,6 +11635,26 @@
   [(set_attr "length" "4")
    (set_attr "fp_mode" "double")])
 
+(define_insn "*cmpltgtdf_t"
+  [(set (reg:SI T_REG)
+	(ltgt:SI (match_operand:DF 0 "fp_arith_reg_operand" "f")
+		 (match_operand:DF 1 "fp_arith_reg_operand" "f")))
+   (use (match_operand:PSI 2 "fpscr_operand" "c"))]
+  "TARGET_SH4 || TARGET_SH2A_DOUBLE"
+  "fcmp/gt\t%1,%0\;bt\t0f\;fcmp/gt\t%0,%1\\n0:"
+  [(set_attr "length" "6")
+   (set_attr "fp_mode" "double")])
+
+(define_insn "*cmpordereddf_t_4"
+  [(set (reg:SI T_REG)
+	(ordered:SI (match_operand:DF 0 "fp_arith_reg_operand" "f")
+		    (match_operand:DF 1 "fp_arith_reg_operand" "f")))
+   (use (match_operand:PSI 2 "fpscr_operand" "c"))]
+  "TARGET_SH4 || TARGET_SH2A_SINGLE"
+  "fcmp/eq\t%0,%0\;bf\t0f\;fcmp/eq\t%1,%1\\n0:"
+  [(set_attr "length" "6")
+   (set_attr "fp_mode" "double")])
+
 (define_insn "cmpeqdf_media"
   [(set (match_operand:SI 0 "register_operand" "=r")
 	(eq:SI (match_operand:DF 1 "fp_arith_reg_operand" "f")
@@ -11036,7 +11789,7 @@
 (define_expand "extendsfdf2"
   [(set (match_operand:DF 0 "fp_arith_reg_operand" "")
 	(float_extend:DF (match_operand:SF 1 "fpul_operand" "")))]
-  "(TARGET_SH4 || TARGET_SH2A_DOUBLE) || TARGET_SHMEDIA_FPU"
+  "TARGET_FPU_DOUBLE || (TARGET_SH3 && TARGET_OSFP) || TARGET_SHMEDIA_FPU"
   "
 {
   if (TARGET_SH4 || TARGET_SH2A_DOUBLE)
@@ -11045,6 +11798,18 @@
 					get_fpscr_rtx ()));
       DONE;
     }
+  if (TARGET_SH2E && TARGET_OSFP)
+    {
+      expand_sfunc_unop (SFmode, &gen_extendsfdf2_i2e, \"__extendsfdf2\",
+ 		 FLOAT_EXTEND, operands);
+      DONE;
+    }
+  else if (TARGET_SH1 && TARGET_OSFP)
+    {
+      expand_sfunc_unop (SFmode, &gen_extendsfdf2_i1, \"__extendsfdf2\",
+			 FLOAT_EXTEND, operands);
+      DONE;
+    }
 }")
 
 (define_insn "*extendsfdf2_media"
@@ -11063,10 +11828,76 @@
   [(set_attr "type" "fp")
    (set_attr "fp_mode" "double")])
 
+;; ??? In order to use this efficiently, we'd have to have an extra
+;; register class for r0 and r1 - and that would cause repercussions in
+;; register allocation elsewhere.  So just say we clobber r0 / r1, and
+;; that we can use an arbitrary target.  */
+(define_insn_and_split "extendsfdf2_i1"
+  [(set (match_operand:DF 0 "arith_reg_dest" "=r")
+	(float_extend:DF (reg:SF R4_REG)))
+   (clobber (reg:SI T_REG))
+   (clobber (reg:SI PR_REG))
+   (clobber (reg:SI R0_REG))
+   (clobber (reg:SI R1_REG))
+   (clobber (reg:SI R2_REG))
+   (clobber (reg:SI R3_REG))
+   (use (match_operand:SI 1 "arith_reg_operand" "r"))]
+  "TARGET_SH1_SOFTFP && !TARGET_SH2E"
+  "#"
+  "&& reload_completed"
+  [(set (match_dup 0) (reg:DF R0_REG))]
+  "emit_insn (gen_extendsfdf2_i1_r0 (operands[1]));"
+  [(set_attr "type" "sfunc")])
+
+(define_insn "extendsfdf2_i1_r0"
+  [(set (reg:DF R0_REG) (float_extend:DF (reg:SF R4_REG)))
+   (clobber (reg:SI T_REG))
+   (clobber (reg:SI PR_REG))
+   (clobber (reg:SI R2_REG))
+   (clobber (reg:SI R3_REG))
+   (use (match_operand:SI 0 "arith_reg_operand" "r"))]
+  "TARGET_SH1_SOFTFP && !TARGET_SH2E"
+  "jsr	@%0%#"
+  [(set_attr "type" "sfunc")
+   (set_attr "needs_delay_slot" "yes")])
+
+(define_insn_and_split "extendsfdf2_i2e"
+  [(set (match_operand:DF 0 "arith_reg_dest" "=r")
+	(float_extend:DF (reg:SF FR4_REG)))
+   (clobber (reg:SI T_REG))
+   (clobber (reg:SI PR_REG))
+   (clobber (reg:SI R0_REG))
+   (clobber (reg:SI R1_REG))
+   (clobber (reg:SI R2_REG))
+   (clobber (reg:SI R3_REG))
+   (clobber (reg:SI R4_REG))
+   (clobber (reg:SI FPUL_REG))
+   (use (match_operand:SI 1 "arith_reg_operand" "r"))]
+  "TARGET_SH1_SOFTFP && TARGET_SH2E"
+  "#"
+  "&& reload_completed"
+  [(set (match_dup 0) (reg:DF R0_REG))]
+  "emit_insn (gen_extendsfdf2_i2e_r0 (operands[1]));"
+  [(set_attr "type" "sfunc")])
+
+(define_insn "extendsfdf2_i2e_r0"
+  [(set (reg:DF R0_REG) (float_extend:DF (reg:SF FR4_REG)))
+   (clobber (reg:SI T_REG))
+   (clobber (reg:SI PR_REG))
+   (clobber (reg:SI R2_REG))
+   (clobber (reg:SI R3_REG))
+   (clobber (reg:SI R4_REG))
+   (clobber (reg:SI FPUL_REG))
+   (use (match_operand:SI 0 "arith_reg_operand" "r"))]
+  "TARGET_SH1_SOFTFP && TARGET_SH2E"
+  "jsr	@%0%#"
+  [(set_attr "type" "sfunc")
+   (set_attr "needs_delay_slot" "yes")])
+
 (define_expand "truncdfsf2"
   [(set (match_operand:SF 0 "fpul_operand" "")
 	(float_truncate:SF (match_operand:DF 1 "fp_arith_reg_operand" "")))]
-  "(TARGET_SH4 || TARGET_SH2A_DOUBLE) || TARGET_SHMEDIA_FPU"
+  "TARGET_FPU_DOUBLE || (TARGET_SH3 && TARGET_OSFP) || TARGET_SHMEDIA_FPU"
   "
 {
   if (TARGET_SH4 || TARGET_SH2A_DOUBLE)
@@ -11075,6 +11906,18 @@
 				       get_fpscr_rtx ()));
       DONE;
     }
+  else if (TARGET_SH2E && TARGET_OSFP)
+    {
+      expand_sfunc_unop (DFmode, &gen_truncdfsf2_i2e, \"__truncdfsf2\",
+			 FLOAT_TRUNCATE, operands);
+      DONE;
+    }
+  else if (TARGET_SH1 && TARGET_OSFP)
+    {
+      expand_sfunc_unop (DFmode, &gen_truncdfsf2_i1, \"__truncdfsf2\",
+			 FLOAT_TRUNCATE, operands);
+      DONE;
+    }
 }")
 
 (define_insn "*truncdfsf2_media"
@@ -11092,6 +11935,37 @@
   "fcnvds  %1,%0"
   [(set_attr "type" "fp")
    (set_attr "fp_mode" "double")])
+
+(define_insn "truncdfsf2_i1"
+  [(set (match_operand:SF 0 "arith_reg_dest" "=z")
+	(float_truncate:SF (reg:DF R4_REG)))
+   (clobber (reg:SI T_REG))
+   (clobber (reg:SI PR_REG))
+   (clobber (reg:SI R1_REG))
+   (clobber (reg:SI R2_REG))
+   (clobber (reg:SI R3_REG))
+   (use (match_operand:SI 1 "arith_reg_operand" "r"))]
+  "TARGET_SH1_SOFTFP && !TARGET_SH2E"
+  "jsr	@%1%#"
+  [(set_attr "type" "sfunc")
+   (set_attr "needs_delay_slot" "yes")])
+
+(define_insn "truncdfsf2_i2e"
+  [(set (match_operand:SF 0 "arith_reg_dest" "=w")
+	(float_truncate:SF (reg:DF R4_REG)))
+   (clobber (reg:SI T_REG))
+   (clobber (reg:SI PR_REG))
+   (clobber (reg:SI FPUL_REG))
+   (clobber (reg:SI R0_REG))
+   (clobber (reg:SI R1_REG))
+   (clobber (reg:SI R2_REG))
+   (clobber (reg:SI R3_REG))
+   (use (match_operand:SI 1 "arith_reg_operand" "r"))]
+  "TARGET_SH1_SOFTFP && TARGET_SH2E"
+  "jsr	@%1%#"
+  [(set_attr "type" "sfunc")
+   (set_attr "needs_delay_slot" "yes")])
+
 
 ;; Bit field extract patterns.  These give better code for packed bitfields,
 ;; because they allow auto-increment addresses to be generated.
diff -Naur gcc-4.2.1/gcc/config/sh/sh-modes.def gcc-4.2.1-stm/gcc/config/sh/sh-modes.def
--- gcc-4.2.1/gcc/config/sh/sh-modes.def	2006-08-06 20:43:03.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/sh-modes.def	2007-08-10 13:08:59.000000000 +0100
@@ -23,6 +23,11 @@
 /* PDI mode is used to represent a function address in a target register.  */
 PARTIAL_INT_MODE (DI);
 
+/* For software floating point comparisons.  */
+CC_MODE (CC_FP_NE);
+CC_MODE (CC_FP_GT);
+CC_MODE (CC_FP_UNLT);
+
 /* Vector modes.  */
 VECTOR_MODE  (INT, QI, 2);    /*                 V2QI */
 VECTOR_MODES (INT, 4);        /*            V4QI V2HI */
diff -Naur gcc-4.2.1/gcc/config/sh/sh.opt gcc-4.2.1-stm/gcc/config/sh/sh.opt
--- gcc-4.2.1/gcc/config/sh/sh.opt	2006-03-27 15:11:14.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/sh.opt	2007-08-10 13:08:59.000000000 +0100
@@ -1,6 +1,7 @@
 ; Options for the SH port of the compiler.
 
 ; Copyright (C) 2005, 2006 Free Software Foundation, Inc.
+; Copyright (c) 2006  STMicroelectronics.
 ;
 ; This file is part of GCC.
 ;
@@ -22,7 +23,7 @@
 ;; Used for various architecture options.
 Mask(SH_E)
 
-;; Set if the default precision of th FPU is single.
+;; Set if the default precision of the FPU is single.
 Mask(FPU_SINGLE)
 
 ;; Set if we should generate code using type 2A insns.
@@ -57,11 +58,11 @@
 Generate SH2a FPU-less code
 
 m2a-single
-Target RejectNegative Condition (SUPPORT_SH2A_SINGLE)
+Target RejectNegative Condition(SUPPORT_SH2A_SINGLE)
 Generate default single-precision SH2a code
 
 m2a-single-only
-Target RejectNegative Condition (SUPPORT_SH2A_SINGLE_ONLY)
+Target RejectNegative Condition(SUPPORT_SH2A_SINGLE_ONLY)
 Generate only single-precision SH2a code
 
 m2e
@@ -88,10 +89,33 @@
 Target RejectNegative Condition(SUPPORT_SH4)
 Generate SH4-200 code
 
+;; TARGET_SH4_300 indicates if we have the ST40-300 instruction set and
+;; pipeline - irrespective of ABI.
+m4-300
+Target RejectNegative Condition(SUPPORT_SH4) Var(TARGET_SH4_300)
+Generate SH4-300 code
+
 m4-nofpu
 Target RejectNegative Condition(SUPPORT_SH4_NOFPU)
 Generate SH4 FPU-less code
 
+m4-100-nofpu
+Target RejectNegative Condition(SUPPORT_SH4_NOFPU)
+Generate SH4-100 FPU-less code
+
+m4-200-nofpu
+Target RejectNegative Condition(SUPPORT_SH4_NOFPU)
+Generate SH4-200 FPU-less code
+
+m4-300-nofpu
+Target RejectNegative Condition(SUPPORT_SH4_NOFPU) Var(TARGET_SH4_300) VarExists
+Generate SH4-300 FPU-less code
+
+m4-340
+Target RejectNegative Condition(SUPPORT_SH4_NOFPU) Var(TARGET_SH4_300) VarExists
+Generate code for SH4 340 series (MMU/FPU-less)
+;; passes -isa=sh4-nommu-nofpu to the assembler.
+
 m4-400
 Target RejectNegative Condition(SUPPORT_SH4_NOFPU)
 Generate code for SH4 400 series (MMU/FPU-less)
@@ -114,6 +138,10 @@
 Target RejectNegative Condition(SUPPORT_SH4_SINGLE)
 Generate default single-precision SH4-200 code
 
+m4-300-single
+Target RejectNegative Condition(SUPPORT_SH4_SINGLE) Var(TARGET_SH4_300) VarExists
+Generate default single-precision SH4-300 code
+
 m4-single-only
 Target RejectNegative Condition(SUPPORT_SH4_SINGLE_ONLY)
 Generate only single-precision SH4 code
@@ -126,6 +154,10 @@
 Target RejectNegative Condition(SUPPORT_SH4_SINGLE_ONLY)
 Generate only single-precision SH4-200 code
 
+m4-300-single-only
+Target RejectNegative Condition(SUPPORT_SH4_SINGLE_ONLY) Var(TARGET_SH4_300) VarExists
+Generate only single-precision SH4-300 code
+
 m4a
 Target RejectNegative Mask(SH4A) Condition(SUPPORT_SH4A)
 Generate SH4a code
@@ -174,6 +206,10 @@
 Target Report Mask(ADJUST_UNROLL) Condition(SUPPORT_ANY_SH5)
 Throttle unrolling to avoid thrashing target registers unless the unroll benefit outweighs this
 
+malign-small-blocks=
+Target RejectNegative Joined UInteger Var(sh_align_small_blocks) Init(16)
+Honor align-jump-loops for basic block bigger than number of instructions.
+
 mb
 Target Report RejectNegative InverseMask(LITTLE_ENDIAN)
 Generate code in big endian mode
@@ -182,6 +218,22 @@
 Target Report RejectNegative Mask(BIGTABLE)
 Generate 32-bit offsets in switch tables
 
+mbranch-cost=
+Target RejectNegative Joined UInteger Var(sh_branch_cost) Init(-1)
+Cost to assume for a branch insn
+
+mcbranchdi
+Target Var(TARGET_CBRANCHDI4)
+Enable cbranchdi4 pattern
+
+mexpand-cbranchdi
+Target Var(TARGET_EXPAND_CBRANCHDI4)
+Expand cbranchdi4 pattern early into separate comparisons and branches.
+
+mcmpeqdi
+Target Var(TARGET_CMPEQDI_T)
+Emit cmpeqdi_t pattern even when -mcbranchdi and -mexpand-cbranchdi are in effect.
+
 mcut2-workaround
 Target RejectNegative Var(TARGET_SH5_CUT2_WORKAROUND)
 Enable SH5 cut2 workaround
@@ -192,7 +244,7 @@
 
 mdiv=
 Target RejectNegative Joined Var(sh_div_str) Init("")
-Division strategy, one of: call, call2, fp, inv, inv:minlat, inv20u, inv20l, inv:call, inv:call2, inv:fp call-div1 call-fp call-table
+Division strategy, one of: call, call2, fp, inv, inv:minlat, inv20u, inv20l, inv:call, inv:call2, inv:fp, call-div1, call-fp, call-table
 
 mdivsi3_libfunc=
 Target RejectNegative Joined Var(sh_divsi3_libfunc) Init("")
@@ -201,6 +253,10 @@
 mfmovd
 Target RejectNegative Mask(FMOVD) Undocumented
 
+mfused-madd
+Target Var(TARGET_FMAC)
+Enable the use of the fused floating point multiply-accumulate operation
+
 mgettrcost=
 Target RejectNegative Joined UInteger Var(sh_gettrcost) Init(-1)
 Cost to assume for gettr insn
@@ -217,6 +273,10 @@
 Target Report Mask(ALLOW_INDEXED_ADDRESS) Condition(SUPPORT_ANY_SH5_32MEDIA)
 Enable the use of the indexed addressing mode for SHmedia32/SHcompact
 
+minline-ic_invalidate
+Target Report Var(TARGET_INLINE_IC_INVALIDATE)
+inline code to invalidate instruction cache entries after setting up nested function trampolines
+
 minvalid-symbols
 Target Report Mask(INVALID_SYMBOLS) Condition(SUPPORT_ANY_SH5)
 Assume symbols might be invalid
@@ -229,6 +289,10 @@
 Target Report RejectNegative Mask(LITTLE_ENDIAN)
 Generate code in little endian mode
 
+mlate-r0r3-to-reg-mul
+Target RejectNegative Var(TARGET_R0R3_TO_REG_MUL, 1) VarExists
+Assume availability of integer multiply instruction (src only opd in r0-r3), but only try to use this instruction after register allocation.
+
 mnomacsave
 Target Report RejectNegative Mask(NOMACSAVE)
 Mark MAC register as call-clobbered
@@ -247,6 +311,10 @@
 Target Report Mask(PT_FIXED) Condition(SUPPORT_ANY_SH5)
 Assume pt* instructions won't trap
 
+mr0r3-to-reg-mul
+Target Var(TARGET_R0R3_TO_REG_MUL, 2) Init(-1)
+Assume availability of integer multiply instruction (src only opd in r0-r3)
+
 mrelax
 Target Report RejectNegative Mask(RELAX)
 Shorten address references during linking
@@ -271,4 +339,4 @@
 ;; zero-offset branches have zero latency.  Needs some benchmarking.
 mpretend-cmove
 Target Var(TARGET_PRETEND_CMOVE)
-Pretend a branch-around-a-move is a conditional move.
+Pretend a branch-around-a-move is a conditional move.
\ No newline at end of file
diff -Naur gcc-4.2.1/gcc/config/sh/sh-protos.h gcc-4.2.1-stm/gcc/config/sh/sh-protos.h
--- gcc-4.2.1/gcc/config/sh/sh-protos.h	2005-11-24 18:55:53.000000000 +0000
+++ gcc-4.2.1-stm/gcc/config/sh/sh-protos.h	2007-08-10 13:08:59.000000000 +0100
@@ -4,6 +4,7 @@
    Free Software Foundation, Inc.
    Contributed by Steve Chamberlain (sac@cygnus.com).
    Improved by Jim Wilson (wilson@cygnus.com).
+   Copyright (c) 2006  STMicroelectronics.
 
 This file is part of GCC.
 
@@ -26,8 +27,13 @@
 #define GCC_SH_PROTOS_H
 
 enum sh_function_kind {
-  /* A function with normal C ABI  */
+  /* A function with normal C ABI, or an SH1..SH4 sfunc that may resolved via
+     a PLT.  */
   FUNCTION_ORDINARY,
+  /* A function that is a bit large to put it in every calling dso, but that's
+     typically used often enough so that calling via GOT makes sense for
+     speed.  */
+  SFUNC_FREQUENT,
   /* A special function that guarantees that some otherwise call-clobbered
      registers are not clobbered.  These can't go through the SH5 resolver,
      because it only saves argument passing registers.  */
@@ -54,6 +60,7 @@
 extern const char *output_far_jump (rtx, rtx);
 
 extern struct rtx_def *sfunc_uses_reg (rtx);
+extern int sh_jump_align (rtx);
 extern int barrier_align (rtx);
 extern int sh_loop_align (rtx);
 extern int fp_zero_operand (rtx);
@@ -69,6 +76,10 @@
 extern void output_pic_addr_const (FILE *, rtx);
 extern int expand_block_move (rtx *);
 extern int prepare_move_operands (rtx[], enum machine_mode mode);
+extern enum rtx_code prepare_cbranch_operands (rtx *, enum machine_mode mode,
+					       enum rtx_code comparison);
+extern void expand_cbranchsi4 (rtx *operands, enum rtx_code comparison, int);
+extern bool expand_cbranchdi4 (rtx *operands, enum rtx_code comparison);
 extern void from_compare (rtx *, int);
 extern int shift_insns_rtx (rtx);
 extern void gen_ashift (int, int, rtx);
@@ -110,6 +121,10 @@
 extern void expand_df_unop (rtx (*)(rtx, rtx, rtx), rtx *);
 extern void expand_df_binop (rtx (*)(rtx, rtx, rtx, rtx), rtx *);
 extern void expand_fp_branch (rtx (*)(void), rtx (*)(void));
+extern void expand_sfunc_unop (enum machine_mode, rtx (*) (rtx, rtx),
+			       const char *, enum rtx_code code, rtx *);
+extern void expand_sfunc_binop (enum machine_mode, rtx (*) (rtx, rtx),
+				const char *, enum rtx_code code, rtx *);
 extern int sh_insn_length_adjustment (rtx);
 extern int sh_can_redirect_branch (rtx, rtx);
 extern void sh_expand_unop_v2sf (enum rtx_code, rtx, rtx);
@@ -128,6 +143,8 @@
 extern int sh_media_register_for_return (void);
 extern void sh_expand_prologue (void);
 extern void sh_expand_epilogue (bool);
+extern void sh_expand_float_cbranch (rtx operands[4]);
+extern void sh_expand_float_scc (rtx operands[4]);
 extern int sh_need_epilogue (void);
 extern void sh_set_return_address (rtx, rtx);
 extern int initial_elimination_offset (int, int);
@@ -147,6 +164,7 @@
 #ifdef HARD_CONST
 extern void fpscr_set_from_mem (int, HARD_REG_SET);
 #endif
+extern void emit_fpu_flip (void);
 
 extern void sh_pr_interrupt (struct cpp_reader *);
 extern void sh_pr_trapa (struct cpp_reader *);
@@ -164,12 +182,17 @@
 extern int shmedia_cleanup_truncate (rtx *, void *);
 
 extern int sh_contains_memref_p (rtx);
+extern int sh_loads_bankedreg_p (rtx);
 extern rtx shmedia_prepare_call_address (rtx fnaddr, int is_sibcall);
 struct secondary_reload_info;
 extern enum reg_class sh_secondary_reload (bool, rtx, enum reg_class,
 					   enum machine_mode,
 					   struct secondary_reload_info *);
 
+extern bool sh_cfun_trap_exit_p (void);
+
+extern int sh_match_adjust (rtx, int);
+
 #endif /* ! GCC_SH_PROTOS_H */
 
 #ifdef SYMBIAN
diff -Naur gcc-4.2.1/gcc/config/sh/superh.h gcc-4.2.1-stm/gcc/config/sh/superh.h
--- gcc-4.2.1/gcc/config/sh/superh.h	2006-05-28 18:46:46.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/superh.h	2007-08-10 13:08:59.000000000 +0100
@@ -1,5 +1,6 @@
 /* Definitions of target machine for gcc for Super-H using sh-superh-elf.
    Copyright (C) 2001, 2006 Free Software Foundation, Inc.
+   Copyright (c) 2006  STMicroelectronics.
 
 This file is part of GNU CC.
 
@@ -75,17 +76,17 @@
    on newlib and provide the runtime support */
 #undef SUBTARGET_CPP_SPEC
 #define SUBTARGET_CPP_SPEC \
-"-D__EMBEDDED_CROSS__ %{m4-100*:-D__SH4_100__} %{m4-200*:-D__SH4_200__} %{m4-400:-D__SH4_400__} %{m4-500:-D__SH4_500__} \
+"-D__EMBEDDED_CROSS__ %{m4-100*:-D__SH4_100__} %{m4-200*:-D__SH4_200__} %{m4-300*:-D__SH4_300__} %{m4-340:-D__SH4_340__} %{m4-400:-D__SH4_400__} %{m4-500:-D__SH4_500__} \
 %(cppruntime)"
 
 /* Override the SUBTARGET_ASM_SPEC to add the runtime support */
 #undef SUBTARGET_ASM_SPEC
-#define SUBTARGET_ASM_SPEC "%{m4-100*|m4-200*:-isa=sh4} %{m4-400:-isa=sh4-nommu-nofpu} %{m4-500:-isa=sh4-nofpu} %(asruntime)"
+#define SUBTARGET_ASM_SPEC "%{m4-100*|m4-200*:-isa=sh4} %{m4-400|m4-340:-isa=sh4-nommu-nofpu} %{m4-500:-isa=sh4-nofpu} %(asruntime)"
 
 /* Override the SUBTARGET_ASM_RELAX_SPEC so it doesn't interfere with the
    runtime support by adding -isa=sh4 in the wrong place.  */
 #undef SUBTARGET_ASM_RELAX_SPEC
-#define SUBTARGET_ASM_RELAX_SPEC "%{!m4-100*:%{!m4-200*:%{!m4-400:%{!m4-500:-isa=sh4}}}}"
+#define SUBTARGET_ASM_RELAX_SPEC "%{!m4-100*:%{!m4-200*:%{!m4-300*:%{!m4-340:%{!m4-400:%{!m4-500:-isa=sh4}}}}}}"
 
 /* Create the CC1_SPEC to add the runtime support */
 #undef CC1_SPEC
@@ -102,7 +103,7 @@
 /* Override STARTFILE_SPEC to add profiling and MMU support.  */
 #undef STARTFILE_SPEC
 #define STARTFILE_SPEC \
-  "%{!shared: %{!m4-400*: %{pg:gcrt1-mmu.o%s}%{!pg:crt1-mmu.o%s}}} \
-   %{!shared: %{m4-400*: %{pg:gcrt1.o%s}%{!pg:crt1.o%s}}} \
+  "%{!shared: %{!m4-400*:%{!m4-340*: %{pg:gcrt1-mmu.o%s}%{!pg:crt1-mmu.o%s}}}} \
+   %{!shared: %{m4-340*|m4-400*: %{pg:gcrt1.o%s}%{!pg:crt1.o%s}}} \
    crti.o%s \
-   %{!shared:crtbegin.o%s} %{shared:crtbeginS.o%s}"
+   %{!shared:crtbegin.o%s trap-handler.o%s} %{shared:crtbeginS.o%s}"
diff -Naur gcc-4.2.1/gcc/config/sh/t-elf gcc-4.2.1-stm/gcc/config/sh/t-elf
--- gcc-4.2.1/gcc/config/sh/t-elf	2006-04-21 09:10:42.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/t-elf	2007-08-10 13:08:59.000000000 +0100
@@ -1,5 +1,6 @@
 EXTRA_MULTILIB_PARTS= crt1.o crti.o crtn.o \
-	crtbegin.o crtend.o crtbeginS.o crtendS.o $(IC_EXTRA_PARTS) $(OPT_EXTRA_PARTS)
+	crtbegin.o crtend.o crtbeginS.o crtendS.o $(IC_EXTRA_PARTS) \
+	$(OPT_EXTRA_PARTS) trap-handler.o
 
 # Compile crtbeginS.o and crtendS.o with pic.
 CRTSTUFF_T_CFLAGS_S = -fPIC
diff -Naur gcc-4.2.1/gcc/config/sh/t-mlib-sh4-300 gcc-4.2.1-stm/gcc/config/sh/t-mlib-sh4-300
--- gcc-4.2.1/gcc/config/sh/t-mlib-sh4-300	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/t-mlib-sh4-300	2007-08-10 13:08:59.000000000 +0100
@@ -0,0 +1 @@
+ML_sh4_300=m4-300/
diff -Naur gcc-4.2.1/gcc/config/sh/trap-handler.c gcc-4.2.1-stm/gcc/config/sh/trap-handler.c
--- gcc-4.2.1/gcc/config/sh/trap-handler.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/trap-handler.c	2007-08-10 13:08:59.000000000 +0100
@@ -0,0 +1,10 @@
+void exit (int);
+
+void
+_superh_trap_handler (unsigned int trap_reason)
+{
+  exit(*(int*)0xff000024);  /* return EXPEVT */
+
+  /* in case exit returns ... */
+  while(1);
+}
diff -Naur gcc-4.2.1/gcc/config/sh/t-sh gcc-4.2.1-stm/gcc/config/sh/t-sh
--- gcc-4.2.1/gcc/config/sh/t-sh	2006-06-13 18:44:56.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/t-sh	2007-08-10 13:08:59.000000000 +0100
@@ -6,7 +6,9 @@
 LIB1ASMFUNCS = _ashiftrt _ashiftrt_n _ashiftlt _lshiftrt _movmem \
   _movmem_i4 _mulsi3 _sdivsi3 _sdivsi3_i4 _udivsi3 _udivsi3_i4 _set_fpscr \
   _div_table _udiv_qrnnd_16 \
+  _nesf2f _nedf2f _gtsf2t _gtdf2t _gesf2f _gedf2f \
   $(LIB1ASMFUNCS_CACHE)
+LIB1ASMFUNCS_CACHE = _ic_invalidate _ic_invalidate_array
 
 # We want fine grained libraries, so use the new code to build the
 # floating point emulation libraries.
@@ -38,11 +40,12 @@
 # is why sh2a and sh2a-single need their own multilibs.
 MULTILIB_MATCHES = $(shell \
   multilibs="$(MULTILIB_OPTIONS)" ; \
-  for abi in m1,m2,m3,m4-nofpu,m4-400,m4-500,m4al,m4a-nofpu m1,m2,m2a-nofpu \
-             m2e,m3e,m4-single-only,m4-100-single-only,m4-200-single-only,m4a-single-only \
+  for abi in m1,m2,m3,m4-nofpu,m4-100-nofpu,m4-200-nofpu,m4-400,m4-500,m4-340,m4-300-nofpu,m4al,m4a-nofpu \
+             m1,m2,m2a-nofpu \
+             m2e,m3e,m4-single-only,m4-100-single-only,m4-200-single-only,m4-300-single-only,m4a-single-only \
              m2e,m2a-single-only \
-             m4-single,m4-100-single,m4-200-single,m4a-single \
-             m4,m4-100,m4-200,m4a \
+             m4-single,m4-100-single,m4-200-single,m4-300-single,m4a-single \
+             m4,m4-100,m4-200,m4-300,m4a \
              m5-32media,m5-compact,m5-32media \
              m5-32media-nofpu,m5-compact-nofpu,m5-32media-nofpu; do \
     subst= ; \
@@ -67,30 +70,32 @@
 	$(GCC_FOR_TARGET) $(MULTILIB_CFLAGS) -c -o $(T)crti.o -x assembler-with-cpp $(srcdir)/config/sh/crti.asm
 $(T)crtn.o: $(srcdir)/config/sh/crtn.asm $(GCC_PASSES)
 	$(GCC_FOR_TARGET) $(MULTILIB_CFLAGS) -c -o $(T)crtn.o -x assembler-with-cpp $(srcdir)/config/sh/crtn.asm
+$(T)trap-handler.o : $(srcdir)/config/sh/trap-handler.c
+	$(GCC_FOR_TARGET) $(MULTILIB_CFLAGS) -c -o $(T)trap-handler.o -g $(srcdir)/config/sh/trap-handler.c
 
 $(out_object_file): gt-sh.h
 gt-sh.h : s-gtype ; @true
 
 # These are not suitable for COFF.
-# EXTRA_MULTILIB_PARTS= crt1.o crti.o crtn.o crtbegin.o crtend.o
+# EXTRA_MULTILIB_PARTS= crt1.o crti.o crtn.o crtbegin.o crtend.o trap-handler.o
 
 IC_EXTRA_PARTS= libic_invalidate_array_4-100.a libic_invalidate_array_4-200.a \
 libic_invalidate_array_4a.a
-OPT_EXTRA_PARTS= libgcc-Os-4-200.a
+OPT_EXTRA_PARTS= libgcc-Os-4-200.a libgcc-4-300.a
 EXTRA_MULTILIB_PARTS= $(IC_EXTRA_PARTS) $(OPT_EXTRA_PARTS)
 
 $(T)ic_invalidate_array_4-100.o: $(srcdir)/config/sh/lib1funcs.asm $(GCC_PASSES)
-	$(GCC_FOR_TARGET) $(MULTILIB_CFLAGS) -c -o $(T)ic_invalidate_array_4-100.o -DL_ic_invalidate_array -DWAYS=1 -DWAY_SIZE=0x2000 -x assembler-with-cpp $(srcdir)/config/sh/lib1funcs.asm
+	$(GCC_FOR_TARGET) $(MULTILIB_CFLAGS) -I. -c -o $(T)ic_invalidate_array_4-100.o -DL_ic_invalidate_array -DWAYS=1 -DWAY_SIZE=0x2000 -x assembler-with-cpp $(srcdir)/config/sh/lib1funcs.asm
 $(T)libic_invalidate_array_4-100.a: $(T)ic_invalidate_array_4-100.o $(GCC_PASSES)
 	$(AR_CREATE_FOR_TARGET) $(T)libic_invalidate_array_4-100.a $(T)ic_invalidate_array_4-100.o
 
 $(T)ic_invalidate_array_4-200.o: $(srcdir)/config/sh/lib1funcs.asm $(GCC_PASSES)
-	$(GCC_FOR_TARGET) $(MULTILIB_CFLAGS) -c -o $(T)ic_invalidate_array_4-200.o -DL_ic_invalidate_array -DWAYS=2 -DWAY_SIZE=0x2000 -x assembler-with-cpp $(srcdir)/config/sh/lib1funcs.asm
+	$(GCC_FOR_TARGET) $(MULTILIB_CFLAGS) -I. -c -o $(T)ic_invalidate_array_4-200.o -DL_ic_invalidate_array -DWAYS=2 -DWAY_SIZE=0x2000 -x assembler-with-cpp $(srcdir)/config/sh/lib1funcs.asm
 $(T)libic_invalidate_array_4-200.a: $(T)ic_invalidate_array_4-200.o $(GCC_PASSES)
 	$(AR_CREATE_FOR_TARGET) $(T)libic_invalidate_array_4-200.a $(T)ic_invalidate_array_4-200.o
 
 $(T)ic_invalidate_array_4a.o: $(srcdir)/config/sh/lib1funcs.asm $(GCC_PASSES)
-	$(GCC_FOR_TARGET) $(MULTILIB_CFLAGS) -c -o $(T)ic_invalidate_array_4a.o -DL_ic_invalidate_array -D__FORCE_SH4A__ -x assembler-with-cpp $(srcdir)/config/sh/lib1funcs.asm
+	$(GCC_FOR_TARGET) $(MULTILIB_CFLAGS) -I. -c -o $(T)ic_invalidate_array_4a.o -DL_ic_invalidate_array -D__FORCE_SH4A__ -x assembler-with-cpp $(srcdir)/config/sh/lib1funcs.asm
 $(T)libic_invalidate_array_4a.a: $(T)ic_invalidate_array_4a.o $(GCC_PASSES)
 	$(AR_CREATE_FOR_TARGET) $(T)libic_invalidate_array_4a.a $(T)ic_invalidate_array_4a.o
 
@@ -104,6 +109,12 @@
 $(T)libgcc-Os-4-200.a: $(OBJS_Os_4_200) $(GCC_PASSES)
 	$(AR_CREATE_FOR_TARGET) $@ $(OBJS_Os_4_200)
 
+$(T)div_table-4-300.o: $(srcdir)/config/sh/lib1funcs-4-300.asm $(GCC_PASSES)
+	$(GCC_FOR_TARGET) $(MULTILIB_CFLAGS) -c -o $@ -DL_div_table -x assembler-with-cpp $<
+
+$(T)libgcc-4-300.a: $(T)div_table-4-300.o $(GCC_PASSES)
+	$(AR_CREATE_FOR_TARGET) $@ $(T)div_table-4-300.o
+
 # Local Variables:
 # mode: Makefile
 # End:
diff -Naur gcc-4.2.1/gcc/config/sh/t-superh gcc-4.2.1-stm/gcc/config/sh/t-superh
--- gcc-4.2.1/gcc/config/sh/t-superh	2006-04-12 21:09:41.000000000 +0100
+++ gcc-4.2.1-stm/gcc/config/sh/t-superh	2007-08-10 13:08:59.000000000 +0100
@@ -1,6 +1,7 @@
 EXTRA_MULTILIB_PARTS= crt1.o crti.o crtn.o \
 	crtbegin.o crtend.o crtbeginS.o crtendS.o \
-	crt1-mmu.o gcrt1-mmu.o gcrt1.o $(IC_EXTRA_PARTS) $(OPT_EXTRA_PARTS)
+	crt1-mmu.o gcrt1-mmu.o gcrt1.o $(IC_EXTRA_PARTS) $(OPT_EXTRA_PARTS) \
+	trap-handler.o
 
 # Compile crt1-mmu.o as crt1.o with -DMMU_SUPPORT
 $(T)crt1-mmu.o: $(srcdir)/config/sh/crt1.asm $(GCC_PASSES)
@@ -13,3 +14,4 @@
 # For sh4-400: Compile gcrt1.o as crt1.o with -DPROFILE
 $(T)gcrt1.o: $(srcdir)/config/sh/crt1.asm $(GCC_PASSES)
 	$(GCC_FOR_TARGET) $(MULTILIB_CFLAGS) -c -o $(T)gcrt1.o -DPROFILE -x assembler-with-cpp $(srcdir)/config/sh/crt1.asm
+
diff -Naur gcc-4.2.1/gcc/config.gcc gcc-4.2.1-stm/gcc/config.gcc
--- gcc-4.2.1/gcc/config.gcc	2007-02-03 05:25:20.000000000 +0000
+++ gcc-4.2.1-stm/gcc/config.gcc	2007-08-10 13:08:59.000000000 +0100
@@ -1,6 +1,7 @@
 # GCC target-specific configuration file.
 # Copyright 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006
 # Free Software Foundation, Inc.
+# Copyright (c) 2006  STMicroelectronics.
 
 #This file is part of GCC.
 
@@ -2003,7 +2004,8 @@
 				with_libgloss=yes
 				tm_file="${tm_file} sh/newlib.h"
 			fi
-			tm_file="${tm_file} sh/embed-elf.h"
+			tm_file="${tm_file} sh/embed-elf.h sh/superh.h"
+			tmake_file="${tmake_file} sh/t-superh"
 			extra_options="${extra_options} sh/superh.opt" ;;
 	*)		if test x$with_newlib = xyes \
 			   && test x$with_libgloss = xyes; then
@@ -2077,7 +2079,7 @@
 		esac
 		tm_defines="$tm_defines STRICT_NOFPU=1"
 	fi
-	sh_cpu_default="`echo $with_cpu|sed s/^m/sh/|tr A-Z_ a-z-`"
+	sh_cpu_default="`echo $with_cpu|sed 's/^m/sh/'|tr ABCDEFGHIJKLMNOPQRSTUVWXYZ_ abcdefghijklmnopqrstuvwxyz-`"
 	case $sh_cpu_default in
 	sh5-64media-nofpu | sh5-64media | \
 	  sh5-32media-nofpu | sh5-32media | sh5-compact-nofpu | sh5-compact | \
@@ -2100,7 +2102,7 @@
 		*) sh_multilibs=m1,m2,m2e,m4,m4-single,m4-single-only,m2a,m2a-single ;;
 		esac
 		if test x$with_fp = xno; then
-			sh_multilibs="`echo $sh_multilibs|sed -e s/m4/sh4-nofpu/ -e s/,m4-[^,]*//g -e s/,m[23]e// -e s/m2a,m2a-single/m2a-nofpu/ -e s/m5-..m....,//g`"
+			sh_multilibs="`echo $sh_multilibs|sed -e s/m4/sh4-nofpu/ -e 's/,m4-[^,]*//g' -e 's/,m[23]e//' -e s/m2a,m2a-single/m2a-nofpu/ -e s/m5-..m....,//g`"
 		fi
 	fi
 	target_cpu_default=SELECT_`echo ${sh_cpu_default}|tr abcdefghijklmnopqrstuvwxyz- ABCDEFGHIJKLMNOPQRSTUVWXYZ_`
@@ -2109,7 +2111,7 @@
 	for sh_multilib in ${sh_multilibs}; do
 		case ${sh_multilib} in
 		sh1 | sh2 | sh2e | sh3 | sh3e | \
-		sh4 | sh4-single | sh4-single-only | sh4-nofpu | \
+		sh4 | sh4-single | sh4-single-only | sh4-nofpu | sh4-300 | \
 		sh4a | sh4a-single | sh4a-single-only | sh4a-nofpu | sh4al | \
 		sh2a | sh2a-single | sh2a-single-only | sh2a-nofpu | \
 		sh5-64media | sh5-64media-nofpu | \
diff -Naur gcc-4.2.1/gcc/configure gcc-4.2.1-stm/gcc/configure
--- gcc-4.2.1/gcc/configure	2007-01-02 03:44:31.000000000 +0000
+++ gcc-4.2.1-stm/gcc/configure	2007-08-10 13:08:58.000000000 +0100
@@ -309,7 +309,7 @@
 # include <unistd.h>
 #endif"
 
-ac_subst_vars='SHELL PATH_SEPARATOR PACKAGE_NAME PACKAGE_TARNAME PACKAGE_VERSION PACKAGE_STRING PACKAGE_BUGREPORT exec_prefix prefix program_transform_name bindir sbindir libexecdir datadir sysconfdir sharedstatedir localstatedir libdir includedir oldincludedir infodir mandir build_alias host_alias target_alias DEFS ECHO_C ECHO_N ECHO_T LIBS build build_cpu build_vendor build_os host host_cpu host_vendor host_os target target_cpu target_vendor target_os target_noncanonical build_libsubdir build_subdir host_subdir target_subdir GENINSRC CC CFLAGS LDFLAGS CPPFLAGS ac_ct_CC EXEEXT OBJEXT NO_MINUS_C_MINUS_O OUTPUT_OPTION CPP EGREP strict1_warn cxx_compat_warn warn_cflags WERROR nocommon_flag TREEBROWSER valgrind_path valgrind_path_defines valgrind_command coverage_flags enable_multilib enable_decimal_float enable_shared TARGET_SYSTEM_ROOT TARGET_SYSTEM_ROOT_DEFINE CROSS_SYSTEM_HEADER_DIR onestep datarootdir docdir htmldir SET_MAKE AWK LN_S LN RANLIB ac_ct_RANLIB ranlib_flags INSTALL INSTALL_PROGRAM INSTALL_DATA make_compare_target have_mktemp_command MAKEINFO BUILD_INFO GENERATED_MANPAGES FLEX BISON NM AR stage1_cflags COLLECT2_LIBS GNAT_LIBEXC LDEXP_LIB TARGET_GETGROUPS_T LIBICONV LTLIBICONV LIBICONV_DEP manext objext gthread_flags extra_modes_file extra_opt_files USE_NLS LIBINTL LIBINTL_DEP INCINTL XGETTEXT GMSGFMT POSUB CATALOGS DATADIRNAME INSTOBJEXT GENCAT CATOBJEXT host_cc_for_libada CROSS ALL SYSTEM_HEADER_DIR inhibit_libc CC_FOR_BUILD BUILD_CFLAGS STMP_FIXINC STMP_FIXPROTO collect2 gcc_cv_as ORIGINAL_AS_FOR_TARGET gcc_cv_ld ORIGINAL_LD_FOR_TARGET gcc_cv_nm ORIGINAL_NM_FOR_TARGET gcc_cv_objdump libgcc_visibility GGC zlibdir zlibinc MAINT gcc_tooldir dollar slibdir objdir subdirs srcdir all_boot_languages all_compilers all_gtfiles all_gtfiles_files_langs all_gtfiles_files_files all_lang_makefrags all_lang_makefiles all_languages all_selected_languages all_stagestuff build_exeext build_install_headers_dir build_xm_file_list build_xm_include_list build_xm_defines check_languages cc_set_by_configure quoted_cc_set_by_configure cpp_install_dir xmake_file tmake_file extra_gcc_objs extra_headers_list extra_objs extra_parts extra_passes extra_programs float_h_file gcc_config_arguments gcc_gxx_include_dir host_exeext host_xm_file_list host_xm_include_list host_xm_defines out_host_hook_obj install lang_opt_files lang_specs_files lang_tree_files local_prefix md_file objc_boehm_gc out_file out_object_file stage_prefix_set_by_configure quoted_stage_prefix_set_by_configure thread_file tm_file_list tm_include_list tm_defines tm_p_file_list tm_p_include_list xm_file_list xm_include_list xm_defines c_target_objs cxx_target_objs target_cpu_default GMPLIBS GMPINC LIBOBJS LTLIBOBJS'
+ac_subst_vars='SHELL PATH_SEPARATOR PACKAGE_NAME PACKAGE_TARNAME PACKAGE_VERSION PACKAGE_STRING PACKAGE_BUGREPORT exec_prefix prefix program_transform_name bindir sbindir libexecdir datadir sysconfdir sharedstatedir localstatedir libdir includedir oldincludedir infodir mandir build_alias host_alias target_alias DEFS ECHO_C ECHO_N ECHO_T LIBS build build_cpu build_vendor build_os host host_cpu host_vendor host_os target target_cpu target_vendor target_os target_noncanonical build_libsubdir build_subdir host_subdir target_subdir GENINSRC CC CFLAGS LDFLAGS CPPFLAGS ac_ct_CC EXEEXT OBJEXT NO_MINUS_C_MINUS_O OUTPUT_OPTION CPP EGREP strict1_warn cxx_compat_warn warn_cflags WERROR nocommon_flag TREEBROWSER valgrind_path valgrind_path_defines valgrind_command coverage_flags enable_multilib enable_decimal_float enable_shared TARGET_SYSTEM_ROOT TARGET_SYSTEM_ROOT_DEFINE CROSS_SYSTEM_HEADER_DIR onestep datarootdir docdir htmldir SET_MAKE AWK LN_S LN RANLIB ac_ct_RANLIB ranlib_flags INSTALL INSTALL_PROGRAM INSTALL_DATA make_compare_target have_mktemp_command MAKEINFO BUILD_INFO GENERATED_MANPAGES FLEX BISON NM AR stage1_cflags COLLECT2_LIBS GNAT_LIBEXC LDEXP_LIB TARGET_GETGROUPS_T LIBICONV LTLIBICONV LIBICONV_DEP manext objext gthread_flags extra_modes_file extra_opt_files USE_NLS LIBINTL LIBINTL_DEP INCINTL XGETTEXT GMSGFMT POSUB CATALOGS DATADIRNAME INSTOBJEXT GENCAT CATOBJEXT host_cc_for_libada CROSS ALL SYSTEM_HEADER_DIR inhibit_libc CC_FOR_BUILD BUILD_CFLAGS STMP_FIXINC STMP_FIXPROTO collect2 gcc_cv_as ORIGINAL_AS_FOR_TARGET gcc_cv_ld ORIGINAL_LD_FOR_TARGET gcc_cv_nm ORIGINAL_NM_FOR_TARGET gcc_cv_objdump libgcc_visibility GGC zlibdir zlibinc MAINT gcc_tooldir dollar slibdir objdir subdirs srcdir all_boot_languages all_compilers all_gtfiles all_gtfiles_files_langs all_gtfiles_files_files all_lang_makefrags all_lang_makefiles all_languages all_selected_languages all_stagestuff build_exeext build_install_headers_dir build_xm_file_list build_xm_include_list build_xm_defines check_languages cc_set_by_configure quoted_cc_set_by_configure cpp_install_dir xmake_file tmake_file extra_gcc_objs extra_headers_list extra_libgcc_srcs extra_objs extra_parts extra_passes extra_programs float_h_file gcc_config_arguments gcc_gxx_include_dir host_exeext host_xm_file_list host_xm_include_list host_xm_defines out_host_hook_obj install lang_opt_files lang_specs_files lang_tree_files local_prefix md_file objc_boehm_gc out_file out_object_file stage_prefix_set_by_configure quoted_stage_prefix_set_by_configure thread_file tm_file_list tm_include_list tm_defines tm_p_file_list tm_p_include_list xm_file_list xm_include_list xm_defines c_target_objs cxx_target_objs target_cpu_default GMPLIBS GMPINC LIBOBJS LTLIBOBJS'
 ac_subst_files='language_hooks'
 
 # Initialize some variables set by options.
@@ -12389,7 +12389,7 @@
     target_thread_file='single'
     ;;
   aix | dce | gnat | irix | posix | posix95 | rtems | \
-  single | solaris | vxworks | win32 )
+  single | solaris | vxworks | win32 | generic)
     target_thread_file=${enable_threads}
     ;;
   *)
@@ -12410,6 +12410,14 @@
   rm -f gthr-default.h
   echo "#include \"gthr-${thread_file}.h\"" > gthr-default.h
   gthread_flags=-DHAVE_GTHR_DEFAULT
+  if test $thread_file != posix; then
+    if test -f $srcdir/gthr-${thread_file}.c; then
+      extra_libgcc_srcs=$srcdir/gthr-${thread_file}.c
+    fi
+    if test -f $srcdir/gthr-objc-${thread_file}.c; then
+      extra_libgcc_srcs="${extra_libgcc_srcs} $srcdir/gthr-objc-${thread_file}.c"
+    fi
+  fi
 fi
 
 
@@ -12716,6 +12724,7 @@
 	esac
 	saved_CFLAGS="${CFLAGS}"
 	CC="${CC_FOR_BUILD}" CFLAGS="${CFLAGS_FOR_BUILD}" \
+	CONFIG_SITE="" \
 	${realsrcdir}/configure \
 		--enable-languages=${enable_languages-all} \
 		--target=$target_alias --host=$build_alias --build=$build_alias
@@ -17280,6 +17289,7 @@
 s,@tmake_file@,$tmake_file,;t t
 s,@extra_gcc_objs@,$extra_gcc_objs,;t t
 s,@extra_headers_list@,$extra_headers_list,;t t
+s,@extra_libgcc_srcs@,$extra_libgcc_srcs,;t t
 s,@extra_objs@,$extra_objs,;t t
 s,@extra_parts@,$extra_parts,;t t
 s,@extra_passes@,$extra_passes,;t t
diff -Naur gcc-4.2.1/gcc/configure.ac gcc-4.2.1-stm/gcc/configure.ac
--- gcc-4.2.1/gcc/configure.ac	2007-01-02 03:44:31.000000000 +0000
+++ gcc-4.2.1-stm/gcc/configure.ac	2007-08-10 13:08:58.000000000 +0100
@@ -1390,7 +1390,7 @@
     target_thread_file='single'
     ;;
   aix | dce | gnat | irix | posix | posix95 | rtems | \
-  single | solaris | vxworks | win32 )
+  single | solaris | vxworks | win32 | generic)
     target_thread_file=${enable_threads}
     ;;
   *)
@@ -1411,6 +1411,14 @@
   rm -f gthr-default.h
   echo "#include \"gthr-${thread_file}.h\"" > gthr-default.h
   gthread_flags=-DHAVE_GTHR_DEFAULT
+  if test $thread_file != posix; then
+    if test -f $srcdir/gthr-${thread_file}.c; then
+      extra_libgcc_srcs=$srcdir/gthr-${thread_file}.c
+    fi
+    if test -f $srcdir/gthr-objc-${thread_file}.c; then
+      extra_libgcc_srcs="${extra_libgcc_srcs} $srcdir/gthr-objc-${thread_file}.c"
+    fi
+  fi
 fi
 AC_SUBST(gthread_flags)
 
@@ -1491,6 +1499,7 @@
 	esac
 	saved_CFLAGS="${CFLAGS}"
 	CC="${CC_FOR_BUILD}" CFLAGS="${CFLAGS_FOR_BUILD}" \
+	CONFIG_SITE="" \
 	${realsrcdir}/configure \
 		--enable-languages=${enable_languages-all} \
 		--target=$target_alias --host=$build_alias --build=$build_alias
@@ -3606,6 +3615,7 @@
 AC_SUBST(tmake_file)
 AC_SUBST(extra_gcc_objs)
 AC_SUBST(extra_headers_list)
+AC_SUBST(extra_libgcc_srcs)
 AC_SUBST(extra_objs)
 AC_SUBST(extra_parts)
 AC_SUBST(extra_passes)
diff -Naur gcc-4.2.1/gcc/c-opts.c gcc-4.2.1-stm/gcc/c-opts.c
--- gcc-4.2.1/gcc/c-opts.c	2007-03-12 18:50:38.000000000 +0000
+++ gcc-4.2.1-stm/gcc/c-opts.c	2007-08-10 13:08:59.000000000 +0100
@@ -2,6 +2,7 @@
    Copyright (C) 2002, 2003, 2004, 2005, 2006, 2007
    Free Software Foundation, Inc.
    Contributed by Neil Booth.
+   Copyright (c) 2006  STMicroelectronics.
 
 This file is part of GCC.
 
@@ -286,6 +287,7 @@
       break;
 
     case OPT__output_pch_:
+      CYGPATH (arg);
       pch_file = arg;
       break;
 
@@ -345,11 +347,13 @@
     case OPT_MD:
     case OPT_MMD:
       cpp_opts->deps.style = (code == OPT_MD ? DEPS_SYSTEM: DEPS_USER);
+      CYGPATH (arg);
       deps_file = arg;
       break;
 
     case OPT_MF:
       deps_seen = true;
+      CYGPATH (arg);
       deps_file = arg;
       break;
 
@@ -821,6 +825,7 @@
 
     case OPT_imacros:
     case OPT_include:
+      CYGPATH (arg);
       defer_opt (code, arg);
       break;
 
@@ -829,6 +834,7 @@
       break;
 
     case OPT_iprefix:
+      CYGPATH (arg);
       iprefix = arg;
       break;
 
@@ -837,6 +843,7 @@
       break;
 
     case OPT_isysroot:
+      CYGPATH (arg);
       sysroot = arg;
       break;
 
@@ -875,7 +882,10 @@
 
     case OPT_o:
       if (!out_fname)
-	out_fname = arg;
+	{
+	  CYGPATH (arg);
+	  out_fname = arg;
+	}
       else
 	error ("output filename specified twice");
       break;
@@ -1267,7 +1277,10 @@
 
       /* Command line -MF overrides environment variables and default.  */
       if (!deps_file)
-	deps_file = spec;
+	{
+	  CYGPATH (spec);
+	  deps_file = spec;
+	}
 
       deps_append = 1;
       deps_seen = true;
diff -Naur gcc-4.2.1/gcc/cp/ChangeLog.STM gcc-4.2.1-stm/gcc/cp/ChangeLog.STM
--- gcc-4.2.1/gcc/cp/ChangeLog.STM	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.2.1-stm/gcc/cp/ChangeLog.STM	2007-08-10 13:08:58.000000000 +0100
@@ -0,0 +1,17 @@
+2006-07-28  Volker Reichelt  <reichelt@igpm.rwth-aachen.de>
+
+	http://gcc.gnu.org/ml/gcc-patches/2006-07/msg01189.html
+	PR c++/28274
+	* decl.c (duplicate_decls): Check check_default_args here.
+	(start_preparsed_function): Do not call check_default_args.
+	* name-lookup.c (pushdecl_maybe_friend): Only call
+	check_default_args if duplicate_decls got bypassed.
+
+	* ../testsuite/g++.dg/other/default5.C: New test.
+
+2006-06-23  J"orn Rennecke <joern.rennecke@st.com>
+
+	http://gcc.gnu.org/ml/gcc-patches/2006-06/msg01295.html
+	* except.c (expand_start_catch_block): Use correct types for bitwise
+	copy.
+
diff -Naur gcc-4.2.1/gcc/DEV-PHASE gcc-4.2.1-stm/gcc/DEV-PHASE
--- gcc-4.2.1/gcc/DEV-PHASE	2007-07-19 15:31:06.000000000 +0100
+++ gcc-4.2.1-stm/gcc/DEV-PHASE	2007-08-10 13:08:58.000000000 +0100
@@ -0,0 +1 @@
+experimental
diff -Naur gcc-4.2.1/gcc/doc/invoke.texi gcc-4.2.1-stm/gcc/doc/invoke.texi
--- gcc-4.2.1/gcc/doc/invoke.texi	2007-04-24 22:54:22.000000000 +0100
+++ gcc-4.2.1-stm/gcc/doc/invoke.texi	2007-08-10 13:08:58.000000000 +0100
@@ -322,7 +322,7 @@
 -fno-inline  -fno-math-errno  -fno-peephole  -fno-peephole2 @gol
 -funsafe-math-optimizations  -funsafe-loop-optimizations  -ffinite-math-only @gol
 -fno-toplevel-reorder -fno-trapping-math  -fno-zero-initialized-in-bss @gol
--fomit-frame-pointer  -foptimize-register-move @gol
+-fomit-frame-pointer  -foptimize-register-move -foptimize-related-values @gol
 -foptimize-sibling-calls  -fprefetch-loop-arrays @gol
 -fprofile-generate -fprofile-use @gol
 -fregmove  -frename-registers @gol
@@ -704,6 +704,7 @@
 @emph{SH Options}
 @gccoptlist{-m1  -m2  -m2e  -m3  -m3e @gol
 -m4-nofpu  -m4-single-only  -m4-single  -m4 @gol
+-m4-300-nofpu  -m4-300-single-only  -m4-300-single  -m4-300 @gol
 -m4a-nofpu -m4a-single-only -m4a-single -m4a -m4al @gol
 -m5-64media  -m5-64media-nofpu @gol
 -m5-32media  -m5-32media-nofpu @gol
@@ -714,7 +715,9 @@
 -mprefergot  -musermode -multcost=@var{number} -mdiv=@var{strategy} @gol
 -mdivsi3_libfunc=@var{name}  @gol
 -madjust-unroll -mindexed-addressing -mgettrcost=@var{number} -mpt-fixed @gol
- -minvalid-symbols}
+-minvalid-symbols @gol
+-malign-small-blocks=@var{block-size} @gol
+}
 
 @emph{SPARC Options}
 @gccoptlist{-mcpu=@var{cpu-type} @gol
@@ -4601,7 +4604,7 @@
 -fpeephole2 @gol
 -fschedule-insns  -fschedule-insns2 @gol
 -fsched-interblock  -fsched-spec @gol
--fregmove @gol
+-fregmove -foptimize-related-values @gol
 -fstrict-aliasing -fstrict-overflow @gol
 -fdelete-null-pointer-checks @gol
 -freorder-blocks  -freorder-functions @gol
@@ -5041,6 +5044,15 @@
 
 Enabled at levels @option{-O2}, @option{-O3}, @option{-Os}.
 
+@item -foptimize-related-values
+@opindex foptimize-related-values
+For targets with auto-increment addressing modes, attempt to re-arrange
+register + offset calculations and memory acesses in order to reduce
+the overall instruction count and register pressure.
+
+Enabled at levels @option{-O2}, @option{-O3}, @option{-Os}, unless
+-foptimize-register-move is disabled.
+
 @item -fdelayed-branch
 @opindex fdelayed-branch
 If supported for the target machine, attempt to reorder instructions
@@ -12360,6 +12372,24 @@
 @opindex m4
 Generate code for the SH4.
 
+@item -m4-300-nofpu
+@opindex m4-300-nofpu
+Generate code for the ST40-300 without a floating-point unit.
+
+@item -m4-300-single-only
+@opindex m4-300-single-only
+Generate code for the ST40-300 with a floating-point unit that only
+supports single-precision arithmetic.
+
+@item -m4-300-single
+@opindex m4-300-single
+Generate code for the ST40-300 assuming the floating-point unit is in
+single-precision mode by default.
+
+@item -m4-300
+@opindex m4-300
+Generate code for the ST40-300.
+
 @item -m4a-nofpu
 @opindex m4a-nofpu
 Generate code for the SH4al-dsp, or for a SH4a in such a way that the
@@ -12523,6 +12553,11 @@
 This option only has an effect if the gcc code base supports the
 TARGET_ADJUST_UNROLL_MAX target hook.
 
+@item -malign-small-blocks=@var{number}
+@opindex align-small-blocks=@var{number}
+Set the size among which basic blocks are aligned on cache line boundaries. 
+Default is 16 bytes. 0 means default alignment.
+
 @item -mindexed-addressing
 @opindex mindexed-addressing
 Enable the use of the indexed addressing mode for SHmedia32/SHcompact.
diff -Naur gcc-4.2.1/gcc/doc/tm.texi gcc-4.2.1-stm/gcc/doc/tm.texi
--- gcc-4.2.1/gcc/doc/tm.texi	2007-03-10 00:59:15.000000000 +0000
+++ gcc-4.2.1-stm/gcc/doc/tm.texi	2007-08-10 13:08:58.000000000 +0100
@@ -2102,6 +2102,16 @@
 @code{CCmode} is incomplete.
 @end defmac
 
+@deftypefn {Target Hook} int TARGET_MATCH_ADJUST (rtx @var{x}, int @var{regno})
+@var{regno} contains the register number of the first hard register in @var{x}.
+Return a register number suitable to test if two registers are considered
+matching for the purposes of a matching constraint.
+This is neeed to get useful results when matching registers of different
+sizes for big endian targets.
+The default is to point to the least significant hard register for
+scalar integer modes, but to the first register for any other mode.
+@end deftypefn
+
 @node Leaf Functions
 @subsection Handling Leaf Functions
 
@@ -8758,10 +8768,10 @@
 @code{num_modes_for_mode_switching[@var{entity}] - 1}.
 @end defmac
 
-@defmac EMIT_MODE_SET (@var{entity}, @var{mode}, @var{hard_regs_live})
+@defmac EMIT_MODE_SET (@var{entity}, @var{mode}, @var{flip}, @var{hard_regs_live})
 Generate one or more insns to set @var{entity} to @var{mode}.
 @var{hard_reg_live} is the set of hard registers live at the point where
-the insn(s) are to be inserted.
+the insn(s) are to be inserted. @var{flip} is a boolean to indicate that current mode can be flipped.
 @end defmac
 
 @node Target Attributes
diff -Naur gcc-4.2.1/gcc/flow.c gcc-4.2.1-stm/gcc/flow.c
--- gcc-4.2.1/gcc/flow.c	2006-09-13 11:59:03.000000000 +0100
+++ gcc-4.2.1-stm/gcc/flow.c	2007-08-10 13:08:58.000000000 +0100
@@ -2,6 +2,7 @@
    Copyright (C) 1987, 1988, 1992, 1993, 1994, 1995, 1996, 1997, 1998,
    1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006 Free Software Foundation,
    Inc.
+   Copyright (c) 2006  STMicroelectronics.
 
 This file is part of GCC.
 
diff -Naur gcc-4.2.1/gcc/fold-const.c gcc-4.2.1-stm/gcc/fold-const.c
--- gcc-4.2.1/gcc/fold-const.c	2007-06-06 14:56:46.000000000 +0100
+++ gcc-4.2.1-stm/gcc/fold-const.c	2007-08-10 13:08:58.000000000 +0100
@@ -2,6 +2,7 @@
    Copyright (C) 1987, 1988, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999,
    2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007
    Free Software Foundation, Inc.
+   Copyright (c) 2006  STMicroelectronics.
 
 This file is part of GCC.
 
@@ -2029,7 +2030,13 @@
   if (! overflow)
     {
       tree lt = TYPE_MIN_VALUE (type);
-      REAL_VALUE_TYPE l = real_value_from_int_cst (NULL_TREE, lt);
+      REAL_VALUE_TYPE l;
+
+      if (TYPE_PRECISION (type) >= 32)
+	l = real_value_from_int_cst (NULL_TREE, lt);
+      else
+	real_from_integer (&l, VOIDmode, (HOST_WIDE_INT) -1 << 31,
+			   (HOST_WIDE_INT) -1, 0);
       if (REAL_VALUES_LESS (r, l))
 	{
 	  overflow = 1;
@@ -2043,7 +2050,14 @@
       tree ut = TYPE_MAX_VALUE (type);
       if (ut)
 	{
-	  REAL_VALUE_TYPE u = real_value_from_int_cst (NULL_TREE, ut);
+	  REAL_VALUE_TYPE u;
+
+	  if (TYPE_PRECISION (type) >= 32)
+	    u = real_value_from_int_cst (NULL_TREE, ut);
+	  else
+	    real_from_integer (&u, VOIDmode,
+			       (HOST_WIDE_INT) 32767 << 16 | 65535L,
+			       (HOST_WIDE_INT) 0, 0);
 	  if (REAL_VALUES_LESS (u, r))
 	    {
 	      overflow = 1;
diff -Naur gcc-4.2.1/gcc/gcc.c gcc-4.2.1-stm/gcc/gcc.c
--- gcc-4.2.1/gcc/gcc.c	2007-03-05 20:37:05.000000000 +0000
+++ gcc-4.2.1-stm/gcc/gcc.c	2007-08-10 13:08:58.000000000 +0100
@@ -2018,6 +2018,7 @@
   char *buffer;
   char *p;
 
+  CYGPATH (filename);
   buffer = load_specs (filename);
 
   /* Scan BUFFER for specs, putting them in the vector.  */
@@ -2240,6 +2241,8 @@
 
   if (link_command_spec == 0)
     fatal ("spec file has no spec for linking");
+
+  CYGPATH_FREE (filename);
 }
 
 /* Record the names of temporary files we tell compilers to write,
@@ -2663,7 +2666,10 @@
 	     bool do_multi)
 {
   struct file_at_path_info info;
+  char *temp;
+
 
+  CYGPATH (name);
 #ifdef DEFAULT_ASSEMBLER
   if (! strcmp (name, "as") && access (DEFAULT_ASSEMBLER, mode) == 0)
     return xstrdup (DEFAULT_ASSEMBLER);
@@ -2679,8 +2685,13 @@
   if (IS_ABSOLUTE_PATH (name))
     {
       if (access (name, mode) == 0)
-	return xstrdup (name);
+	{
+	  temp =  xstrdup (name);
+	  CYGPATH_FREE (name);
+	  return temp;
+	}
 
+      CYGPATH_FREE (name);
       return NULL;
     }
 
@@ -2690,8 +2701,10 @@
   info.suffix_len = strlen (info.suffix);
   info.mode = mode;
 
-  return for_each_path (pprefix, do_multi, info.name_len + info.suffix_len,
+  temp = for_each_path (pprefix, do_multi, info.name_len + info.suffix_len,
 			file_at_path, &info);
+  CYGPATH_FREE (name);
+  return temp;  
 }
 
 /* Ranking of prefixes in the sort list. -B prefixes are put before
@@ -4218,6 +4231,7 @@
 #ifdef HAVE_TARGET_OBJECT_SUFFIX
 	  argv[i] = convert_filename (argv[i], 0, access (argv[i], F_OK));
 #endif
+	  CYGPATH (argv[i]);
 
 	  if (strcmp (argv[i], "-") != 0 && access (argv[i], F_OK) < 0)
 	    {
diff -Naur gcc-4.2.1/gcc/gthr-generic.c gcc-4.2.1-stm/gcc/gthr-generic.c
--- gcc-4.2.1/gcc/gthr-generic.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.2.1-stm/gcc/gthr-generic.c	2007-08-10 13:08:59.000000000 +0100
@@ -0,0 +1,170 @@
+/* Generic threads supplementary implementation. */
+/* Compile this one with gcc.  */
+/* Copyright (C) 1997, 1999, 2000, 2002, 2006 Free Software Foundation, Inc.
+   Copyright (c) 2006  STMicroelectronics.
+
+This file is part of GCC.
+
+GCC is free software; you can redistribute it and/or modify
+it under the terms of the GNU General Public License as published by
+the Free Software Foundation; either version 2, or (at your option)
+any later version.
+
+GCC is distributed in the hope that it will be useful,
+but WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with GCC; see the file COPYING.  If not, write to the Free
+Software Foundation, 51 Franklin Street, Fifth Floor, Boston, MA
+02110-1301, USA.  */
+
+/* As a special exception, if you link this library with other files,
+   some of which are compiled with GCC, to produce an executable,
+   this library does not by itself cause the resulting executable
+   to be covered by the GNU General Public License.
+   This exception does not however invalidate any other reasons why
+   the executable file might be covered by the GNU General Public License.  */
+
+#define __GTHR_WEAK __attribute__ ((weak))
+
+#include "tconfig.h"
+#include "gthr.h"
+
+#ifndef __gthr_generic_h
+#error "Generic thread support package not supported"
+#endif
+
+/* These are stub functions.  When threading is available, a suitable set of definitions should be linked in.  */
+
+/* Return 1 if thread system is active, 0 if not.  */
+int
+__generic_gxx_active_p (void)
+{
+  return 0;
+}
+
+/* The following functions should return zero on success or the error
+   number.  If the operation is not supported, -1 is returned.
+
+   __generic_gxx_once
+   __generic_gxx_key_create
+   __generic_gxx_key_delete
+   __generic_gxx_setspecific
+   __generic_gxx_mutex_lock
+   __generic_gxx_mutex_trylock
+   __generic_gxx_mutex_unlock
+   __generic_gxx_recursive_mutex_lock
+   __generic_gxx_recursive_mutex_trylock
+   __generic_gxx_recursive_mutex_unlock  */
+
+/* FUNC is a function that should be called without parameters.
+   *ONCE has been initialized to __GTHREAD_ONCE_INIT and is otherwise only
+   used in calls to __generic_gxx_once with FUNC as the second parameter.
+   If __generic_gxx_once succeeds, FUNC will have been called exactly once
+   since the initialization of ONCE through any number of calls of
+   __generic_gxx_once with this pair of ONCE and FUNC values.  */
+int
+__generic_gxx_once (__gthread_once_t *once ATTRIBUTE_UNUSED,
+		    void (*func)(void) ATTRIBUTE_UNUSED)
+{
+  return -1;
+}
+
+/* Assign a key to *KEY that can be used in calls to
+   __generic_gxx_setspecific / __generic_gxx_getspecific.
+   If DTOR is nonzero, and at thread exit the value associated with the key
+   is nonzero, DTOR will be called at thread exit with the value associated
+   with the key as its only argument.  */
+int
+__generic_gxx_key_create (__gthread_key_t *key ATTRIBUTE_UNUSED,
+			  void (*dtor)(void *) ATTRIBUTE_UNUSED)
+{
+  return -1;
+}
+
+/* KEY is a key previously allocated by __generic_gxx_key_create.
+   Remove it from the set of keys known for this thread.  */
+int
+__generic_gxx_key_delete (__gthread_key_t key ATTRIBUTE_UNUSED)
+{
+  return -1;
+}
+
+/* Return thread-specific data associated with KEY.  */
+void *
+__generic_gxx_getspecific (__gthread_key_t key ATTRIBUTE_UNUSED)
+{
+  return 0;
+}
+
+/* Set thread-specific data associated with KEY to PTR.  */
+int
+__generic_gxx_setspecific (__gthread_key_t key ATTRIBUTE_UNUSED,
+		      const void *ptr ATTRIBUTE_UNUSED)
+{
+  return -1;
+}
+
+/* Initialize *MUTEX.  */
+void
+__generic_gxx_mutex_init_function (__gthread_mutex_t *mutex ATTRIBUTE_UNUSED)
+{
+}
+
+/* Acquire a lock on *MUTEX.  The behaviour is undefined if a lock on *MUTEX
+   has already been acquired by the same thread.  */
+int
+__generic_gxx_mutex_lock (__gthread_mutex_t *mutex ATTRIBUTE_UNUSED)
+{
+  return 0;
+}
+
+/* Try to acquire a lock on *MUTEX.  If a lock on *MUTEX already exists,
+   return an error code.  */
+int
+__generic_gxx_mutex_trylock (__gthread_mutex_t *mutex ATTRIBUTE_UNUSED)
+{
+  return 0;
+}
+
+/* A lock on *MUTEX has previously been acquired with __generic_gxx_mutex_lock
+   or __generic_gxx_mutex_trylock.  Release the lock.  */
+int
+__generic_gxx_mutex_unlock (__gthread_mutex_t *mutex ATTRIBUTE_UNUSED)
+{
+  return 0;
+}
+
+/* Initialize *MUTEX.  */
+void
+__generic_gxx_recursive_mutex_init_function (__gthread_recursive_mutex_t *mutex ATTRIBUTE_UNUSED)
+{
+}
+
+/* Acquire a lock on *MUTEX.  If a lock on *MUTEX has already been acquired by
+   the same thread, succeed.  */
+int
+__generic_gxx_recursive_mutex_lock (__gthread_recursive_mutex_t *mutex ATTRIBUTE_UNUSED)
+{
+  return 0;
+}
+
+/* Try to acquire a lock on *MUTEX.  If a lock on *MUTEX has already been
+   acquired by the same thread, succeed.  If any other lock on *MUTEX
+   already exists, return an error code.  */
+int
+__generic_gxx_recursive_mutex_trylock (__gthread_recursive_mutex_t *mutex ATTRIBUTE_UNUSED)
+{
+  return 0;
+}
+
+/* A lock on *MUTEX has previously been acquired with
+   __generic_gxx_recursive_mutex_lock or
+   __generic_gxx_recursive_mutex_trylock.  Release the lock.  */
+int
+__generic_gxx_recursive_mutex_unlock (__gthread_recursive_mutex_t *mutex ATTRIBUTE_UNUSED)
+{
+  return 0;
+}
diff -Naur gcc-4.2.1/gcc/gthr-generic.h gcc-4.2.1-stm/gcc/gthr-generic.h
--- gcc-4.2.1/gcc/gthr-generic.h	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.2.1-stm/gcc/gthr-generic.h	2007-08-10 13:08:59.000000000 +0100
@@ -0,0 +1,372 @@
+/* Generic threads compatibility routines for libgcc2 and libobjc. */
+/* Compile this one with gcc.  */
+/* Copyright (C) 1997, 1999, 2000, 2002, 2006 Free Software Foundation, Inc.
+   Copyright (c) 2006  STMicroelectronics.
+
+This file is part of GCC.
+
+GCC is free software; you can redistribute it and/or modify
+it under the terms of the GNU General Public License as published by
+the Free Software Foundation; either version 2, or (at your option)
+any later version.
+
+GCC is distributed in the hope that it will be useful,
+but WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with GCC; see the file COPYING.  If not, write to the Free
+Software Foundation, 51 Franklin Street, Fifth Floor, Boston, MA
+02110-1301, USA.  */
+
+/* As a special exception, if you link this library with other files,
+   some of which are compiled with GCC, to produce an executable,
+   this library does not by itself cause the resulting executable
+   to be covered by the GNU General Public License.
+   This exception does not however invalidate any other reasons why
+   the executable file might be covered by the GNU General Public License.  */
+
+#ifndef __gthr_generic_h
+#define __gthr_generic_h
+
+#define __GTHREADS 1
+
+#define __GTHREAD_ONCE_INIT 0
+#define __GTHREAD_MUTEX_INIT_FUNCTION __gthread_mutex_init_function
+#define __GTHREAD_RECURSIVE_MUTEX_INIT_FUNCTION __gthread_recursive_mutex_init_function
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+/* Avoid depedency on specific headers.
+   The general idea is that you dynamically allocate the required data
+   structures, and a void * is used to point to this dynamically allocated
+   data.  If your implementation can put all the required information in
+   the void * itself, that's fine, too, of course.
+   libstdc++ inherits from the mutex types, whcih is why they need to be
+   wrapped up as structs.  */
+typedef void *__gthread_key_t;
+typedef void *__gthread_once_t;
+typedef struct __gthread_mutex_s { void *__p; } __gthread_mutex_t;
+typedef struct __gthread_recursive_mutex_s { void *__p; } __gthread_recursive_mutex_t;
+
+/* We should always link with at least one definition, so we want strong
+   references.  The stub definitions are weak so that they can be overriden.  */
+#ifndef __GTHR_WEAK
+#define __GTHR_WEAK
+#endif
+
+extern int __generic_gxx_active_p (void) __GTHR_WEAK;
+
+extern int __generic_gxx_once (__gthread_once_t *, void (*)(void)) __GTHR_WEAK;
+
+extern int __generic_gxx_key_create (__gthread_key_t *,
+				     void (*)(void *)) __GTHR_WEAK;
+
+extern int __generic_gxx_key_delete (__gthread_key_t key) __GTHR_WEAK;
+
+extern void *__generic_gxx_getspecific (__gthread_key_t key) __GTHR_WEAK;
+
+extern int __generic_gxx_setspecific (__gthread_key_t, const void *) __GTHR_WEAK;
+
+extern void __generic_gxx_mutex_init_function (__gthread_mutex_t *) __GTHR_WEAK;
+
+extern int __generic_gxx_mutex_lock (__gthread_mutex_t *) __GTHR_WEAK;
+
+extern int __generic_gxx_mutex_trylock (__gthread_mutex_t *) __GTHR_WEAK;
+
+extern int __generic_gxx_mutex_unlock (__gthread_mutex_t *) __GTHR_WEAK;
+
+extern void __generic_gxx_recursive_mutex_init_function (__gthread_recursive_mutex_t *) __GTHR_WEAK;
+
+extern int __generic_gxx_recursive_mutex_lock (__gthread_recursive_mutex_t *) __GTHR_WEAK;
+
+extern int __generic_gxx_recursive_mutex_trylock (__gthread_recursive_mutex_t *) __GTHR_WEAK;
+
+extern int __generic_gxx_recursive_mutex_unlock (__gthread_recursive_mutex_t *) __GTHR_WEAK;
+
+#ifdef __cplusplus
+}
+#endif
+
+#ifdef _LIBOBJC
+
+extern int __generic_gxx_objc_init_thread_system (void) __GTHR_WEAK;
+
+extern int __generic_gxx_objc_close_thread_system (void) __GTHR_WEAK;
+
+extern objc_thread_t __generic_gxx_objc_thread_detach (void (*)(void *), void *) __GTHR_WEAK;
+
+extern int __generic_gxx_objc_thread_set_priority (int priority) __GTHR_WEAK;
+
+extern int __generic_gxx_objc_thread_get_priority (void) __GTHR_WEAK;
+
+extern void __generic_gxx_objc_thread_yield (void) __GTHR_WEAK;
+
+extern int __generic_gxx_objc_thread_exit (void) __GTHR_WEAK;
+
+extern objc_thread_t __generic_gxx_objc_thread_id (void) __GTHR_WEAK;
+
+extern int __generic_gxx_objc_thread_set_data (void *value) __GTHR_WEAK;
+
+extern void *__generic_gxx_objc_thread_get_data (void) __GTHR_WEAK;
+
+extern int __generic_gxx_objc_mutex_allocate (objc_mutex_t) __GTHR_WEAK;
+
+extern int __generic_gxx_objc_mutex_deallocate (objc_mutex_t) __GTHR_WEAK;
+
+extern int __generic_gxx_objc_mutex_lock (objc_mutex_t) __GTHR_WEAK;
+
+extern int __generic_gxx_objc_mutex_trylock (objc_mutex_t) __GTHR_WEAK;
+
+extern int __generic_gxx_objc_mutex_unlock (objc_mutex_t) __GTHR_WEAK;
+
+extern int __generic_gxx_objc_condition_allocate (objc_condition_t) __GTHR_WEAK;
+
+extern int __generic_gxx_objc_condition_deallocate (objc_condition_t) __GTHR_WEAK;
+
+extern int __generic_gxx_objc_condition_wait (objc_condition_t, objc_mutex_t) __GTHR_WEAK;
+
+extern int __generic_gxx_objc_condition_broadcast (objc_condition_t) __GTHR_WEAK;
+
+extern int __generic_gxx_objc_condition_signal (objc_condition_t) __GTHR_WEAK;
+
+/* Backend initialization functions */
+
+/* Initialize the threads subsystem.  */
+static inline int
+__gthread_objc_init_thread_system (void)
+{
+  return __generic_gxx_objc_init_thread_system ();
+}
+
+/* Close the threads subsystem.  */
+static inline int
+__gthread_objc_close_thread_system (void)
+{
+  return __generic_gxx_objc_close_thread_system ();
+}
+
+/* Backend thread functions */
+
+/* Create a new thread of execution.  */
+static inline objc_thread_t
+__gthread_objc_thread_detach (void (* func)(void *), void * arg)
+{
+  return __generic_gxx_objc_thread_detach (func, arg);
+}
+
+/* Set the current thread's priority.  */
+static inline int
+__gthread_objc_thread_set_priority (int priority)
+{
+  return __generic_gxx_objc_thread_set_priority (priority);
+}
+
+/* Return the current thread's priority.  */
+static inline int
+__gthread_objc_thread_get_priority (void)
+{
+  return __generic_gxx_objc_thread_get_priority ();
+}
+
+/* Yield our process time to another thread.  */
+static inline void
+__gthread_objc_thread_yield (void)
+{
+  __generic_gxx_objc_thread_yield ();
+}
+
+/* Terminate the current thread.  */
+static inline int
+__gthread_objc_thread_exit (void)
+{
+  return __generic_gxx_objc_thread_exit ();
+}
+
+/* Returns an integer value which uniquely describes a thread.  */
+static inline objc_thread_t
+__gthread_objc_thread_id (void)
+{
+  return __generic_gxx_objc_thread_id ();
+}
+
+/* Sets the thread's local storage pointer.  */
+static inline int
+__gthread_objc_thread_set_data (void *value)
+{
+  return __generic_gxx_objc_thread_set_data (value);
+}
+
+/* Returns the thread's local storage pointer.  */
+static inline void *
+__gthread_objc_thread_get_data (void)
+{
+  return __generic_gxx_objc_thread_get_data ();
+}
+
+/* Backend mutex functions */
+
+/* Allocate a mutex.  */
+static inline int
+__gthread_objc_mutex_allocate (objc_mutex_t mutex)
+{
+  return __generic_gxx_objc_mutex_allocate (mutex);
+}
+
+/* Deallocate a mutex.  */
+static inline int
+__gthread_objc_mutex_deallocate (objc_mutex_t mutex)
+{
+  return __generic_gxx_objc_mutex_deallocate (mutex);
+}
+
+/* Grab a lock on a mutex.  */
+static inline int
+__gthread_objc_mutex_lock (objc_mutex_t mutex)
+{
+  return __generic_gxx_objc_mutex_lock (mutex);
+}
+
+/* Try to grab a lock on a mutex.  */
+static inline int
+__gthread_objc_mutex_trylock (objc_mutex_t mutex)
+{
+  return __generic_gxx_objc_mutex_trylock (mutex);
+}
+
+/* Unlock the mutex */
+static inline int
+__gthread_objc_mutex_unlock (objc_mutex_t mutex)
+{
+  return __generic_gxx_objc_mutex_unlock (mutex);
+}
+
+/* Backend condition mutex functions */
+
+/* Allocate a condition.  */
+static inline int
+__gthread_objc_condition_allocate (objc_condition_t condition)
+{
+  return __generic_gxx_objc_condition_allocate (condition);
+}
+
+/* Deallocate a condition.  */
+static inline int
+__gthread_objc_condition_deallocate (objc_condition_t condition)
+{
+  return __generic_gxx_objc_condition_deallocate (condition);
+}
+
+/* Wait on the condition */
+static inline int
+__gthread_objc_condition_wait (objc_condition_t condition, objc_mutex_t mutex)
+{
+  return __generic_gxx_objc_condition_wait (condition, mutex);
+}
+
+/* Wake up all threads waiting on this condition.  */
+static inline int
+__gthread_objc_condition_broadcast (objc_condition_t condition)
+{
+  return __generic_gxx_objc_condition_broadcast ( condition);
+}
+
+/* Wake up one thread waiting on this condition.  */
+static inline int
+__gthread_objc_condition_signal (objc_condition_t condition)
+{
+  return __generic_gxx_objc_condition_signal (condition);
+}
+
+#else /* !_LIBOBJC */
+
+static inline int
+__gthread_active_p (void)
+{
+  return __generic_gxx_active_p ();
+}
+
+static inline int
+__gthread_once (__gthread_once_t *once, void (*func)(void))
+{
+  return __generic_gxx_once (once, func);
+}
+
+static inline int
+__gthread_key_create (__gthread_key_t *key, void (*dtor)(void *))
+{
+  return __generic_gxx_key_create (key, dtor);
+}
+
+static inline int
+__gthread_key_delete (__gthread_key_t key)
+{
+  return __generic_gxx_key_delete (key);
+}
+
+static inline void *
+__gthread_getspecific (__gthread_key_t key)
+{
+  return __generic_gxx_getspecific (key);
+}
+
+static inline int
+__gthread_setspecific (__gthread_key_t key, const void *ptr)
+{
+  return __generic_gxx_setspecific (key, ptr);
+}
+
+static inline void
+__gthread_mutex_init_function (__gthread_mutex_t *mutex)
+{
+  __generic_gxx_mutex_init_function (mutex);
+}
+
+static inline int
+__gthread_mutex_lock (__gthread_mutex_t * mutex)
+{
+  return __generic_gxx_mutex_lock (mutex);
+}
+
+static inline int
+__gthread_mutex_trylock (__gthread_mutex_t * mutex)
+{
+  return __generic_gxx_mutex_trylock (mutex);
+}
+
+static inline int
+__gthread_mutex_unlock (__gthread_mutex_t * mutex)
+{
+  return __generic_gxx_mutex_unlock (mutex);
+}
+
+static inline void
+__gthread_recursive_mutex_init_function (__gthread_recursive_mutex_t *mutex)
+{
+  __generic_gxx_recursive_mutex_init_function (mutex);
+}
+
+static inline int
+__gthread_recursive_mutex_lock (__gthread_recursive_mutex_t * mutex)
+{
+  return __generic_gxx_recursive_mutex_lock (mutex);
+}
+
+static inline int
+__gthread_recursive_mutex_trylock (__gthread_recursive_mutex_t * mutex)
+{
+  return __generic_gxx_recursive_mutex_trylock (mutex);
+}
+
+static inline int
+__gthread_recursive_mutex_unlock (__gthread_recursive_mutex_t * mutex)
+{
+  return __generic_gxx_recursive_mutex_unlock (mutex);
+}
+
+#endif /* _LIBOBJC */
+
+#endif /* __gthr_generic_h */
diff -Naur gcc-4.2.1/gcc/gthr-objc-generic.c gcc-4.2.1-stm/gcc/gthr-objc-generic.c
--- gcc-4.2.1/gcc/gthr-objc-generic.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.2.1-stm/gcc/gthr-objc-generic.c	2007-08-10 13:08:58.000000000 +0100
@@ -0,0 +1,221 @@
+/* Threads compatibility routines for libobjc.  */
+/* Compile this one with gcc.  */
+/* Copyright (C) 1997, 1999, 2000, 2006 Free Software Foundation, Inc.
+   Copyright (c) 2006  STMicroelectronics.
+
+This file is part of GCC.
+
+GCC is free software; you can redistribute it and/or modify it under
+the terms of the GNU General Public License as published by the Free
+Software Foundation; either version 2, or (at your option) any later
+version.
+
+GCC is distributed in the hope that it will be useful, but WITHOUT ANY
+WARRANTY; without even the implied warranty of MERCHANTABILITY or
+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+for more details.
+
+You should have received a copy of the GNU General Public License
+along with GCC; see the file COPYING.  If not, write to the Free
+Software Foundation, 51 Franklin Street, Fifth Floor, Boston, MA
+02110-1301, USA.  */
+
+/* As a special exception, if you link this library with other files,
+   some of which are compiled with GCC, to produce an executable,
+   this library does not by itself cause the resulting executable
+   to be covered by the GNU General Public License.
+   This exception does not however invalidate any other reasons why
+   the executable file might be covered by the GNU General Public License.  */
+
+#include "tconfig.h"
+
+#define __GTHR_WEAK __attribute__ ((weak))
+#define _LIBOBJC
+
+/* ??? The objc thread types are defined in ../libobjc/objc/thr.h,
+   but we don't want the gcc core to depend on libobjc.  */
+typedef void * objc_thread_t;
+typedef struct objc_mutex *objc_mutex_t;
+typedef struct objc_condition *objc_condition_t;
+#define OBJC_THREAD_INTERACTIVE_PRIORITY        2
+
+#include "gthr.h"
+
+#define UNUSED(x) x ATTRIBUTE_UNUSED
+
+/* Just provide compatibility for mutex handling.  */
+
+/* Thread local storage for a single thread */
+static void *thread_local_storage = 0;
+
+/* Backend initialization functions */
+
+/* Initialize the threads subsystem.  */
+int
+__generic_gxx_objc_init_thread_system (void)
+{
+  /* No thread support available */
+  return -1;
+}
+
+/* Close the threads subsystem.  */
+int
+__generic_gxx_objc_close_thread_system (void)
+{
+  /* No thread support available */
+  return -1;
+}
+
+/* Backend thread functions */
+
+/* Create a new thread of execution.  The thread starts executing by calling
+   FUNC with ARG as its only argument.
+   On success, a handle for the new thread is returned.
+   On failure, zero is returned.  */
+objc_thread_t
+__generic_gxx_objc_thread_detach (void UNUSED ((* func)(void *)),
+				  void * UNUSED(arg))
+{
+  /* No thread support available */
+  return 0;
+}
+
+/* Set the current thread's priority.  */
+int
+__generic_gxx_objc_thread_set_priority (int UNUSED(priority))
+{
+  /* No thread support available */
+  return -1;
+}
+
+/* Return the current thread's priority.  */
+int
+__generic_gxx_objc_thread_get_priority (void)
+{
+  return OBJC_THREAD_INTERACTIVE_PRIORITY;
+}
+
+/* Yield our process time to another thread.  */
+void
+__generic_gxx_objc_thread_yield (void)
+{
+  return;
+}
+
+/* Terminate the current thread.  */
+int
+__generic_gxx_objc_thread_exit (void)
+{
+  /* No thread support available */
+  /* Should we really exit the program */
+  /* exit (&__objc_thread_exit_status); */
+  return -1;
+}
+
+/* Returns an integer value which uniquely describes a thread.  */
+objc_thread_t
+__generic_gxx_objc_thread_id (void)
+{
+  /* No thread support, use 1.  */
+  return (objc_thread_t) 1;
+}
+
+/* Sets the thread's objc local storage pointer.  */
+int
+__generic_gxx_objc_thread_set_data (void *value)
+{
+  thread_local_storage = value;
+  return 0;
+}
+
+/* Returns the thread's objc local storage pointer.  */
+void *
+__generic_gxx_objc_thread_get_data (void)
+{
+  return thread_local_storage;
+}
+
+/* Backend mutex functions */
+
+/* Allocate a backend-specific mutex data in MUTEX->backend.  
+   Return 0 on success, -1 for failure.  */
+int
+__generic_gxx_objc_mutex_allocate (objc_mutex_t UNUSED(mutex))
+{
+  return 0;
+}
+
+/* Deallocate backend-specific mutex data in MUTEX->backend.
+   Return 0 on success, -1 for failure.  */
+int
+__generic_gxx_objc_mutex_deallocate (objc_mutex_t UNUSED(mutex))
+{
+  return 0;
+}
+
+/* Grab a lock on MUTEX.  Return 0 on success.  */
+int
+__generic_gxx_objc_mutex_lock (objc_mutex_t UNUSED(mutex))
+{
+  /* There can only be one thread, so we always get the lock */
+  return 0;
+}
+
+/* Try to grab a lock on MUTEX.  Return 0 on success.  */
+int
+__generic_gxx_objc_mutex_trylock (objc_mutex_t UNUSED(mutex))
+{
+  /* There can only be one thread, so we always get the lock */
+  return 0;
+}
+
+/* Unlock MUTEX.  Return 0 on success.  */
+int
+__generic_gxx_objc_mutex_unlock (objc_mutex_t UNUSED(mutex))
+{
+  return 0;
+}
+
+/* Backend condition mutex functions */
+
+/* Allocate backend-specific condition data in CONDITION->backend.
+   Return 0 on success, -1 for failure.  */
+int
+__generic_gxx_objc_condition_allocate (objc_condition_t UNUSED(condition))
+{
+  return 0;
+}
+
+/* Deallocate backend-specific condition data in CONDITION->backend.
+   Return 0 for success.  */
+int
+__generic_gxx_objc_condition_deallocate (objc_condition_t UNUSED(condition))
+{
+  return 0;
+}
+
+/* MUTEX is a locked mutex.  Atomically release MUTEX and wait on
+   CONDITION, i.e. so that no other thread can observe a state after
+   the release of MUTEX but before this thread has blocked.
+   Then re-acquire a lock on MUTEX.
+   Return 0 on success.  */
+int
+__generic_gxx_objc_condition_wait (objc_condition_t UNUSED(condition),
+				   objc_mutex_t UNUSED(mutex))
+{
+  return 0;
+}
+
+/* Wake up all threads waiting on CONDITION.  Return 0 on success.  */
+int
+__generic_gxx_objc_condition_broadcast (objc_condition_t UNUSED(condition))
+{
+  return 0;
+}
+
+/* Wake up one thread waiting on CONDITION.  Return 0 on success.  */
+int
+__generic_gxx_objc_condition_signal (objc_condition_t UNUSED(condition))
+{
+  return 0;
+}
diff -Naur gcc-4.2.1/gcc/ipa-pure-const.c gcc-4.2.1-stm/gcc/ipa-pure-const.c
--- gcc-4.2.1/gcc/ipa-pure-const.c	2006-11-25 16:45:09.000000000 +0000
+++ gcc-4.2.1-stm/gcc/ipa-pure-const.c	2007-08-10 13:08:58.000000000 +0100
@@ -509,11 +509,9 @@
   l->pure_const_state = IPA_CONST;
   l->state_set_in_source = false;
 
-  /* If this function does not return normally or does not bind local,
-     do not touch this unless it has been marked as const or pure by the
-     front end.  */
-  if (TREE_THIS_VOLATILE (decl)
-      || !targetm.binds_local_p (decl))
+  /* If this function does not bind locally, do not touch this unless it has
+     been marked as const or pure by the front end.  */
+  if (!targetm.binds_local_p (decl))
     {
       l->pure_const_state = IPA_NEITHER;
       return;
diff -Naur gcc-4.2.1/gcc/lcm.c gcc-4.2.1-stm/gcc/lcm.c
--- gcc-4.2.1/gcc/lcm.c	2006-02-06 18:20:47.000000000 +0000
+++ gcc-4.2.1-stm/gcc/lcm.c	2007-08-10 13:08:58.000000000 +0100
@@ -371,17 +371,18 @@
     }
 }
 
-/* Given local properties TRANSP, ANTLOC, AVOUT, KILL return the insert and
-   delete vectors for edge based LCM.  Returns an edgelist which is used to
-   map the insert vector to what edge an expression should be inserted on.  */
+/* Given local properties TRANSP, ANTLOC, AVLOC, KILL return the insert and
+   delete vectors for edge based LCM, and return the AVIN, AVOUT bitmap.
+   Returns an edgelist which is used to map the insert vector to
+   what edge an expression should be inserted on.  */
 
 struct edge_list *
-pre_edge_lcm (int n_exprs, sbitmap *transp,
-	      sbitmap *avloc, sbitmap *antloc, sbitmap *kill,
-	      sbitmap **insert, sbitmap **delete)
+pre_edge_lcm_avs (int n_exprs, sbitmap *transp,
+		  sbitmap *avloc, sbitmap *antloc, sbitmap *kill,
+		  sbitmap *avin, sbitmap *avout,
+		  sbitmap **insert, sbitmap **delete)
 {
   sbitmap *antin, *antout, *earliest;
-  sbitmap *avin, *avout;
   sbitmap *later, *laterin;
   struct edge_list *edge_list;
   int num_edges;
@@ -403,10 +404,7 @@
 #endif
 
   /* Compute global availability.  */
-  avin = sbitmap_vector_alloc (last_basic_block, n_exprs);
-  avout = sbitmap_vector_alloc (last_basic_block, n_exprs);
   compute_available (avloc, kill, avout, avin);
-  sbitmap_vector_free (avin);
 
   /* Compute global anticipatability.  */
   antin = sbitmap_vector_alloc (last_basic_block, n_exprs);
@@ -432,7 +430,6 @@
 
   sbitmap_vector_free (antout);
   sbitmap_vector_free (antin);
-  sbitmap_vector_free (avout);
 
   later = sbitmap_vector_alloc (num_edges, n_exprs);
 
@@ -469,6 +466,28 @@
   return edge_list;
 }
 
+/* Wrapper to allocate avin/avout and call pre_edge_lcm_avs.  */
+
+struct edge_list *
+pre_edge_lcm (int n_exprs, sbitmap *transp,
+	      sbitmap *avloc, sbitmap *antloc, sbitmap *kill, 
+	      sbitmap **insert, sbitmap **delete)
+{
+  struct edge_list *edge_list;
+  sbitmap *avin, *avout;
+
+  avin = sbitmap_vector_alloc (last_basic_block, n_exprs);
+  avout = sbitmap_vector_alloc (last_basic_block, n_exprs);
+
+  edge_list = pre_edge_lcm_avs (n_exprs, transp, avloc, antloc, kill,
+				 avin, avout, insert, delete);
+
+  sbitmap_vector_free (avout);
+  sbitmap_vector_free (avin);
+
+  return edge_list;
+}
+
 /* Compute the AVIN and AVOUT vectors from the AVLOC and KILL vectors.
    Return the number of passes we performed to iterate to a solution.  */
 
diff -Naur gcc-4.2.1/gcc/loop-iv.c gcc-4.2.1-stm/gcc/loop-iv.c
--- gcc-4.2.1/gcc/loop-iv.c	2007-07-03 15:33:16.000000000 +0100
+++ gcc-4.2.1-stm/gcc/loop-iv.c	2007-08-10 13:08:59.000000000 +0100
@@ -1358,7 +1358,7 @@
   rtx op0, op1;
 
   if (CONSTANT_P (rhs)
-      || REG_P (rhs))
+      || (REG_P (rhs) && !HARD_REGISTER_P (rhs)))
     return true;
 
   switch (GET_CODE (rhs))
@@ -1368,9 +1368,9 @@
       op0 = XEXP (rhs, 0);
       op1 = XEXP (rhs, 1);
       /* Allow reg + const sets only.  */
-      if (REG_P (op0) && CONSTANT_P (op1))
+      if (REG_P (op0) && !HARD_REGISTER_P (op0) && CONSTANT_P (op1))
 	return true;
-      if (REG_P (op1) && CONSTANT_P (op0))
+      if (REG_P (op1) && !HARD_REGISTER_P (op1) && CONSTANT_P (op0))
 	return true;
 
       return false;
diff -Naur gcc-4.2.1/gcc/Makefile.in gcc-4.2.1-stm/gcc/Makefile.in
--- gcc-4.2.1/gcc/Makefile.in	2007-05-31 16:37:38.000000000 +0100
+++ gcc-4.2.1-stm/gcc/Makefile.in	2007-08-10 13:08:59.000000000 +0100
@@ -4,6 +4,7 @@
 # Copyright (C) 1987, 1988, 1990, 1991, 1992, 1993, 1994, 1995, 1996,
 # 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006
 # Free Software Foundation, Inc.
+# Copyright (c) 2006  STMicroelectronics.
 
 #This file is part of GCC.
 
@@ -634,7 +635,7 @@
 LIBCONVERT =
 
 # Control whether header files are installed.
-INSTALL_HEADERS=install-headers install-mkheaders
+INSTALL_HEADERS=install-headers #broken: install-mkheaders
 
 # Control whether Info documentation is built and installed.
 BUILD_INFO = @BUILD_INFO@
@@ -1413,7 +1414,7 @@
 #
 # Build libgcc.a.
 
-LIB2ADD = $(LIB2FUNCS_EXTRA)
+LIB2ADD = $(LIB2FUNCS_EXTRA) @extra_libgcc_srcs@
 LIB2ADD_ST = $(LIB2FUNCS_STATIC_EXTRA)
 
 libgcc.mk: config.status Makefile mklibgcc $(LIB2ADD) $(LIB2ADD_ST) specs \
@@ -1774,6 +1775,8 @@
 
 dumpvers: dumpvers.c
 
+version.o: $(OBJS:version.o=)
+
 version.o: version.c version.h $(DATESTAMP) $(BASEVER) $(DEVPHASE)
 	$(CC) $(ALL_CFLAGS) $(ALL_CPPFLAGS) \
 	-DBASEVER=$(BASEVER_s) -DDATESTAMP=$(DATESTAMP_s) \
@@ -2142,7 +2145,7 @@
    coretypes.h intl.h
 targhooks.o : targhooks.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TREE_H) \
    $(EXPR_H) $(TM_H) $(RTL_H) $(TM_P_H) $(FUNCTION_H) output.h toplev.h \
-   $(MACHMODE_H) $(TARGET_DEF_H) $(TARGET_H) $(GGC_H) gt-targhooks.h
+   $(MACHMODE_H) $(TARGET_DEF_H) $(TARGET_H) $(GGC_H) $(REGS_H) gt-targhooks.h
 
 toplev.o : toplev.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) $(TREE_H) \
    version.h $(RTL_H) $(FUNCTION_H) $(FLAGS_H) xcoffout.h input.h \
@@ -2543,7 +2546,8 @@
 regmove.o : regmove.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) $(RTL_H) \
    insn-config.h $(TIMEVAR_H) tree-pass.h \
    $(RECOG_H) output.h $(REGS_H) hard-reg-set.h $(FLAGS_H) $(FUNCTION_H) \
-   $(EXPR_H) $(BASIC_BLOCK_H) toplev.h $(TM_P_H) except.h reload.h
+   $(EXPR_H) $(BASIC_BLOCK_H) toplev.h $(TM_P_H) except.h reload.h \
+   $(OPTABS_H) gt-regmove.h
 ddg.o : ddg.c $(DDG_H) $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TARGET_H) \
    toplev.h $(RTL_H) $(TM_P_H) $(REGS_H) $(FUNCTION_H) \
    $(FLAGS_H) insn-config.h $(INSN_ATTR_H) except.h $(RECOG_H) \
@@ -2867,7 +2871,7 @@
   $(srcdir)/tree-iterator.c $(srcdir)/gimplify.c \
   $(srcdir)/tree-chrec.h $(srcdir)/tree-vect-generic.c \
   $(srcdir)/tree-ssa-operands.h $(srcdir)/tree-ssa-operands.c \
-  $(srcdir)/tree-profile.c $(srcdir)/tree-nested.c \
+  $(srcdir)/tree-profile.c $(srcdir)/tree-nested.c $(srcdir)/regmove.c \
   $(srcdir)/ipa-reference.c $(srcdir)/tree-ssa-structalias.h \
   $(srcdir)/tree-ssa-structalias.c \
   $(srcdir)/c-pragma.h $(srcdir)/omp-low.c \
@@ -2896,7 +2900,7 @@
 gt-expr.h gt-sdbout.h gt-optabs.h gt-bitmap.h gt-dojump.h \
 gt-dwarf2out.h gt-dwarf2asm.h \
 gt-dbxout.h \
-gtype-c.h gt-cfglayout.h \
+gtype-c.h gt-cfglayout.h gt-regmove.h \
 gt-tree-mudflap.h gt-tree-vect-generic.h \
 gt-tree-profile.h gt-tree-ssa-address.h \
 gt-tree-ssanames.h gt-tree-iterator.h gt-gimplify.h \
diff -Naur gcc-4.2.1/gcc/mode-switching.c gcc-4.2.1-stm/gcc/mode-switching.c
--- gcc-4.2.1/gcc/mode-switching.c	2006-09-05 22:41:23.000000000 +0100
+++ gcc-4.2.1-stm/gcc/mode-switching.c	2007-08-10 13:08:58.000000000 +0100
@@ -95,8 +95,184 @@
 static void reg_dies (rtx, HARD_REG_SET);
 static void reg_becomes_live (rtx, rtx, void *);
 static void make_preds_opaque (basic_block, int);
+
+/* Bitmap to compute mode flipping.  */
+
+static sbitmap *mode_in_flip;  /* flip in mode status for each basic blocks.  */
+static sbitmap *mode_out_flip; /* flip out mode status for each basic blocks.  */
+/* To support mode switching, the algorithm cannot set the modes after
+   the insert and delete bitmaps are computed by pre_edge_lcm, because
+   'avin' is computed iteratively for each possible modes for each entity.
+   The mode emission will be done after all mode are processed.
+   (see commit_mode_sets).  */
+
+static int **modes_needed;  /* modes needs to be inserted on this edge.  */
+
+/* Indicates that edge mode information is unknown. Cannot use 'no_mode'
+   because its value depends of its entity */
+#define NO_MODE -1
 
 
+/* Return true when one of the predecessor edges of BB is marked with
+   EDGE_COMPLEX. (similar to bb_has_eh_pred in basic_block.h).  */
+static bool
+bb_has_complex_pred (basic_block bb)
+{
+  edge e;
+  edge_iterator ei;
+
+  FOR_EACH_EDGE (e, ei, bb->preds)
+    {
+      if (e->flags & EDGE_COMPLEX)
+	return true;
+    }
+  return false;
+}
+
+/* Test avin modes.  
+   if 'out' is 'true' we want to know if the mode out of the basic block
+   can be flipped. If 'in' is true we want to know if the mode entering the basic
+   block can be flipped.  */
+
+static int
+test_flip_status(int entity, basic_block bb, bool out)
+{
+  if (out)
+    return TEST_BIT (mode_out_flip[bb->index], entity);
+  else
+    return TEST_BIT (mode_in_flip[bb->index], entity);
+}
+
+/* Merges the avin modes.  */
+
+static void
+set_flip_status (sbitmap *avin, sbitmap *avout)
+{
+  basic_block bb;
+
+  FOR_EACH_BB (bb)
+    {
+      int i = bb->index;
+
+      /* Merge modes for each entity for each bb. 
+	 If multiple avin modes are set for the same bb, they are not
+	 exclusive and a flip may not be emitted.
+	 If more that 2 modes can be defined, flip may not be emitted.  */
+      if (! bb_has_complex_pred (bb))
+	{
+	  sbitmap_a_xor_b (mode_in_flip[i], mode_in_flip[i], avin[i]);
+	  sbitmap_a_xor_b (mode_out_flip[i], mode_out_flip[i], avout[i]);
+	}
+    }
+}
+
+/* Allocates and initializes modes_infos.  */
+
+static void
+init_modes_infos (int n_entities)
+{
+  int j;
+  int num_edges = 0;
+  basic_block bb;
+
+  /* How many edges do we have ?  */
+
+  FOR_BB_BETWEEN (bb, ENTRY_BLOCK_PTR, EXIT_BLOCK_PTR, next_bb)
+      num_edges += EDGE_COUNT (bb->succs);
+
+  modes_needed = XNEWVEC (int *, n_entities);
+
+  for (j = 0; j < n_entities; j++)
+    {
+      modes_needed[j] = XNEWVEC (int, num_edges);
+
+      /* Initial NO_MODE value is -1, because 0 is a value mode.  */
+      memset (modes_needed[j], NO_MODE, num_edges * sizeof (int));
+    }
+
+  /* Allocates bitmaps for modes.  */
+  mode_in_flip = sbitmap_vector_alloc (last_basic_block, n_entities);
+  mode_out_flip = sbitmap_vector_alloc (last_basic_block, n_entities);
+  sbitmap_vector_zero (mode_in_flip, last_basic_block);
+  sbitmap_vector_zero (mode_out_flip, last_basic_block);
+}
+
+/* frees memory used to hold the modes information.  */
+
+static void
+free_modes_infos (int n_entities)
+{
+  int j;
+
+  for (j = 0; j < n_entities; j++)
+    free (modes_needed[j]);
+
+  free (modes_needed);
+  sbitmap_vector_free (mode_in_flip);
+  sbitmap_vector_free (mode_out_flip);
+}
+
+/* records the mode associated with edge e for entity j.  */
+
+static void
+add_mode_set (int j, int e, int mode)
+{
+  modes_needed[j][e] = mode;
+}
+
+/* returns the mode needed on edge e for entity j. -1 if none.  */
+
+static int
+get_mode (int j, int e)
+{
+  return modes_needed[j][e];
+}
+
+/* Finally, after all the modes after been inserted after lcm, we can
+   process with the mode emission.  */
+
+static int
+commit_mode_sets (struct edge_list *edge_list, int j)
+{
+  int need_commit = 0;
+  int e;
+
+  for (e = 0; e < NUM_EDGES (edge_list); e++)
+    {
+      HARD_REG_SET live_at_edge;
+      edge eg = INDEX_EDGE (edge_list, e);
+      basic_block src_bb = eg->src;
+      int mode, prev_mode;
+      rtx mode_set;
+
+      if ((mode = get_mode (j, e)) == NO_MODE)
+	continue;
+
+      prev_mode = test_flip_status (j, src_bb, true);
+
+      REG_SET_TO_HARD_REG_SET (live_at_edge,
+			       src_bb->il.rtl->global_live_at_end);
+
+      start_sequence ();
+      EMIT_MODE_SET (entity_map[j], mode, prev_mode, live_at_edge);
+
+      mode_set = get_insns ();
+      end_sequence ();      
+
+      /* Do not bother to insert empty sequence.  */
+      if (mode_set == NULL_RTX)
+	continue;
+
+      /* We should not get an abnormal edge here.  */
+      gcc_assert (! (eg->flags & EDGE_ABNORMAL));
+	  
+      need_commit = 1;
+      insert_insn_on_edge (mode_set, eg);
+    }
+
+  return need_commit;
+}
+
 /* This function will allocate a new BBINFO structure, initialized
    with the MODE, INSN, and basic block BB parameters.  */
 
@@ -390,7 +566,6 @@
   basic_block bb;
   int need_commit = 0;
   sbitmap *kill;
-  struct edge_list *edge_list;
   static const int num_modes[] = NUM_MODES_FOR_MODE_SWITCHING;
 #define N_ENTITIES ARRAY_SIZE (num_modes)
   int entity_map[N_ENTITIES];
@@ -400,6 +575,8 @@
   int max_num_modes = 0;
   bool emited = false;
   basic_block post_entry ATTRIBUTE_UNUSED, pre_exit ATTRIBUTE_UNUSED;
+  sbitmap *avin, *avout;
+  struct edge_list *edge_list = 0;
 
   clear_bb_flags ();
 
@@ -436,6 +613,8 @@
   antic = sbitmap_vector_alloc (last_basic_block, n_entities);
   transp = sbitmap_vector_alloc (last_basic_block, n_entities);
   comp = sbitmap_vector_alloc (last_basic_block, n_entities);
+  avin = sbitmap_vector_alloc (last_basic_block, n_entities);
+  avout = sbitmap_vector_alloc (last_basic_block, n_entities);
 
   sbitmap_vector_ones (transp, last_basic_block);
 
@@ -538,6 +717,9 @@
     }
 
   kill = sbitmap_vector_alloc (last_basic_block, n_entities);
+
+  init_modes_infos (n_entities);
+
   for (i = 0; i < max_num_modes; i++)
     {
       int current_mode[N_ENTITIES];
@@ -547,6 +729,7 @@
       /* Set the anticipatable and computing arrays.  */
       sbitmap_vector_zero (antic, last_basic_block);
       sbitmap_vector_zero (comp, last_basic_block);
+
       for (j = n_entities - 1; j >= 0; j--)
 	{
 	  int m = current_mode[j] = MODE_PRIORITY_TO_MODE (entity_map[j], i);
@@ -567,8 +750,11 @@
 
       FOR_EACH_BB (bb)
 	sbitmap_not (kill[bb->index], transp[bb->index]);
-      edge_list = pre_edge_lcm (n_entities, transp, comp, antic,
-				kill, &insert, &delete);
+      edge_list = pre_edge_lcm_avs (n_entities, transp, comp, antic,
+				kill, avin, avout, &insert, &delete);
+
+      /* Merge modes for all entities.  */
+      set_flip_status (avin, avout);
 
       for (j = n_entities - 1; j >= 0; j--)
 	{
@@ -585,10 +771,6 @@
 	  for (e = NUM_EDGES (edge_list) - 1; e >= 0; e--)
 	    {
 	      edge eg = INDEX_EDGE (edge_list, e);
-	      int mode;
-	      basic_block src_bb;
-	      HARD_REG_SET live_at_edge;
-	      rtx mode_set;
 
 	      eg->aux = 0;
 
@@ -597,26 +779,8 @@
 
 	      eg->aux = (void *)1;
 
-	      mode = current_mode[j];
-	      src_bb = eg->src;
-
-	      REG_SET_TO_HARD_REG_SET (live_at_edge,
-				       src_bb->il.rtl->global_live_at_end);
-
-	      start_sequence ();
-	      EMIT_MODE_SET (entity_map[j], mode, live_at_edge);
-	      mode_set = get_insns ();
-	      end_sequence ();
-
-	      /* Do not bother to insert empty sequence.  */
-	      if (mode_set == NULL_RTX)
-		continue;
-
-	      /* We should not get an abnormal edge here.  */
-	      gcc_assert (! (eg->flags & EDGE_ABNORMAL));
-
-	      need_commit = 1;
-	      insert_insn_on_edge (mode_set, eg);
+	      /* Remember we need to emit it.  */
+	      add_mode_set(j, e, current_mode[j]);
 	    }
 
 	  FOR_EACH_BB_REVERSE (bb)
@@ -631,7 +795,10 @@
       sbitmap_vector_free (delete);
       sbitmap_vector_free (insert);
       clear_aux_for_edges ();
-      free_edge_list (edge_list);
+
+      /* Keep an edge_list for later.  */
+      if (i != max_num_modes - 1)
+	free_edge_list (edge_list);
     }
 
   /* Now output the remaining mode sets in all the segments.  */
@@ -639,9 +806,16 @@
     {
       int no_mode = num_modes[entity_map[j]];
 
+      /* In case there was no mode inserted. the mode information on the edge
+	 might not be complete. 
+	 Update mode info on edges and commit pending mode sets.  */
+      need_commit |= commit_mode_sets (edge_list, j);
+
       FOR_EACH_BB_REVERSE (bb)
 	{
 	  struct seginfo *ptr, *next;
+	  int last_mode = test_flip_status (j, bb, false);
+
 	  for (ptr = bb_info[j][bb->index].seginfo; ptr; ptr = next)
 	    {
 	      next = ptr->next;
@@ -650,10 +824,14 @@
 		  rtx mode_set;
 
 		  start_sequence ();
-		  EMIT_MODE_SET (entity_map[j], ptr->mode, ptr->regs_live);
+		  EMIT_MODE_SET (entity_map[j], ptr->mode, last_mode,
+				 ptr->regs_live);
 		  mode_set = get_insns ();
 		  end_sequence ();
 
+		  /* modes are are localy set.  */
+		  last_mode = 1;
+
 		  /* Insert MODE_SET only if it is nonempty.  */
 		  if (mode_set != NULL_RTX)
 		    {
@@ -674,12 +852,17 @@
       free (bb_info[j]);
     }
 
+  free_edge_list (edge_list); 
+  free_modes_infos (n_entities);
+
   /* Finished. Free up all the things we've allocated.  */
 
   sbitmap_vector_free (kill);
   sbitmap_vector_free (antic);
   sbitmap_vector_free (transp);
   sbitmap_vector_free (comp);
+  sbitmap_vector_free (avout);
+  sbitmap_vector_free (avin);
 
   if (need_commit)
     commit_edge_insertions ();
diff -Naur gcc-4.2.1/gcc/optabs.c gcc-4.2.1-stm/gcc/optabs.c
--- gcc-4.2.1/gcc/optabs.c	2007-03-12 04:40:09.000000000 +0000
+++ gcc-4.2.1-stm/gcc/optabs.c	2007-08-10 13:08:58.000000000 +0100
@@ -2,6 +2,7 @@
    Copyright (C) 1987, 1988, 1992, 1993, 1994, 1995, 1996, 1997, 1998,
    1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007
    Free Software Foundation, Inc.
+   Copyright (c) 2006  STMicroelectronics.
 
 This file is part of GCC.
 
@@ -3545,6 +3546,9 @@
 {
   do
     {
+      if (purpose == ccp_jump
+	  && cbranch_optab->handlers[(int) mode].insn_code != CODE_FOR_nothing)
+	return 1;
       if (cmp_optab->handlers[(int) mode].insn_code != CODE_FOR_nothing)
 	{
 	  if (purpose == ccp_jump)
@@ -3555,9 +3559,6 @@
 	    /* There's only one cmov entry point, and it's allowed to fail.  */
 	    return 1;
 	}
-      if (purpose == ccp_jump
-	  && cbranch_optab->handlers[(int) mode].insn_code != CODE_FOR_nothing)
-	return 1;
       if (purpose == ccp_cmov
 	  && cmov_optab->handlers[(int) mode].insn_code != CODE_FOR_nothing)
 	return 1;
@@ -4293,19 +4294,41 @@
 gen_add3_insn (rtx r0, rtx r1, rtx c)
 {
   int icode = (int) add_optab->handlers[(int) GET_MODE (r0)].insn_code;
+  int mcode;
+  rtx s;
 
   if (icode == CODE_FOR_nothing
       || !(insn_data[icode].operand[0].predicate
-	   (r0, insn_data[icode].operand[0].mode))
-      || !(insn_data[icode].operand[1].predicate
+	   (r0, insn_data[icode].operand[0].mode)))
+    return NULL_RTX;
+
+  if ((insn_data[icode].operand[1].predicate
 	   (r1, insn_data[icode].operand[1].mode))
-      || !(insn_data[icode].operand[2].predicate
+      && (insn_data[icode].operand[2].predicate
 	   (c, insn_data[icode].operand[2].mode)))
+    return GEN_FCN (icode) (r0, r1, c);
+  
+  mcode = (int) mov_optab->handlers[(int) GET_MODE (r0)].insn_code;
+  if (REGNO (r0) == REGNO (r1)
+      || !(insn_data[icode].operand[1].predicate
+	   (r0, insn_data[icode].operand[1].mode))
+      || !(insn_data[icode].operand[2].predicate
+	   (r1, insn_data[icode].operand[2].mode))
+      || !(insn_data[mcode].operand[0].predicate
+	   (r0, insn_data[mcode].operand[0].mode))
+      || !(insn_data[mcode].operand[1].predicate
+	   (c, insn_data[mcode].operand[1].mode)))
     return NULL_RTX;
 
-  return GEN_FCN (icode) (r0, r1, c);
+  start_sequence ();
+  emit_insn (GEN_FCN (mcode) (r0, c));
+  emit_insn (GEN_FCN (icode) (r0, r0, r1));
+  s = get_insns ();
+  end_sequence ();
+  return s;
 }
 
+
 int
 have_add2_insn (rtx x, rtx y)
 {
diff -Naur gcc-4.2.1/gcc/opts.c gcc-4.2.1-stm/gcc/opts.c
--- gcc-4.2.1/gcc/opts.c	2007-03-23 08:06:40.000000000 +0000
+++ gcc-4.2.1-stm/gcc/opts.c	2007-08-10 13:08:58.000000000 +0100
@@ -2,6 +2,7 @@
    Copyright (C) 2002, 2003, 2004, 2005, 2006, 2007
    Free Software Foundation, Inc.
    Contributed by Neil Booth.
+   Copyright (c) 2006  STMicroelectronics.
 
 This file is part of GCC.
 
@@ -492,6 +493,7 @@
       flag_schedule_insns_after_reload = 1;
 #endif
       flag_regmove = 1;
+      flag_optimize_related_values = 1;
       flag_strict_aliasing = 1;
       flag_strict_overflow = 1;
       flag_delete_null_pointer_checks = 1;
@@ -741,6 +743,7 @@
 
     case OPT_aux_info:
     case OPT_aux_info_:
+      CYGPATH (arg);
       aux_info_file_name = arg;
       flag_gen_aux_info = 1;
       break;
@@ -763,6 +766,7 @@
       break;
 
     case OPT_dumpbase:
+      CYGPATH (arg);
       dump_base_name = arg;
       break;
 
@@ -1002,6 +1006,7 @@
       break;
 
     case OPT_o:
+      CYGPATH (arg);
       asm_file_name = arg;
       break;
 
diff -Naur gcc-4.2.1/gcc/postreload.c gcc-4.2.1-stm/gcc/postreload.c
--- gcc-4.2.1/gcc/postreload.c	2006-03-02 19:00:11.000000000 +0000
+++ gcc-4.2.1-stm/gcc/postreload.c	2007-08-10 13:08:58.000000000 +0100
@@ -1,6 +1,7 @@
 /* Perform simple optimizations to clean up the result of reload.
    Copyright (C) 1987, 1988, 1989, 1992, 1993, 1994, 1995, 1996, 1997, 1998,
    1999, 2000, 2001, 2002, 2003, 2004, 2005 Free Software Foundation, Inc.
+   Copyright (c) 2006  STMicroelectronics.
 
 This file is part of GCC.
 
@@ -889,6 +890,23 @@
 		{
 		  rtx *np;
 
+		  /* For every new use of REG_SUM, we have to record the use
+		     of BASE therein.  */
+		  for (i = reg_state[regno].use_index;
+		       i < RELOAD_COMBINE_MAX_USES; i++)
+		    {
+		      rtx *basep = &XEXP (*reg_state[regno].reg_use[i].usep, 1);
+		      rtx use_insn = reg_state[regno].reg_use[i].insn;
+
+		      if (*basep != base)
+			abort ();
+		      reload_combine_note_use (basep, use_insn);
+		    }
+		  if (reg_state[REGNO (base)].use_ruid
+		      > reg_state[regno].use_ruid)
+		    reg_state[REGNO (base)].use_ruid
+		      = reg_state[regno].use_ruid;
+
 		  /* Delete the reg-reg addition.  */
 		  delete_insn (insn);
 
diff -Naur gcc-4.2.1/gcc/prefix.c gcc-4.2.1-stm/gcc/prefix.c
--- gcc-4.2.1/gcc/prefix.c	2005-12-17 20:45:46.000000000 +0000
+++ gcc-4.2.1-stm/gcc/prefix.c	2007-08-10 13:08:59.000000000 +0100
@@ -1,6 +1,7 @@
 /* Utility to update paths from internal to external forms.
    Copyright (C) 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005
    Free Software Foundation, Inc.
+   Copyright (c) 2006  STMicroelectronics.
 
 This file is part of GCC.
 
@@ -252,6 +253,8 @@
   char *result, *p;
   const int len = strlen (std_prefix);
 
+  CYGPATH (path);
+  
   if (! strncmp (path, std_prefix, len)
       && (IS_DIR_SEPARATOR(path[len])
           || path[len] == '\0')
@@ -346,6 +349,8 @@
     tr (result, '/', DIR_SEPARATOR);
 #endif
 
+  CYGPATH_FREE (path);
+  
   return result;
 }
 
diff -Naur gcc-4.2.1/gcc/recog.c gcc-4.2.1-stm/gcc/recog.c
--- gcc-4.2.1/gcc/recog.c	2006-05-04 10:19:02.000000000 +0100
+++ gcc-4.2.1-stm/gcc/recog.c	2007-08-10 13:08:58.000000000 +0100
@@ -1,6 +1,7 @@
 /* Subroutines used by or related to instruction recognition.
    Copyright (C) 1987, 1988, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998
    1999, 2000, 2001, 2002, 2003, 2004, 2005 Free Software Foundation, Inc.
+   Copyright (c) 2006  STMicroelectronics.
 
 This file is part of GCC.
 
diff -Naur gcc-4.2.1/gcc/recog.h gcc-4.2.1-stm/gcc/recog.h
--- gcc-4.2.1/gcc/recog.h	2006-05-04 10:19:02.000000000 +0100
+++ gcc-4.2.1-stm/gcc/recog.h	2007-08-10 13:08:58.000000000 +0100
@@ -1,6 +1,7 @@
 /* Declarations for interface to insn recognizer and insn-output.c.
    Copyright (C) 1987, 1996, 1997, 1998, 1999, 2000, 2001, 2003, 2004, 2005
    Free Software Foundation, Inc.
+   Copyright (c) 2006  STMicroelectronics.
 
 This file is part of GCC.
 
diff -Naur gcc-4.2.1/gcc/regmove.c gcc-4.2.1-stm/gcc/regmove.c
--- gcc-4.2.1/gcc/regmove.c	2006-07-26 23:51:56.000000000 +0100
+++ gcc-4.2.1-stm/gcc/regmove.c	2007-08-10 13:08:58.000000000 +0100
@@ -1,6 +1,7 @@
 /* Move registers around to reduce number of move instructions needed.
    Copyright (C) 1987, 1988, 1989, 1992, 1993, 1994, 1995, 1996, 1997, 1998,
    1999, 2000, 2001, 2002, 2003, 2004, 2005 Free Software Foundation, Inc.
+   Copyright (c) 2006  STMicroelectronics.
 
 This file is part of GCC.
 
@@ -43,6 +44,9 @@
 #include "except.h"
 #include "toplev.h"
 #include "reload.h"
+#include "obstack.h"
+#include "ggc.h"
+#include "optabs.h"
 #include "timevar.h"
 #include "tree-pass.h"
 
@@ -54,6 +58,13 @@
 #else
 #define STACK_GROWS_DOWNWARD 0
 #endif
+/* Likewise for AUTO_INC_DEC.  */
+#ifdef AUTO_INC_DEC
+#undef AUTO_INC_DEC
+#define AUTO_INC_DEC 1
+#else
+#define AUTO_INC_DEC 0
+#endif
 
 static int perhaps_ends_bb_p (rtx);
 static int optimize_reg_copy_1 (rtx, rtx, rtx);
@@ -82,6 +93,37 @@
 static int replacement_quality (rtx);
 static int fixup_match_2 (rtx, rtx, rtx, rtx);
 
+struct related;
+struct rel_use_chain;
+struct rel_mod;
+struct rel_use;
+
+static struct rel_use *lookup_related (int, enum reg_class, HOST_WIDE_INT, int);
+static void rel_build_chain (struct rel_use *, struct rel_use *,
+			     struct related *);
+static int recognize_related_for_insn (rtx, int, int);
+static void record_reg_use (rtx *, rtx, int, int);
+static struct rel_use *create_rel_use (rtx, rtx *, int, int, int);
+static void new_reg_use (rtx, rtx *, int, int, int, int);
+static void rel_record_mem (rtx *, rtx, int, int, int, rtx, int, int);
+static void new_base (rtx, rtx, int, int);
+static void invalidate_related (rtx, rtx, int, int);
+static void find_related (rtx *, rtx, int, int);
+static void find_related_toplev (rtx, int, int);
+static int chain_starts_earlier (const void *, const void *);
+static int chain_ends_later (const void *, const void *);
+static int mod_before (const void *, const void *);
+static void remove_setting_insns (struct related *, rtx);
+static rtx perform_addition (struct rel_mod *, struct rel_use *, rtx,
+			     struct rel_use_chain *);
+static void modify_address (struct rel_mod *, struct rel_use *, HOST_WIDE_INT);
+static void optimize_related_values_1 (struct related *, int, int, rtx);
+static void optimize_related_values_0 (struct related *, int, int, rtx);
+static void optimize_related_values (int);
+static void count_sets (rtx, rtx, void *);
+static int link_chains (struct rel_use_chain *, struct rel_use_chain *,
+			enum machine_mode);
+
 /* Return nonzero if registers with CLASS1 and CLASS2 can be merged without
    causing too much register allocation problems.  */
 static int
@@ -133,193 +175,2098 @@
 			       gen_rtx_fmt_e (inc_code, Pmode, reg), 1);
 	      if (apply_change_group ())
 		{
-		  /* If there is a REG_DEAD note on this insn, we must
-		     change this not to REG_UNUSED meaning that the register
-		     is set, but the value is dead.  Failure to do so will
-		     result in a sched1 dieing -- when it recomputes lifetime
-		     information, the number of REG_DEAD notes will have
-		     changed.  */
-		  rtx note = find_reg_note (insn, REG_DEAD, reg);
-		  if (note)
-		    PUT_MODE (note, REG_UNUSED);
+		  /* If there is a REG_DEAD note on this insn, we must
+		     change this not to REG_UNUSED meaning that the register
+		     is set, but the value is dead.  Failure to do so will
+		     result in a sched1 dieing -- when it recomputes lifetime
+		     information, the number of REG_DEAD notes will have
+		     changed.  */
+		  rtx note = find_reg_note (insn, REG_DEAD, reg);
+		  if (note)
+		    PUT_MODE (note, REG_UNUSED);
+
+		  REG_NOTES (insn)
+		    = gen_rtx_EXPR_LIST (REG_INC,
+					 reg, REG_NOTES (insn));
+		  if (! inc_insn_set)
+		    delete_insn (inc_insn);
+		  return 1;
+		}
+	    }
+	}
+    }
+  return 0;
+}
+
+/* Determine if the pattern generated by add_optab has a clobber,
+   such as might be issued for a flags hard register.  To make the
+   code elsewhere simpler, we handle cc0 in this same framework.
+
+   Return the register if one was discovered.  Return NULL_RTX if
+   if no flags were found.  Return pc_rtx if we got confused.  */
+
+static rtx
+discover_flags_reg (void)
+{
+  rtx tmp;
+  tmp = gen_rtx_REG (word_mode, 10000);
+  tmp = gen_add3_insn (tmp, tmp, const2_rtx);
+
+  /* If we get something that isn't a simple set, or a
+     [(set ..) (clobber ..)], this whole function will go wrong.  */
+  if (GET_CODE (tmp) == INSN)
+    tmp = PATTERN (tmp);
+  if (GET_CODE (tmp) == SET)
+    return NULL_RTX;
+  else if (GET_CODE (tmp) == PARALLEL)
+    {
+      int found;
+
+      if (XVECLEN (tmp, 0) != 2)
+	return pc_rtx;
+      tmp = XVECEXP (tmp, 0, 1);
+      if (GET_CODE (tmp) != CLOBBER)
+	return pc_rtx;
+      tmp = XEXP (tmp, 0);
+
+      /* Don't do anything foolish if the md wanted to clobber a
+	 scratch or something.  We only care about hard regs.
+	 Moreover we don't like the notion of subregs of hard regs.  */
+      if (GET_CODE (tmp) == SUBREG
+	  && REG_P (SUBREG_REG (tmp))
+	  && REGNO (SUBREG_REG (tmp)) < FIRST_PSEUDO_REGISTER)
+	return pc_rtx;
+      found = (REG_P (tmp) && REGNO (tmp) < FIRST_PSEUDO_REGISTER);
+
+      return (found ? tmp : NULL_RTX);
+    }
+
+  return pc_rtx;
+}
+
+/* It is a tedious task identifying when the flags register is live and
+   when it is safe to optimize.  Since we process the instruction stream
+   multiple times, locate and record these live zones by marking the
+   mode of the instructions --
+
+   QImode is used on the instruction at which the flags becomes live.
+
+   HImode is used within the range (exclusive) that the flags are
+   live.  Thus the user of the flags is not marked.
+
+   All other instructions are cleared to VOIDmode.  */
+
+/* Used to communicate with flags_set_1.  */
+static rtx flags_set_1_rtx;
+static int flags_set_1_set;
+
+static void
+mark_flags_life_zones (rtx flags)
+{
+  int flags_regno;
+  int flags_nregs;
+  basic_block block;
+
+#ifdef HAVE_cc0
+  /* If we found a flags register on a cc0 host, bail.  */
+  if (flags == NULL_RTX)
+    flags = cc0_rtx;
+  else if (flags != cc0_rtx)
+    flags = pc_rtx;
+#endif
+
+  /* Simple cases first: if no flags, clear all modes.  If confusing,
+     mark the entire function as being in a flags shadow.  */
+  if (flags == NULL_RTX || flags == pc_rtx)
+    {
+      enum machine_mode mode = (flags ? HImode : VOIDmode);
+      rtx insn;
+      for (insn = get_insns (); insn; insn = NEXT_INSN (insn))
+	PUT_MODE (insn, mode);
+      return;
+    }
+
+#ifdef HAVE_cc0
+  flags_regno = -1;
+  flags_nregs = 1;
+#else
+  flags_regno = REGNO (flags);
+  flags_nregs = hard_regno_nregs[flags_regno][GET_MODE (flags)];
+#endif
+  flags_set_1_rtx = flags;
+
+  /* Process each basic block.  */
+  FOR_EACH_BB_REVERSE (block)
+    {
+      rtx insn, end;
+      int live;
+
+      insn = BB_HEAD (block);
+      end = BB_END (block);
+
+      /* Look out for the (unlikely) case of flags being live across
+	 basic block boundaries.  */
+      live = 0;
+#ifndef HAVE_cc0
+      {
+	int i;
+	for (i = 0; i < flags_nregs; ++i)
+	  live |= REGNO_REG_SET_P (block->il.rtl->global_live_at_start,
+				   flags_regno + i);
+      }
+#endif
+
+      while (1)
+	{
+	  /* Process liveness in reverse order of importance --
+	     alive, death, birth.  This lets more important info
+	     overwrite the mode of lesser info.  */
+
+	  if (INSN_P (insn))
+	    {
+#ifdef HAVE_cc0
+	      /* In the cc0 case, death is not marked in reg notes,
+		 but is instead the mere use of cc0 when it is alive.  */
+	      if (live && reg_mentioned_p (cc0_rtx, PATTERN (insn)))
+		live = 0;
+#else
+	      /* In the hard reg case, we watch death notes.  */
+	      if (live && find_regno_note (insn, REG_DEAD, flags_regno))
+		live = 0;
+#endif
+	      PUT_MODE (insn, (live ? HImode : VOIDmode));
+
+	      /* In either case, birth is denoted simply by its presence
+		 as the destination of a set.  */
+	      flags_set_1_set = 0;
+	      note_stores (PATTERN (insn), flags_set_1, NULL);
+	      if (flags_set_1_set)
+		{
+		  live = 1;
+		  PUT_MODE (insn, QImode);
+		}
+	    }
+	  else
+	    PUT_MODE (insn, (live ? HImode : VOIDmode));
+
+	  if (insn == end)
+	    break;
+	  insn = NEXT_INSN (insn);
+	}
+    }
+}
+
+/* A subroutine of mark_flags_life_zones, called through note_stores.  */
+
+static void
+flags_set_1 (rtx x, rtx pat, void *data ATTRIBUTE_UNUSED)
+{
+  if (GET_CODE (pat) == SET
+      && reg_overlap_mentioned_p (x, flags_set_1_rtx))
+    flags_set_1_set = 1;
+}
+
+static GTY (()) rtx test_addr;
+
+/* Some machines have two-address-adds and instructions that can
+   use only register-indirect addressing and auto_increment, but no
+   offsets.  If multiple fields of a struct are accessed more than
+   once, cse will load each of the member addresses in separate registers.
+   This not only costs a lot of registers, but also of instructions,
+   since each add to initialize an address register must be really expanded
+   into a register-register move followed by an add.
+   regmove_optimize uses some heuristics to detect this case; if these
+   indicate that this is likely, optimize_related_values is run once for
+   the entire function.
+
+   We build chains of uses of related values that can be satisfied with the
+   same base register by taking advantage of auto-increment address modes
+   instead of explicit add instructions.
+
+   We try to link chains with disjoint lifetimes together to reduce the
+   number of temporary registers and register-register copies.
+
+   This optimization pass operates on basic blocks one at a time; it could be
+   extended to work on extended basic blocks or entire functions.  */
+
+/* For each set of values related to a common base register, we use a
+   hash table which maps constant offsets to instructions.
+
+   The instructions mapped to are those that use a register which may,
+   (possibly with a change in addressing mode) differ from the initial
+   value of the base register by exactly that offset after the
+   execution of the instruction.
+   Here we define the size of the hash table, and the hash function to use.  */
+#define REL_USE_HASH_SIZE 43
+#define REL_USE_HASH(I) ((I) % (unsigned HOST_WIDE_INT) REL_USE_HASH_SIZE)
+
+/* For each register in a set of registers that are related, we keep a
+   struct related.
+
+   BASE points to the struct related of the base register (i.e. the one
+   that was the source of the first three-address add for this set of
+   related values).
+
+   INSN is the instruction that initialized the register, or, for the
+   base, the instruction that initialized the first non-base register.
+
+   BASE is the register number of the base register.
+
+   For the base register only, the member BASEINFO points to some extra data.
+
+   'luid' here means linear uid.  We count them starting at the function
+   start; they are used to avoid overlapping lifetimes.
+
+   UPDATES is a list of instructions that set the register to a new
+   value that is still related to the same base.
+
+   When a register in a set of related values is set to something that
+   is not related to the base, INVALIDATE_LUID is set to the luid of
+   the instruction that does this set.  This is used to avoid re-using
+   this register in an overlapping liftime for a related value.
+
+   DEATH is first used to store the insn (if any) where the register dies.
+   When the optimization is actually performed, the REG_DEAD note from
+   the insn denoted by DEATH is removed.
+   Thereafter, the removed death note is stored in DEATH, marking not
+   only that the register dies, but also making the note available for reuse.
+
+   We also use a struct related to keep track of registers that have been
+   used for anything that we don't recognize as related values.
+   The only really interesting datum for these is u.last_luid, which is
+   the luid of the last reference we have seen.  These struct relateds
+   are marked by a zero INSN field; most other members are not used and
+   remain uninitialized.  */
+
+struct related
+{
+  rtx insn;
+  rtx reg;
+  struct related *base;
+  HOST_WIDE_INT offset;
+  struct related *prev;
+  struct update *updates;
+  struct related_baseinfo *baseinfo;
+  int invalidate_luid;
+  rtx death;
+  int reg_orig_calls_crossed;
+  int reg_set_call_tally;
+};
+
+/* HASHTAB maps offsets to register uses with a matching MATCH_OFFSET.
+   PREV_BASE points to the struct related for the previous base register
+   that we currently keep track of.
+   INSN_LUID is the luid of the instruction that started this set of
+   related values.  */
+struct related_baseinfo
+{
+  struct rel_use *hashtab[REL_USE_HASH_SIZE];
+  struct rel_use_chain *chains;
+  struct related *prev_base;
+  int insn_luid;
+};
+
+/* INSN is an instruction that sets a register that previously contained
+   a related value to a new value that is related to the same base register.
+   When the optimization is performed, we have to delete INSN.
+   DEATH_INSN points to the insn (if any) where the register died that we
+   set in INSN.  When we perform the optimization, the REG_DEAD note has
+   to be removed from DEATH_INSN.
+   PREV points to the struct update that pertains to the previous
+   instruction pertaining to the same register that set it from one
+   related value to another one.  */
+struct update
+{
+  rtx insn;
+  rtx death_insn;
+  struct update *prev;
+};
+
+struct rel_use_chain
+{
+  /* Points to first use in this chain.  */
+  struct rel_use *uses;
+  struct rel_use_chain *prev;
+  struct rel_use_chain *linked;
+  /* The following fields are only set after the chain has been completed:  */
+  /* Last use in this chain.  */
+  struct rel_use *end;
+  int start_luid;
+  int end_luid;
+  int calls_crossed;
+  /* The register allocated for this chain.  */
+  rtx reg;
+  /* The death note for this register.  */
+  rtx death_note;
+  /* Offset after execution of last insn.  */
+  HOST_WIDE_INT match_offset;
+  int invalidate_luid;
+};
+
+/* ADDRP points to the place where the actual use of the related value is.
+   This is commonly a memory address, and has to be set to a register
+   or some auto_inc addressing of this register.
+   But ADDRP is also used for all other uses of related values to
+   the place where the register is inserted; we can tell that an
+   unardorned register is to be inserted because no offset adjustment
+   is required, hence this is handled by the same logic as register-indirect
+   addressing.  The only exception to this is when SET_IN_PARALLEL is set,
+   see below.
+
+   OFFSET is the offset that is actually used in this instance, i.e.
+   the value of the base register when the set of related values was
+   created plus OFFSET yields the value that is used.
+   This might be different from the value of the used register before
+   executing INSN if we elected to use pre-{in,de}crement addressing.
+   If we have the option to use post-{in,de}crement addressing, all
+   choices are linked cyclically together with the SIBLING field.
+   Otherwise, it's a one-link-cycle, i.e. SIBLING points at the
+   struct rel_use it is a member of.
+
+   MATCH_OFFSET is the offset that is available after the execution
+   of INSN.  It is the same as OFFSET for straight register-indirect
+   addressing and for pre-{in,de}crement addressing, while it differs
+   for the post-{in,de}crement addressing modes.
+
+   If SET_IN_PARALLEL is set, MATCH_OFFSET differs from OFFSET, yet
+   this is no post-{in,de}crement addressing.  Rather, it is a set
+   inside a PARALLEL that adds some constant to a register that holds
+   one value of a set of related values that we keep track of.
+
+   NEXT_CHAIN is the link in a chain of rel_use structures.  If nonzero,
+   we will ignore this rel_use in a hash table lookup, since it has
+   already been appended to.  This field can point to its containing
+   rel_use; this means that we found a reason not to append to this
+   chain anymore (e.g. if a use comes with a clobber).
+
+   ADDRP then points only to the set destination of this set; another
+   struct rel_use is used for the source of the set.
+
+   NO_LINK_PRED is nonzero for the last use in a chain if it cannot be
+   the predecessor for a another chain to be linked to.  This can happen
+   for uses that come with a clobber, and for uses by a register that
+   is live at the end of the processed range of insns (usually a basic
+   block).  */
+
+struct rel_use
+{
+  rtx insn, *addrp;
+  int luid;
+  int call_tally;
+  enum reg_class class;
+  HOST_WIDE_INT offset;
+  HOST_WIDE_INT match_offset;
+  struct rel_use *next_chain;
+  struct rel_use **prev_chain_ref;
+  struct rel_use *next_hash;
+  struct rel_use *sibling;
+  unsigned int set_in_parallel:1;
+  unsigned int no_link_pred:1;
+};
+
+/* Describe a modification we have to do to the rtl when doing the
+   related value optimization.
+   There are two types of modifications: emitting a new add or move
+   insn, or updating an address within an existing insn.  We distinguish
+   between these two cases by testing whether the INSN field is nonzero.  */
+struct rel_mod
+{
+  /* Nonzero if we have to emit a new addition before this insn.
+     Otherwise, this describes an address update.  */
+  rtx insn;
+  /* The chain which this modification belongs to.  */
+  struct rel_use_chain *chain;
+  /* The position within the insn stream.  Used for sorting the set of
+     modifications in ascending order.  */
+  int luid;
+  /* Used to make the sort stable.  */
+  int count;
+  /* If this structure describes an addition, this is nonzero if the
+     source register is the base reg.  */
+  unsigned int from_base:1;
+};
+
+struct related **regno_related, *rel_base_list, *unrelatedly_used;
+
+#define rel_alloc(N) obstack_alloc(&related_obstack, (N))
+#define rel_new(X) ((X) = rel_alloc (sizeof *(X)))
+
+static struct obstack related_obstack;
+
+/* For each integer machine mode, the minimum and maximum constant that
+   can be added with a single constant.
+   This is supposed to define an interval around zero; if there are
+   singular points disconnected from this interval, we want to leave
+   them out.  */
+   
+static HOST_WIDE_INT add_limits[NUM_MACHINE_MODES][2];
+static char have_3addr_const_add[NUM_MACHINE_MODES];
+
+/* Try to find a related value with offset OFFSET from the base
+   register belonging to REGNO, using a register with preferred class
+   that is compatible with CLASS.  LUID is the insn in which we want
+   to use the matched register; this is used to avoid returning a
+   match that is an autoincrement within the same insn.  */
+
+static struct rel_use *
+lookup_related (int regno, enum reg_class class, HOST_WIDE_INT offset,
+		int luid)
+{
+  struct related *base = regno_related[regno]->base;
+  int hash = REL_USE_HASH (offset);
+  struct rel_use *match = base->baseinfo->hashtab[hash];
+  
+  for (; match; match = match->next_hash)
+    {
+      if (offset != match->match_offset)
+	continue;
+
+      /* If MATCH is an autoincrement in the same insn, ensure that it
+	 will not be used; otherwise we can end up with invalid rtl
+	 that uses the register outside the autoincrement.  */
+      if (match->luid == luid && match->offset != match->match_offset)
+	continue;
+
+      /* We are looking for a use which we can append to, so ignore
+	 anything that has already been appended to, and anything that
+	 must terminate a chain for other reasons.  */
+      if (match->next_chain)
+	continue;
+
+      if (regclass_compatible_p (class, match->class))
+	break;
+    }
+  
+  return match;
+}
+
+/* Add NEW_USE at the end of the chain that currently ends with MATCH;
+   If MATCH is not set, create a new chain.
+   BASE is the base register number the chain belongs to.  */
+
+static void
+rel_build_chain (struct rel_use *new_use, struct rel_use *match,
+		 struct related *base)
+{
+  int hash;
+
+  if (match)
+    {
+      struct rel_use *sibling = match;
+
+      do
+	{
+	  sibling->next_chain = new_use;
+	  if (sibling->prev_chain_ref)
+	    *sibling->prev_chain_ref = match;
+	  sibling = sibling->sibling;
+	}
+      while (sibling != match);
+
+      new_use->prev_chain_ref = &match->next_chain;
+    }
+  else
+    {
+      struct rel_use_chain *new_chain;
+
+      rel_new (new_chain);
+      new_chain->uses = new_use;
+      new_use->prev_chain_ref = &new_chain->uses;
+      new_chain->linked = 0;
+      new_chain->prev = base->baseinfo->chains;
+      base->baseinfo->chains = new_chain;
+    }
+  new_use->next_chain = 0;
+
+  hash = REL_USE_HASH (new_use->offset);
+  new_use->next_hash = base->baseinfo->hashtab[hash];
+  base->baseinfo->hashtab[hash] = new_use;
+}
+
+static struct rel_use *
+create_rel_use (rtx insn, rtx *xp, int regno, int luid, int call_tally)
+{
+  struct rel_use *new_use;
+  HOST_WIDE_INT offset = regno_related[regno]->offset;
+  enum reg_class class = reg_preferred_class (regno);
+
+  rel_new (new_use);
+  new_use->insn = insn;
+  new_use->addrp = xp;
+  new_use->luid = luid;
+  new_use->call_tally = call_tally;
+  new_use->class = class;
+  new_use->set_in_parallel = 0;
+  new_use->offset = offset;
+  new_use->match_offset = offset;
+  new_use->sibling = new_use;
+  new_use->no_link_pred = 0;
+
+  return new_use;
+}
+
+/* Record a new use of reg REGNO, which is found at address XP in INSN.
+   LUID and CALL_TALLY correspond to INSN.
+
+   There is a special case for uses of REGNO that must be preserved and
+   can't be optimized.  This case can happen either if we reach the end
+   of a block and a register which we track is still live, or if we find
+   a use of that register that can't be replaced inside an insn.  In
+   either case, TERMINATE should be set to a nonzero value.  */
+
+static void
+new_reg_use (rtx insn, rtx *xp, int regno, int luid, int call_tally,
+	     int terminate)
+{
+  struct rel_use *new_use, *match;
+  HOST_WIDE_INT offset = regno_related[regno]->offset;
+  enum reg_class class = reg_preferred_class (regno);
+  struct related *base = regno_related[regno]->base;
+
+  new_use = create_rel_use (insn, xp, regno, luid, call_tally);
+  match = lookup_related (regno, class, offset, luid);
+
+  rel_build_chain (new_use, match, base);
+  if (terminate)
+    new_use->next_chain = new_use;
+}
+
+/* Record the use of register ADDR in a memory reference.
+   ADDRP is the memory location where the address is stored.
+   MEM_MODE is the mode of the enclosing MEM.
+   SIZE is the size of the memory reference.
+   PRE_OFFS is the offset that has to be added to the value in ADDR
+   due to PRE_{IN,DE}CREMENT addressing in the original address; likewise,
+   POST_OFFSET denotes POST_{IN,DE}CREMENT addressing.  INSN is the
+   instruction that uses this address, LUID its luid, and CALL_TALLY
+   the current number of calls encountered since the start of the
+   function.  */
+      
+static void
+rel_record_mem (rtx *addrp, rtx addr, int size, int pre_offs, int post_offs,
+		rtx insn, int luid, int call_tally)
+{
+  rtx orig_addr = *addrp;
+  int regno;
+  struct related *base;
+  HOST_WIDE_INT offset;
+  struct rel_use *new_use, *match;
+  int hash;
+
+  gcc_assert (REG_P (addr));
+  
+  regno = REGNO (addr);
+
+  if (! regno_related[regno] || regno_related[regno]->invalidate_luid)
+    {
+      invalidate_related (addr, insn, luid, call_tally);
+      return;
+    }
+
+  offset = regno_related[regno]->offset += pre_offs;
+  base = regno_related[regno]->base;
+
+  if (base == 0)
+    return;
+
+  if (! test_addr)
+    test_addr = gen_rtx_PLUS (Pmode, addr, const0_rtx);
+
+  XEXP (test_addr, 0) = addr;
+  *addrp = test_addr;
+
+  new_use = create_rel_use (insn, addrp, regno, luid, call_tally);
+
+  match = lookup_related (regno, new_use->class, offset, luid);
+
+  /* Skip all the autoinc stuff if we found a match within the same insn.  */
+  if (match && match->luid == luid)
+    goto found_match;
+
+  if (! match)
+    {
+      /* We can choose PRE_{IN,DE}CREMENT on the spot with the information
+	 we have gathered about the preceding instructions, while we have
+	 to record POST_{IN,DE}CREMENT possibilities so that we can check
+	 later if we have a use for their output value.  */
+      /* We use recog here directly because we are only testing here if
+	 the changes could be made, but don't really want to make a
+	 change right now.  The caching from recog_memoized would only
+	 get in the way.  */
+
+      if (HAVE_PRE_INCREMENT)
+	{
+	  rtx p = PATTERN (insn);
+	  if (GET_CODE (p) == PARALLEL)
+	    p = XVECEXP (p, 0, 0);
+	  if (GET_CODE (p) != SET
+	      || !reg_overlap_mentioned_p (addr, XEXP (p, 1)))
+	    {
+	      match = lookup_related (regno, new_use->class, offset - size, luid);
+	      PUT_CODE (test_addr, PRE_INC);
+	      if (match && match->luid != luid
+		  && recog (PATTERN (insn), insn, NULL) >= 0)
+		goto found_match;
+	    }
+	}
+
+      if (HAVE_PRE_DECREMENT)
+	{
+	  rtx p = PATTERN (insn);
+	  if (GET_CODE (p) == PARALLEL)
+	    p = XVECEXP (p, 0, 0);
+	  if (GET_CODE (p) != SET
+	      || !reg_overlap_mentioned_p (addr, XEXP (p, 1)))
+	    {
+	      match = lookup_related (regno, new_use->class, offset + size, luid);
+	      PUT_CODE (test_addr, PRE_DEC);
+	      if (match && match->luid != luid
+		  && recog (PATTERN (insn), insn, NULL) >= 0)
+		goto found_match;
+	    }
+	}
+
+      match = 0;
+    }
+
+  PUT_CODE (test_addr, POST_INC);
+      
+  if (HAVE_POST_INCREMENT && recog (PATTERN (insn), insn, NULL) >= 0)
+    {
+      struct rel_use *inc_use;
+
+      rel_new (inc_use);
+      *inc_use = *new_use;
+      inc_use->sibling = new_use;
+      new_use->sibling = inc_use;
+      inc_use->prev_chain_ref = NULL;
+      inc_use->next_chain = NULL;
+      hash = REL_USE_HASH (inc_use->match_offset = offset + size);
+      inc_use->next_hash = base->baseinfo->hashtab[hash];
+      base->baseinfo->hashtab[hash] = inc_use;
+    }
+
+  PUT_CODE (test_addr, POST_DEC);
+
+  if (HAVE_POST_DECREMENT && recog (PATTERN (insn), insn, NULL) >= 0)
+    {
+      struct rel_use *dec_use;
+
+      rel_new (dec_use);
+      *dec_use = *new_use;
+      dec_use->sibling = new_use->sibling;
+      new_use->sibling = dec_use;
+      dec_use->prev_chain_ref = NULL;
+      dec_use->next_chain = NULL;
+      hash = REL_USE_HASH (dec_use->match_offset = offset + size);
+      dec_use->next_hash = base->baseinfo->hashtab[hash];
+      base->baseinfo->hashtab[hash] = dec_use;
+    }
+
+ found_match:
+      
+  rel_build_chain (new_use, match, base);
+  *addrp = orig_addr;
+
+  regno_related[regno]->offset += post_offs;
+}
+
+/* Note that REG is set to something that we do not regognize as a
+   related value, at an insn with linear uid LUID.  */
+
+static void
+invalidate_related (rtx reg, rtx insn, int luid, int call_tally)
+{
+  int regno = REGNO (reg);
+  struct related *rel = regno_related[regno];
+  if (rel && rel->base)
+    {
+      rel->invalidate_luid = luid;
+      rel->reg_orig_calls_crossed = call_tally - rel->reg_set_call_tally;
+    }
+  if (! rel || rel->base)
+    {
+      rel_new (rel);
+      regno_related[regno] = rel;
+      rel->prev = unrelatedly_used;
+      unrelatedly_used = rel;
+      rel->reg = reg;
+      rel->base = NULL;
+    }
+  rel->invalidate_luid = luid;
+  rel->insn = insn;
+}
+
+/* Record REG as a new base for related values.  INSN is the insn in which
+   we found it, LUID is its luid, and CALL_TALLY the number of calls seen
+   up to this point.  */
+
+static void
+new_base (rtx reg, rtx insn, int luid, int call_tally)
+{
+  int regno = REGNO (reg);
+  struct related *new_related;
+
+  rel_new (new_related);
+  new_related->reg = reg;
+  new_related->insn = insn;
+  new_related->updates = 0;
+  new_related->reg_set_call_tally = call_tally;
+  new_related->base = new_related;
+  new_related->offset = 0;
+  new_related->prev = 0;
+  new_related->invalidate_luid = 0;
+  new_related->death = NULL_RTX;
+  rel_new (new_related->baseinfo);
+  memset ((char *) new_related->baseinfo, 0, sizeof *new_related->baseinfo);
+  new_related->baseinfo->prev_base = rel_base_list;
+  rel_base_list = new_related;
+  new_related->baseinfo->insn_luid = luid;
+  regno_related[regno] = new_related;
+}
+
+/* Check out if INSN sets a new related value.  Return nonzero if we could
+   handle this insn.  */
+
+static int
+recognize_related_for_insn (rtx insn, int luid, int call_tally)
+{
+  rtx set = single_set (insn);
+  rtx src, dst;
+  rtx src_reg, src_const;
+  int src_regno, dst_regno;
+  struct related *new_related;
+
+  /* We don't care about register class differences here, since
+     we might still find multiple related values share the same
+     class even if it is disjunct from the class of the original
+     register.  */
+
+  if (set == 0)
+    return 0;
+
+  dst = SET_DEST (set);
+  src = SET_SRC (set);
+
+  /* First check that we have actually something like
+     (set (reg pseudo_dst) (plus (reg pseudo_src) (const_int))) .  */
+  if (GET_CODE (src) == PLUS)
+    {
+      src_reg = XEXP (src, 0);
+      src_const = XEXP (src, 1);
+    }
+  else if (REG_P (src) && GET_MODE_CLASS (GET_MODE (src)) == MODE_INT)
+    {
+      src_reg = src;
+      src_const = const0_rtx;
+    }
+  else
+    return 0;
+
+  if (!REG_P (src_reg) || GET_CODE (src_const) != CONST_INT || !REG_P (dst))
+    return 0;
+
+  dst_regno = REGNO (dst);
+  src_regno = REGNO (src_reg);
+
+  if (src_regno < FIRST_PSEUDO_REGISTER
+      || dst_regno < FIRST_PSEUDO_REGISTER)
+    return 0;
+
+  /* Check if this is merely an update of a register with a
+     value belonging to a group of related values we already
+     track.  */
+  if (regno_related[dst_regno] && ! regno_related[dst_regno]->invalidate_luid)
+    {
+      struct update *new_update;
+
+      /* If the base register changes, don't handle this as a
+	 related value.  We can currently only attribute the
+	 register to one base, and keep record of one lifetime
+	 during which we might re-use the register.  */
+      if (! regno_related[src_regno]
+	  || regno_related[src_regno]->invalidate_luid
+	  || (regno_related[dst_regno]->base
+	      != regno_related[src_regno]->base))
+	return 0;
+
+      regno_related[dst_regno]->offset
+	= regno_related[src_regno]->offset + INTVAL (src_const);
+      rel_new (new_update);
+      new_update->insn = insn;
+      new_update->death_insn = regno_related[dst_regno]->death;
+      regno_related[dst_regno]->death = NULL_RTX;
+      new_update->prev = regno_related[dst_regno]->updates;
+      regno_related[dst_regno]->updates = new_update;
+
+      return 1;
+    }
+
+  if (! regno_related[src_regno] || regno_related[src_regno]->invalidate_luid)
+    {
+      if (src_regno == dst_regno)
+	return 0;
+
+      new_base (src_reg, insn, luid, call_tally);
+    }
+  /* If the destination register has been used since we started
+     tracking this group of related values, there would be tricky
+     lifetime problems that we don't want to tackle right now.  */
+  else if (regno_related[dst_regno]
+	   && (regno_related[dst_regno]->invalidate_luid
+	       >= regno_related[src_regno]->base->baseinfo->insn_luid))
+    return 0;
+
+  rel_new (new_related);
+  new_related->reg = dst;
+  new_related->insn = insn;
+  new_related->updates = 0;
+  new_related->reg_set_call_tally = call_tally;
+  new_related->base = regno_related[src_regno]->base;
+  new_related->offset = regno_related[src_regno]->offset + INTVAL (src_const);
+  new_related->invalidate_luid = 0;
+  new_related->death = NULL_RTX;
+  new_related->prev = regno_related[src_regno]->prev;
+  regno_related[src_regno]->prev = new_related;
+  regno_related[dst_regno] = new_related;
+
+  return 1;
+}
+
+/* Record a use of a register at *XP, which is not inside a MEM which we
+   consider changing except for plain register substitution.  */
+static void
+record_reg_use (rtx *xp, rtx insn, int luid, int call_tally)
+{
+  rtx x = *xp;
+  int regno = REGNO (x);
+	
+  if (! regno_related[regno])
+    {
+      rel_new (regno_related[regno]);
+      regno_related[regno]->prev = unrelatedly_used;
+      unrelatedly_used = regno_related[regno];
+      regno_related[regno]->reg = x;
+      regno_related[regno]->base = NULL;
+      regno_related[regno]->invalidate_luid = luid;
+      regno_related[regno]->insn = insn;
+    }
+  else if (regno_related[regno]->invalidate_luid)
+    {
+      regno_related[regno]->invalidate_luid = luid;
+      regno_related[regno]->insn = insn;
+    }
+  else
+    new_reg_use (insn, xp, regno, luid, call_tally, 0);
+}
+
+/* Check the RTL fragment pointed to by XP for related values - that is,
+   if any new are created, or if they are assigned new values.  Also
+   note any other sets so that we can track lifetime conflicts.
+   INSN is the instruction XP points into, LUID its luid, and CALL_TALLY
+   the number of preceding calls in the function.  */
+
+static void
+find_related (rtx *xp, rtx insn, int luid, int call_tally)
+{
+  rtx x = *xp;
+  enum rtx_code code = GET_CODE (x);
+  const char *fmt;
+  int i;
+
+  if (code == REG)
+    record_reg_use (xp, insn, luid, call_tally);
+  else if (code == MEM)
+    {
+      enum machine_mode mem_mode = GET_MODE (x);
+      int size = GET_MODE_SIZE (mem_mode);
+      rtx *addrp= &XEXP (x, 0), addr = *addrp;
+
+      switch (GET_CODE (addr))
+	{
+	case REG:
+	  rel_record_mem (addrp, addr, size, 0, 0,
+			  insn, luid, call_tally);
+	  return;
+	case PRE_INC:
+	  rel_record_mem (addrp, XEXP (addr, 0), size, size, 0,
+			  insn, luid, call_tally);
+	  return;
+	case POST_INC:
+	  rel_record_mem (addrp, XEXP (addr, 0), size, 0, size,
+			  insn, luid, call_tally);
+	  return;
+	case PRE_DEC:
+	  rel_record_mem (addrp, XEXP (addr, 0), size, -size, 0,
+			  insn, luid, call_tally);
+	  return;
+	case POST_DEC:
+	  rel_record_mem (addrp, XEXP (addr, 0), size, 0, -size,
+			  insn, luid, call_tally);
+	  return;
+	default:
+	  break;
+	}
+    }
+  
+  fmt = GET_RTX_FORMAT (code);
+
+  for (i = GET_RTX_LENGTH (code) - 1; i >= 0; i--)
+    {
+      if (fmt[i] == 'e')
+	find_related (&XEXP (x, i), insn, luid, call_tally);
+      
+      if (fmt[i] == 'E')
+	{
+	  register int j;
+	  
+	  for (j = 0; j < XVECLEN (x, i); j++)
+	    find_related (&XVECEXP (x, i, j), insn, luid, call_tally);
+	}
+    }
+}
+
+/* Process one insn for optimize_related_values.  INSN is the insn, LUID
+   and CALL_TALLY its corresponding luid and number of calls seen so
+   far.  */
+static void
+find_related_toplev (rtx insn, int luid, int call_tally)
+{
+  int i;
+
+  /* First try to process the insn as a whole.  */
+  if (recognize_related_for_insn (insn, luid, call_tally))
+    return;
+
+  if (GET_CODE (PATTERN (insn)) == USE
+      || GET_CODE (PATTERN (insn)) == CLOBBER)
+    {
+      rtx *xp = &XEXP (PATTERN (insn), 0);
+      int regno;
+      
+      if (!REG_P (*xp))
+	{
+	  find_related (xp, insn, luid, call_tally);
+	  return;
+	}
+
+      regno = REGNO (*xp);
+      if (GET_CODE (PATTERN (insn)) == USE
+	  && regno_related[regno]
+	  && ! regno_related[regno]->invalidate_luid)
+	new_reg_use (insn, xp, regno, luid, call_tally, 1);
+      invalidate_related (*xp, insn, luid, call_tally);
+      return;
+    }
+
+  if (CALL_P (insn) && CALL_INSN_FUNCTION_USAGE (insn))
+    {
+      rtx usage;
+
+      for (usage = CALL_INSN_FUNCTION_USAGE (insn);
+	   usage;
+	   usage = XEXP (usage, 1))
+	find_related (&XEXP (usage, 0), insn, luid, call_tally);
+    }
+
+  extract_insn (insn);
+  /* Process all inputs.  */
+  for (i = 0; i < recog_data.n_operands; i++)
+    {
+      rtx *loc = recog_data.operand_loc[i];
+      rtx op = *loc;
+
+      if (op == NULL)
+	continue;
+
+      while (GET_CODE (op) == SUBREG
+	     || GET_CODE (op) == ZERO_EXTRACT
+	     || GET_CODE (op) == SIGN_EXTRACT
+	     || GET_CODE (op) == STRICT_LOW_PART)
+	loc = &XEXP (op, 0), op = *loc;
+
+      if (recog_data.operand_type[i] == OP_IN || !REG_P (op))
+	find_related (loc, insn, luid, call_tally);
+    }
+
+
+  /* If we have an OP_IN type operand with match_dups, process those
+     duplicates also.  */
+  for (i = 0; i < recog_data.n_dups; i++)
+    {
+      int opno = recog_data.dup_num[i];
+      rtx *loc = recog_data.dup_loc[i];
+      rtx op = *loc;
+
+      while (GET_CODE (op) == SUBREG
+	     || GET_CODE (op) == ZERO_EXTRACT
+	     || GET_CODE (op) == SIGN_EXTRACT
+	     || GET_CODE (op) == STRICT_LOW_PART)
+	loc = &XEXP (op, 0), op = *loc;
+
+      if (recog_data.operand_type[opno] == OP_IN || !REG_P (op))
+	find_related (loc, insn, luid, call_tally);
+    }
+  
+  /* Process outputs.  */
+  for (i = 0; i < recog_data.n_operands; i++)
+    {
+      enum op_type type = recog_data.operand_type[i];
+      rtx *loc = recog_data.operand_loc[i];
+      rtx op = *loc;
+
+      if (op == NULL)
+	continue;
+
+      while (GET_CODE (op) == SUBREG
+	     || GET_CODE (op) == ZERO_EXTRACT
+	     || GET_CODE (op) == SIGN_EXTRACT
+	     || GET_CODE (op) == STRICT_LOW_PART)
+	loc = &XEXP (op, 0), op = *loc;
+
+      /* Detect if we're storing into only one word of a multiword
+	 subreg.  */
+      if (loc != recog_data.operand_loc[i] && type == OP_OUT)
+	type = OP_INOUT;
+
+      if (REG_P (op))
+	{
+	  int regno = REGNO (op);
+
+	  if (type == OP_INOUT)
+	    {							
+	      /* This is a use we can't handle.  Add a dummy use of this
+		 register as well as invalidating it.  */
+	      if (regno_related[regno]
+		  && ! regno_related[regno]->invalidate_luid)
+		new_reg_use (insn, loc, regno, luid, call_tally, 1);
+	    }
+
+	  if (type != OP_IN)
+	    /* A set of a register invalidates it (unless the set was
+	       handled by recognize_related_for_insn).  */
+	    invalidate_related (op, insn, luid, call_tally);
+	}
+    }
+}
+
+/* Comparison functions for qsort.  */
+static int
+chain_starts_earlier (const void *chain1, const void *chain2)
+{
+  int d = ((*(struct rel_use_chain **)chain2)->start_luid
+	   - (*(struct rel_use_chain **)chain1)->start_luid);
+  if (! d)
+    d = ((*(struct rel_use_chain **)chain2)->uses->offset
+         - (*(struct rel_use_chain **)chain1)->uses->offset);
+  if (! d)
+    d = ((*(struct rel_use_chain **)chain2)->uses->set_in_parallel
+         - (*(struct rel_use_chain **)chain1)->uses->set_in_parallel);
+  
+  /* If set_in_parallel is not set on both chain's first use, they must
+     differ in start_luid or offset, since otherwise they would use the
+     same chain.
+     Thus the remaining problem is with set_in_parallel uses; for these, we
+     know that *addrp is a register.  Since the same register may not be set
+     multiple times in the same insn, the registers must be different.  */
+     
+  if (! d)
+    d = (REGNO (*(*(struct rel_use_chain **)chain2)->uses->addrp)
+         - REGNO (*(*(struct rel_use_chain **)chain1)->uses->addrp));
+  return d;
+}
+
+static int
+chain_ends_later (const void *chain1, const void *chain2)
+{
+  int d = ((*(struct rel_use_chain **)chain1)->end->no_link_pred
+	   - (*(struct rel_use_chain **)chain2)->end->no_link_pred);
+  if (! d)
+    d = ((*(struct rel_use_chain **)chain1)->end_luid
+	 - (*(struct rel_use_chain **)chain2)->end_luid);
+  if (! d)
+    d = ((*(struct rel_use_chain **)chain2)->uses->offset
+         - (*(struct rel_use_chain **)chain1)->uses->offset);
+  if (! d)
+    d = ((*(struct rel_use_chain **)chain2)->uses->set_in_parallel
+         - (*(struct rel_use_chain **)chain1)->uses->set_in_parallel);
+  
+  /* If set_in_parallel is not set on both chain's first use, they must
+     differ in start_luid or offset, since otherwise they would use the
+     same chain.
+     Thus the remaining problem is with set_in_parallel uses; for these, we
+     know that *addrp is a register.  Since the same register may not be set
+     multiple times in the same insn, the registers must be different.  */
+     
+  if (! d)
+    {
+      rtx reg1 = (*(*(struct rel_use_chain **)chain1)->uses->addrp);
+      rtx reg2 = (*(*(struct rel_use_chain **)chain2)->uses->addrp);
+
+      switch (GET_CODE (reg1))
+	{
+	case REG:
+	  break;
+
+	case PRE_INC:
+	case POST_INC:
+	case PRE_DEC:
+	case POST_DEC:
+	  reg1 = XEXP (reg1, 0);
+	  break;
+
+	default:
+	  gcc_unreachable ();
+	}
+
+      switch (GET_CODE (reg2))
+	{
+	case REG:
+	  break;
+
+	case PRE_INC:
+	case POST_INC:
+	case PRE_DEC:
+	case POST_DEC:
+	  reg2 = XEXP (reg2, 0);
+	  break;
+
+	default:
+	  gcc_unreachable ();
+	}
+
+	d = (REGNO (reg2) - REGNO (reg1));
+    }
+  return d;
+}
+
+/* Called through qsort, used to sort rel_mod structures in ascending
+   order by luid.  */
+static int
+mod_before (const void *ptr1, const void *ptr2)
+{
+  const struct rel_mod *insn1 = ptr1;
+  const struct rel_mod *insn2 = ptr2;
+  if (insn1->luid != insn2->luid)
+    return insn1->luid - insn2->luid;
+  /* New add insns get inserted before the luid, modifications are
+     performed within this luid.  */
+  if (insn1->insn == 0 && insn2->insn != 0)
+    return 1;
+  if (insn2->insn == 0 && insn1->insn != 0)
+    return -1;
+  return insn1->count - insn2->count;
+}
+
+/* Update REG_N_SETS given a newly generated insn.  Called through
+   note_stores.  */
+static void
+count_sets (rtx x, rtx pat ATTRIBUTE_UNUSED, void *data ATTRIBUTE_UNUSED)
+{
+  if (REG_P (x))
+    REG_N_SETS (REGNO (x))++;
+}
+
+/* First pass of performing the optimization on a set of related values:
+   remove all the setting insns, death notes and refcount increments that
+   are now obsolete.
+   INSERT_BEFORE is an insn which we not must delete except by by turning it
+   into a note, since it is needed later.  */
+static void
+remove_setting_insns (struct related *rel_base, rtx insert_before)
+{
+  struct related *rel;
+
+  for (rel = rel_base; rel; rel = rel->prev)
+    {
+      struct update *update;
+      int regno = REGNO (rel->reg);
+
+      if (rel != rel_base)
+	{
+	  /* The first setting insn might be the start of a basic block.  */
+	  if (rel->insn == rel_base->insn
+	      /* We have to preserve insert_before.  */
+	      || rel->insn == insert_before)
+	    {
+	      PUT_CODE (rel->insn, NOTE);
+	      NOTE_LINE_NUMBER (rel->insn) = NOTE_INSN_DELETED;
+	      NOTE_SOURCE_FILE (rel->insn) = 0;
+	    }
+	  else
+	    delete_insn (rel->insn);
+	  REG_N_SETS (regno)--;
+	}
+
+      REG_N_CALLS_CROSSED (regno) -= rel->reg_orig_calls_crossed;
+	  
+      for (update = rel->updates; update; update = update->prev)
+	{
+	  rtx death_insn = update->death_insn;
+	      
+	  if (death_insn)
+	    {
+	      rtx death_note
+		= find_reg_note (death_insn, REG_DEAD, rel->reg);
+	      if (! death_note)
+		death_note
+		  = find_reg_note (death_insn, REG_UNUSED, rel->reg);
+	      remove_note (death_insn, death_note);
+	      REG_N_DEATHS (regno)--;
+	    }
+	      
+	  /* We have to preserve insert_before.  */
+	  if (update->insn == insert_before)
+	    {
+	      PUT_CODE (update->insn, NOTE);
+	      NOTE_LINE_NUMBER (update->insn) = NOTE_INSN_DELETED;
+	      NOTE_SOURCE_FILE (update->insn) = 0;
+	    }
+	  else
+	    delete_insn (update->insn);
+	      
+	  REG_N_SETS (regno)--;
+	}
+	  
+      if (rel->death)
+	{
+	  rtx death_note = find_reg_note (rel->death, REG_DEAD, rel->reg);
+	  if (! death_note)
+	    death_note = find_reg_note (rel->death, REG_UNUSED, rel->reg);
+	  remove_note (rel->death, death_note);
+	  rel->death = death_note;
+	  REG_N_DEATHS (regno)--;
+	}
+    }
+}
+
+/* Create a new add (or move) instruction as described by the modification
+   MOD, which is for the rel_use USE.  BASE_REG is the base register for
+   this set of related values, REL_BASE_REG_USER is the chain that uses
+   it.  */
+static rtx
+perform_addition (struct rel_mod *mod, struct rel_use *use, rtx base_reg,
+		  struct rel_use_chain *rel_base_reg_user)
+{
+  HOST_WIDE_INT use_offset = use->offset;
+  /* We have to generate a new addition or move insn and emit it
+     before the current use in this chain.  */
+  HOST_WIDE_INT new_offset = use_offset;
+  rtx reg = mod->chain->reg;
+  rtx src_reg;
+
+  if (mod->from_base)
+    {
+      src_reg = base_reg;
+      if (rel_base_reg_user)
+	use_offset -= rel_base_reg_user->match_offset;
+    }
+  else
+    {
+      src_reg = reg;
+      use_offset -= mod->chain->match_offset;
+    }
+
+  if (use_offset != 0 || src_reg != reg)
+    {
+      rtx new;
+      if (use_offset == 0)
+	new = gen_move_insn (reg, src_reg);
+      else
+	new = gen_add3_insn (reg, src_reg, GEN_INT (use_offset));
+
+      gcc_assert (new);
+
+      if (GET_CODE (new) == SEQUENCE)
+	{
+	  int i;
+
+	  for (i = XVECLEN (new, 0) - 1; i >= 0; i--)
+	    note_stores (PATTERN (XVECEXP (new, 0, i)), count_sets,
+			 NULL);
+	}
+      else
+	note_stores (new, count_sets, NULL);
+      new = emit_insn_before (new, mod->insn);
+
+      mod->chain->match_offset = new_offset;
+      return new;
+    }
+  return 0;
+}
+
+/* Perform the modification described by MOD, which applies to the use
+   described by USE.
+   This function calls validate_change; the caller must call
+   apply_change_group after all modifications for the same insn have
+   been performed.  */
+static void
+modify_address (struct rel_mod *mod, struct rel_use *use,
+		HOST_WIDE_INT current_offset)
+{
+  HOST_WIDE_INT use_offset = use->offset;
+  rtx reg = mod->chain->reg;
+  /* We have to perform a modification on a given use.  The
+     current use will be removed from the chain afterwards.  */
+  rtx addr = *use->addrp;
+
+  if (!REG_P (addr))
+    remove_note (use->insn,
+		 find_reg_note (use->insn, REG_INC,
+				XEXP (addr, 0)));
+
+  if (use_offset == current_offset)
+    {
+      if (use->set_in_parallel)
+	{
+	  REG_N_SETS (REGNO (addr))--;
+	  addr = reg;
+	}
+      else if (use->match_offset > use_offset)
+	addr = gen_rtx_POST_INC (Pmode, reg);
+      else if (use->match_offset < use_offset)
+	addr = gen_rtx_POST_DEC (Pmode, reg);
+      else
+	addr = reg;
+    }
+  else if (use_offset > current_offset)
+    addr = gen_rtx_PRE_INC (Pmode, reg);
+  else
+    addr = gen_rtx_PRE_DEC (Pmode, reg);
+
+  /* Group changes from the same chain for the same insn
+     together, to avoid failures for match_dups.  */
+  validate_change (use->insn, use->addrp, addr, 1);
+
+  if (addr != reg)
+    REG_NOTES (use->insn)
+      = gen_rtx_EXPR_LIST (REG_INC, reg, REG_NOTES (use->insn));
+
+  /* Update the chain's state: set match_offset as appropriate,
+     and move towards the next use.  */
+  mod->chain->match_offset = use->match_offset;
+  mod->chain->uses = use->next_chain;
+  if (mod->chain->uses == 0 && mod->chain->linked)
+    {
+      struct rel_use_chain *linked = mod->chain->linked;
+      mod->chain->linked = linked->linked;
+      mod->chain->uses = linked->uses;
+    }
+}
+
+/* Try to link SUCC_CHAIN as sucessor of PRED_CHAIN.  BASE_MODE is
+   the machine mode of the base register.  Return nonzero on success.  */
+static int
+link_chains (struct rel_use_chain *pred_chain,
+	     struct rel_use_chain *succ_chain, enum machine_mode base_mode)
+{
+  if (succ_chain->start_luid > pred_chain->end_luid
+      && ! pred_chain->end->no_link_pred
+      && (! pred_chain->invalidate_luid
+	  || pred_chain->invalidate_luid > succ_chain->end_luid)
+      && regclass_compatible_p (succ_chain->uses->class,
+				pred_chain->uses->class)
+      /* add_limits is not valid for MODE_PARTIAL_INT .  */
+      && GET_MODE_CLASS (base_mode) == MODE_INT
+      && !have_3addr_const_add[(int) base_mode]
+      && (succ_chain->uses->offset - pred_chain->match_offset
+	  >= add_limits[(int) base_mode][0])
+      && (succ_chain->uses->offset - pred_chain->match_offset
+	  <= add_limits[(int) base_mode][1]))
+    {
+      /* We can link these chains together.  */
+      pred_chain->linked = succ_chain;
+      succ_chain->start_luid = 0;
+      pred_chain->end_luid = succ_chain->end_luid;
+      return 1;
+    }
+  return 0;
+}
+
+/* Perform the optimization for a single set of related values.
+   INSERT_BEFORE is an instruction before which we may emit instructions
+   to initialize registers that remain live beyond the end of the group
+   of instructions which have been examined.  */
+
+static void
+optimize_related_values_1 (struct related *rel_base, int luid, int call_tally,
+			   rtx insert_before)
+{
+  struct related_baseinfo *baseinfo = rel_base->baseinfo;
+  struct related *rel;
+  struct rel_use_chain *chain, **chain_starttab, **chain_endtab;
+  struct rel_use_chain **pred_chainp, *pred_chain;
+  int num_regs, num_av_regs, num_chains, num_linked, max_end_luid, i;
+  int max_start_luid;
+  struct rel_use_chain *rel_base_reg_user;
+  enum machine_mode mode;
+  HOST_WIDE_INT rel_base_reg_user_offset = 0;
+
+  /* For any registers that are still live, we have to arrange
+     to have them set to their proper values.
+     Also count with how many registers (not counting base) we are
+     dealing with here.  */
+  for (num_regs = -1, rel = rel_base; rel; rel = rel->prev, num_regs++)
+    {
+      int regno = REGNO (rel->reg);
+
+      if (! rel->death && ! rel->invalidate_luid)
+	{
+	  new_reg_use (insert_before, &rel->reg, regno, luid, call_tally, 1);
+	  rel->reg_orig_calls_crossed = call_tally - rel->reg_set_call_tally;
+	}
+    }
+
+  /* Now for every chain of values related to the base, set start
+     and end luid, match_offset, and reg.  Also count the number of these
+     chains, and determine the largest end luid.  */
+  num_chains = 0;
+  
+  for (max_end_luid = 0, chain = baseinfo->chains; chain; chain = chain->prev)
+    {
+      struct rel_use *use, *next;
+
+      num_chains++;
+      next = chain->uses;
+      chain->start_luid = next->luid;
+      do
+	{
+	  use = next;
+	  next = use->next_chain;
+	}
+      while (next && next != use);
+
+      use->no_link_pred = next != NULL;
+      use->next_chain = 0;
+
+      chain->end = use;
+      chain->end_luid = use->luid;
+      chain->match_offset = use->match_offset;
+      chain->calls_crossed = use->call_tally - chain->uses->call_tally;
+      
+      chain->reg = ! use->no_link_pred ? NULL_RTX : *use->addrp;
+
+      if (chain->reg)
+	{
+	  /* Copy if an invalidate_luid is attached to this register.
+	     ??? This loop makes the algorithm quadratic on the number
+	     of registers.  If this is a problem, the cost can be brought
+	     down to O(N*log(N)) by using sorting.  */
+	  for (rel = rel_base; rel; rel = rel->prev)
+	    if (rel->reg == chain->reg)
+	      {
+		chain->invalidate_luid = rel->invalidate_luid;
+		break;
+	      }
+	}
+      else
+	chain->invalidate_luid = 0;
+
+      if (use->luid > max_end_luid)
+	max_end_luid = use->luid;
+
+      if (dump_file)
+	fprintf (dump_file, "Chain start: %d end: %d\n",
+		 chain->start_luid, chain->end_luid);
+    }
+
+  if (dump_file)
+    fprintf (dump_file,
+	     "Insn %d reg %d: found %d chains.\n",
+	     INSN_UID (rel_base->insn), REGNO (rel_base->reg), num_chains);
+
+  if (! num_chains)
+    return;
+
+  /* For every chain, we try to find another chain the lifetime of which
+     ends before the lifetime of said chain starts.
+     So we first sort according to luid of first and last instruction that
+     is in the chain, respectively;  this is O(n * log n) on average.  */
+  chain_starttab = rel_alloc (num_chains * sizeof *chain_starttab);
+  chain_endtab = rel_alloc (num_chains * sizeof *chain_starttab);
+  
+  for (chain = baseinfo->chains, i = 0; chain; chain = chain->prev, i++)
+    {
+      chain_starttab[i] = chain;
+      chain_endtab[i] = chain;
+    }
+  
+  qsort (chain_starttab, num_chains, sizeof *chain_starttab,
+	 chain_starts_earlier);
+  qsort (chain_endtab, num_chains, sizeof *chain_endtab, chain_ends_later);
+
+  /* Now we go through every chain, starting with the one that starts
+     second (we can skip the first because we know there would be no match),
+     and check it against the chain that ends first.  */
+  /* ??? We assume here that reg_class_compatible_p will seldom return false.
+     If that is not true, we should do a more thorough search for suitable
+     chain combinations.  */
+  pred_chainp = chain_endtab;
+  pred_chain = *pred_chainp;
+  max_start_luid = chain_starttab[num_chains - 1]->start_luid;
+  
+  mode = GET_MODE (rel_base->reg);
+  for (num_linked = 0, i = num_chains - 2; i >= 0; i--)
+    {
+      struct rel_use_chain *succ_chain = chain_starttab[i];
+
+      if ((pred_chain->calls_crossed
+	   ? succ_chain->calls_crossed
+	   : succ_chain->end->call_tally == pred_chain->uses->call_tally)
+	  && link_chains (pred_chain, succ_chain, mode))
+	{
+	  num_linked++;
+	  pred_chain = *++pred_chainp;
+	}
+      else
+	max_start_luid = succ_chain->start_luid;
+    }
+
+  if (dump_file && num_linked)
+    fprintf (dump_file, "Linked to %d sets of chains.\n",
+	     num_chains - num_linked);
+
+  /* Now count the number of registers that are available for reuse.  */
+  /* ??? In rare cases, we might reuse more if we took different
+     end luids of the chains into account.  Or we could just allocate
+     some new regs.  But that would probably not be worth the effort.  */
+  /* ??? We should pay attention to preferred register classes here too,
+     if the to-be-allocated register have a life outside the range that
+     we handle.  */
+  for (num_av_regs = 0, rel = rel_base->prev; rel; rel = rel->prev)
+    {
+      if (! rel->invalidate_luid
+	  || rel->invalidate_luid > max_end_luid)
+	num_av_regs++;
+    }
+
+  /* Propagate mandatory register assignments to the first chain in
+     all sets of linked chains, and set rel_base_reg_user.  */
+  for (rel_base_reg_user = 0, i = 0; i < num_chains; i++)
+    {
+      struct rel_use_chain *chain = chain_starttab[i];
+      if (chain->linked)
+	chain->reg = chain->linked->reg;
+      if (chain->reg == rel_base->reg)
+	rel_base_reg_user = chain;
+    }
+    
+  /* If rel_base->reg is not a mandatory allocated register, allocate
+     it to that chain that starts first and has no allocated register,
+     and that allows the addition of the start value in a single
+     instruction.  */
+  if (! rel_base_reg_user)
+    {
+      for (i = num_chains - 1; i >= 0; --i)
+	{
+	  struct rel_use_chain *chain = chain_starttab[i];
+	  if (! chain->reg
+	      && chain->start_luid
+	      && (!have_3addr_const_add[(int) mode] || !chain->uses->offset)
+	      && chain->uses->offset >= add_limits[(int) mode][0]
+	      && chain->uses->offset <= add_limits[(int) mode][1]
+	      /* Also can't use this chain if its register is clobbered
+		 and other chains need to start later.  */
+	      && (! (chain->end->no_link_pred && chain->end->insn)
+		  || chain->end_luid >= max_start_luid)
+	      /* Also can't use it if it lasts longer than the
+		 base reg is available.  */
+	      && (! rel_base->invalidate_luid
+		  || rel_base->invalidate_luid > chain->end_luid))
+	    {
+	      chain->reg = rel_base->reg;
+	      chain->invalidate_luid = rel_base->invalidate_luid;
+	      rel_base_reg_user = chain;
+	      if (num_linked < num_chains - 1)
+		{
+		  int old_linked = num_linked;
 
-		  REG_NOTES (insn)
-		    = gen_rtx_EXPR_LIST (REG_INC,
-					 reg, REG_NOTES (insn));
-		  if (! inc_insn_set)
-		    delete_insn (inc_insn);
-		  return 1;
+		  for (i = num_chains - 2; i >= 0; i--)
+		    {
+		      struct rel_use_chain *succ_chain = chain_starttab[i];
+
+		      while (chain->linked)
+			chain = chain->linked;
+		      if (succ_chain->start_luid
+			  && ! succ_chain->reg
+			  && link_chains (chain, succ_chain, mode))
+			{
+			  num_linked++;
+			  chain = succ_chain;
+			}
+		    }
+		  if (dump_file && num_linked > old_linked)
+		      fprintf (dump_file,
+			       "Linked to %d sets of chains.\n",
+			       num_chains - num_linked);
 		}
+	      break;
 	    }
 	}
     }
-  return 0;
-}
-
-/* Determine if the pattern generated by add_optab has a clobber,
-   such as might be issued for a flags hard register.  To make the
-   code elsewhere simpler, we handle cc0 in this same framework.
-
-   Return the register if one was discovered.  Return NULL_RTX if
-   if no flags were found.  Return pc_rtx if we got confused.  */
-
-static rtx
-discover_flags_reg (void)
-{
-  rtx tmp;
-  tmp = gen_rtx_REG (word_mode, 10000);
-  tmp = gen_add3_insn (tmp, tmp, const2_rtx);
+  else
+    rel_base_reg_user_offset = rel_base_reg_user->uses->offset;
 
-  /* If we get something that isn't a simple set, or a
-     [(set ..) (clobber ..)], this whole function will go wrong.  */
-  if (GET_CODE (tmp) == SET)
-    return NULL_RTX;
-  else if (GET_CODE (tmp) == PARALLEL)
+  /* If there are any chains that need to be initialized after the base
+     register has been invalidated, the optimization cannot be done.  */
+  for (i = 0; i < num_chains; i++)
     {
-      int found;
+      struct rel_use_chain *chain = chain_starttab[i];
 
-      if (XVECLEN (tmp, 0) != 2)
-	return pc_rtx;
-      tmp = XVECEXP (tmp, 0, 1);
-      if (GET_CODE (tmp) != CLOBBER)
-	return pc_rtx;
-      tmp = XEXP (tmp, 0);
+      if (rel_base->invalidate_luid
+	  && chain->start_luid > rel_base->invalidate_luid)
+	return;
+    }
 
-      /* Don't do anything foolish if the md wanted to clobber a
-	 scratch or something.  We only care about hard regs.
-	 Moreover we don't like the notion of subregs of hard regs.  */
-      if (GET_CODE (tmp) == SUBREG
-	  && REG_P (SUBREG_REG (tmp))
-	  && REGNO (SUBREG_REG (tmp)) < FIRST_PSEUDO_REGISTER)
-	return pc_rtx;
-      found = (REG_P (tmp) && REGNO (tmp) < FIRST_PSEUDO_REGISTER);
+  /* Now check if it is worth doing this optimization after all.
+     Using separate registers per value, like in the code generated by cse,
+     costs two instructions per register (one move and one add).
+     Using the chains we have set up, we need two instructions for every
+     linked set of chains, plus one instruction for every link;
+     however, if the base register is allocated to a chain
+     (i.e. rel_base_reg_user != 0), we don't need a move insn to start
+     that chain.
+     If we have a three-address add, however, the cost per value / chain
+     is just one insn, and linking chains is pointless.
+     We do the optimization if we save instructions, or if we
+     stay with the same number of instructions, but save registers.
+     We also require that we have enough registers available for reuse.
+     Moreover, we have to check that we can add the offset for
+     rel_base_reg_user, in case it is a mandatory allocated register.  */
+  if ((have_3addr_const_add[(int) mode]
+       ? (num_regs > num_chains - (rel_base_reg_user != 0))
+       : (2 * num_regs
+	  > ((2 * num_chains - num_linked - (rel_base_reg_user != 0))
+	     - (num_linked != 0))))
+      && num_av_regs + (rel_base_reg_user != 0) >= num_chains - num_linked
+      && rel_base_reg_user_offset >= add_limits[(int) mode][0]
+      && rel_base_reg_user_offset <= add_limits[(int) mode][1])
+    {
+      unsigned int base_regno = REGNO (rel_base->reg);
+      int num_mods;
+      int num_uses;
+      struct rel_mod *mods;
+      rtx last_changed_insn = 0;
+
+      /* Record facts about the last place where the base register is used.  */
+      int last_base_call_tally = rel_base->reg_set_call_tally;
+      rtx last_base_insn = 0;
 
-      return (found ? tmp : NULL_RTX);
-    }
+      if (dump_file)
+	fprintf (dump_file, "Optimization is worth while.\n");
 
-  return pc_rtx;
-}
+      remove_setting_insns (rel_base, insert_before);
 
-/* It is a tedious task identifying when the flags register is live and
-   when it is safe to optimize.  Since we process the instruction stream
-   multiple times, locate and record these live zones by marking the
-   mode of the instructions --
+      /* Allocate regs for each chain, and count the number of uses.  */
+      rel = rel_base;
+      for (num_uses = 0, i = 0; i < num_chains; i++)
+	{
+	  struct rel_use_chain *chain0 = chain_starttab[i];
+	  unsigned int regno;
+	  int first_call_tally, last_call_tally;
 
-   QImode is used on the instruction at which the flags becomes live.
+	  if (! chain0->start_luid)
+	    continue;
 
-   HImode is used within the range (exclusive) that the flags are
-   live.  Thus the user of the flags is not marked.
+	  /* If this chain has not got a register yet, assign one.  */
+	  if (! chain0->reg)
+	    {
+	      do
+		rel = rel->prev;
+	      while (! rel->death
+		     || (rel->invalidate_luid
+			 && rel->invalidate_luid <= max_end_luid));
 
-   All other instructions are cleared to VOIDmode.  */
+	      chain0->reg = rel->reg;
+	      chain0->death_note = rel->death;
+	    }
+	  else
+	    chain0->death_note = 0;
 
-/* Used to communicate with flags_set_1.  */
-static rtx flags_set_1_rtx;
-static int flags_set_1_set;
+	  /* For all registers except the base register, we can already
+	     determine the number of calls crossed at this point by
+	     examining the call_tally of the first and the last use.
+	     We can't do this for the base register yet, since we don't
+	     know its exact lifetime yet.  */
+	  regno = REGNO (chain0->reg);
+	  first_call_tally = last_call_tally = chain0->uses->call_tally;
 
-static void
-mark_flags_life_zones (rtx flags)
-{
-  int flags_regno;
-  int flags_nregs;
-  basic_block block;
+	  while (chain0)
+		{
+	      struct rel_use *use;
+	      for (use = chain0->uses; use; use = use->next_chain)
+		    {
+		  num_uses++;
+		  last_call_tally = use->call_tally;
+		    }
+	      chain0 = chain0->linked;
+		}
 
-#ifdef HAVE_cc0
-  /* If we found a flags register on a cc0 host, bail.  */
-  if (flags == NULL_RTX)
-    flags = cc0_rtx;
-  else if (flags != cc0_rtx)
-    flags = pc_rtx;
-#endif
+	  if (regno != base_regno)
+	    REG_N_CALLS_CROSSED (regno) += last_call_tally - first_call_tally;
+	    }
 
-  /* Simple cases first: if no flags, clear all modes.  If confusing,
-     mark the entire function as being in a flags shadow.  */
-  if (flags == NULL_RTX || flags == pc_rtx)
-    {
-      enum machine_mode mode = (flags ? HImode : VOIDmode);
-      rtx insn;
-      for (insn = get_insns (); insn; insn = NEXT_INSN (insn))
-	PUT_MODE (insn, mode);
-      return;
-    }
+      /* Record all the modifications we need to perform together with
+	 their position, then sort the array by position.  */
+      mods = rel_alloc ((num_chains + num_uses) * sizeof *mods);
+      for (i = num_mods = 0; i < num_chains; i++)
+	{
+	  struct rel_use_chain *chain0 = chain_starttab[i];
 
-#ifdef HAVE_cc0
-  flags_regno = -1;
-  flags_nregs = 1;
-#else
-  flags_regno = REGNO (flags);
-  flags_nregs = hard_regno_nregs[flags_regno][GET_MODE (flags)];
-#endif
-  flags_set_1_rtx = flags;
+	  if (! chain0->start_luid)
+	    continue;
 
-  /* Process each basic block.  */
-  FOR_EACH_BB_REVERSE (block)
-    {
-      rtx insn, end;
-      int live;
+	  for (chain = chain0; chain; chain = chain->linked)
+	    {
+	      struct rel_use *use = chain->uses;
 
-      insn = BB_HEAD (block);
-      end = BB_END (block);
+	      /* Initializing insn: an add (or move if offset == 0).  */
+	      mods[num_mods].from_base = use == chain0->uses;
+	      mods[num_mods].chain = chain0;
+	      mods[num_mods].insn = use->insn;
+	      mods[num_mods].luid = use->luid;
+	      mods[num_mods].count = num_mods;
+	      num_mods++;
+
+	      /* All the other uses: no additional insn, but offset
+		 updates.  */
+	      for (; use; use = use->next_chain)
+		{
+		  mods[num_mods].chain = chain0;
+		  mods[num_mods].insn = 0;
+		  mods[num_mods].luid = use->luid;
+		  mods[num_mods].count = num_mods;
+		  num_mods++;
+		}
+	    }
+	}
 
-      /* Look out for the (unlikely) case of flags being live across
-	 basic block boundaries.  */
-      live = 0;
-#ifndef HAVE_cc0
-      {
-	int i;
-	for (i = 0; i < flags_nregs; ++i)
-	  live |= REGNO_REG_SET_P (block->il.rtl->global_live_at_start,
-				   flags_regno + i);
-      }
-#endif
+      gcc_assert (num_mods == num_chains + num_uses);
+      qsort (mods, num_mods, sizeof *mods, mod_before);
 
-      while (1)
+      /* Now we have a list of all changes we have to make, sorted in
+	 ascending order so we can go through the basic block from
+	 start to end and keep track of the current state at all times.  */
+      if (rel_base_reg_user)
+	rel_base_reg_user->match_offset = 0;
+      for (i = 0; i < num_mods; i++)
 	{
-	  /* Process liveness in reverse order of importance --
-	     alive, death, birth.  This lets more important info
-	     overwrite the mode of lesser info.  */
-
-	  if (INSN_P (insn))
+	  struct rel_mod *this = mods + i;
+	  struct rel_use *use = this->chain->uses;
+	  HOST_WIDE_INT current_offset = this->chain->match_offset;
+	  rtx reg = this->chain->reg;
+
+	  /* Calling apply_change_group is deferred to this point from
+	     the call to validate_change in modify_address; the reason is
+	     that we want to group together multiple changes to the same insn,
+	     to avoid failures for match_dups.  */
+	  if (last_changed_insn
+	      && (this->insn != 0 || use->insn != last_changed_insn))
 	    {
-#ifdef HAVE_cc0
-	      /* In the cc0 case, death is not marked in reg notes,
-		 but is instead the mere use of cc0 when it is alive.  */
-	      if (live && reg_mentioned_p (cc0_rtx, PATTERN (insn)))
-		live = 0;
-#else
-	      /* In the hard reg case, we watch death notes.  */
-	      if (live && find_regno_note (insn, REG_DEAD, flags_regno))
-		live = 0;
-#endif
-	      PUT_MODE (insn, (live ? HImode : VOIDmode));
+	      last_changed_insn = 0;
+	      /* Don't use gcc_assert on the result of apply_change_group
+		 because that would prevent setting a breakpoint on the
+		 failure.  */
+	      if (! apply_change_group ())
+		gcc_assert (0);
+	    }
 
-	      /* In either case, birth is denoted simply by its presence
-		 as the destination of a set.  */
-	      flags_set_1_set = 0;
-	      note_stores (PATTERN (insn), flags_set_1, NULL);
-	      if (flags_set_1_set)
+	  if (this->insn != 0)
+	    {
+	      rtx new = perform_addition (this, use, rel_base->reg,
+					  rel_base_reg_user);
+	      if (this->from_base && new)
 		{
-		  live = 1;
-		  PUT_MODE (insn, QImode);
+		  /* If perform_addition emitted more than one insn, find
+		     the last one that actually used the base register.  */
+		  while (! reg_overlap_mentioned_p (rel_base->reg,
+						    PATTERN (new)))
+		    new = PREV_INSN (new);
+		  last_base_call_tally = use->call_tally;
+		  last_base_insn = new;
 		}
 	    }
 	  else
-	    PUT_MODE (insn, (live ? HImode : VOIDmode));
+	    {
+	      if (! use->no_link_pred)
+		modify_address (this, use, current_offset);
 
-	  if (insn == end)
-	    break;
-	  insn = NEXT_INSN (insn);
+	      /* See if the register dies in this insn.  We cannot reliably
+		 detect this for the base register, which is handled later
+		 after all modifications are processed.  We can rely on the
+		 DEATH_NOTE field being 0 for the base register's chain.  */
+	      if (this->chain->death_note && this->chain->uses == 0)
+		{
+		  rtx note = this->chain->death_note;
+		  XEXP (note, 0) = reg;
+
+		  /* Note that passing only PATTERN (LAST_USE->insn) to
+		     reg_set_p here is not enough, since we might have
+		     created an REG_INC for REG above.  */
+
+		  PUT_MODE (note, (reg_set_p (reg, use->insn)
+				   ? REG_UNUSED : REG_DEAD));
+		  XEXP (note, 1) = REG_NOTES (use->insn);
+		  REG_NOTES (use->insn) = note;
+		  REG_N_DEATHS (REGNO (reg))++;
+		}
+
+	      if (REGNO (reg) == base_regno)
+		{
+		  last_base_call_tally = use->call_tally;
+		  last_base_insn = use->insn;
+		}
+	      last_changed_insn = use->insn;
+	    }
+	}
+
+      if (last_changed_insn)
+	if (! apply_change_group ())
+	  gcc_assert (0);
+
+      /* We now have performed all modifications, and we therefore know the
+	 last insn that uses the base register.  This means we can now update
+	 its life information.  */
+      if (rel_base->death)
+	{
+	  rtx note = rel_base->death;
+	  XEXP (note, 0) = rel_base->reg;
+
+	  /* Note that passing only PATTERN (LAST_USE->insn) to
+	     reg_set_p here is not enough, since we might have
+	     created an REG_INC for REG above.  */
+
+	  PUT_MODE (note, (reg_set_p (rel_base->reg, last_base_insn)
+			   ? REG_UNUSED : REG_DEAD));
+	  XEXP (note, 1) = REG_NOTES (last_base_insn);
+	  REG_NOTES (last_base_insn) = note;
+	  REG_N_DEATHS (base_regno)++;
+	}
+      else if (rel_base->invalidate_luid
+	       && ! reg_set_p (rel_base->reg, last_base_insn))
+	{
+	  REG_NOTES (last_base_insn)
+	    = alloc_EXPR_LIST (REG_DEAD, rel_base->reg,
+			       REG_NOTES (last_base_insn));
+	  REG_N_DEATHS (base_regno)++;
 	}
+
+      REG_N_CALLS_CROSSED (base_regno)
+	+= last_base_call_tally - rel_base->reg_set_call_tally;
     }
 }
 
-/* A subroutine of mark_flags_life_zones, called through note_stores.  */
+/* Finalize the optimization for any related values know so far, and reset
+   the entries in regno_related that we have disturbed.  */
+static void
+optimize_related_values_0 (struct related *rel_base_list,
+			   int luid, int call_tally, rtx insert_before)
+{
+  while (rel_base_list)
+    {
+      struct related *rel;
+      optimize_related_values_1 (rel_base_list, luid, call_tally,
+				 insert_before);
+      /* Clear the entries that we used in regno_related.  We do it
+	 item by item here, because doing it with memset for each
+	 basic block would give O(n*n) time complexity.  */
+      for (rel = rel_base_list; rel; rel = rel->prev)
+	regno_related[REGNO (rel->reg)] = 0;
+      rel_base_list = rel_base_list->baseinfo->prev_base;
+    }
+  
+  for ( ; unrelatedly_used; unrelatedly_used = unrelatedly_used->prev)
+    regno_related[REGNO (unrelatedly_used->reg)] = 0;
+}
 
+/* For each integer mode, find minimum and maximum value for a single-
+   instruction reg-constant add.
+   The arm has SImode add patterns that will accept large values - with a
+   matching splitter - but when you use gen_addsi3, you already get
+   multiple instructions.  So getting one insn and testing if it can be
+   changed is not good enough; we need to try to generate each add from
+   scratch.  */
 static void
-flags_set_1 (rtx x, rtx pat, void *data ATTRIBUTE_UNUSED)
+init_add_limits (void)
 {
-  if (GET_CODE (pat) == SET
-      && reg_overlap_mentioned_p (x, flags_set_1_rtx))
-    flags_set_1_set = 1;
+  static int is_initialized;
+
+  enum machine_mode mode;
+
+  if (is_initialized)
+    return;
+
+  for (mode = GET_CLASS_NARROWEST_MODE (MODE_INT); mode != VOIDmode;
+       mode = GET_MODE_WIDER_MODE (mode))
+    {
+      rtx reg = gen_rtx_REG (mode, LAST_VIRTUAL_REGISTER+1);
+      rtx reg2 = gen_rtx_REG (mode, LAST_VIRTUAL_REGISTER+2);
+      int icode = (int) add_optab->handlers[(int) mode].insn_code;
+      HOST_WIDE_INT tmp;
+      rtx add = NULL, set = NULL;
+      int p, p_max;
+      rtx tmp_add;
+      struct match match;
+
+      have_3addr_const_add[(int) mode] = 0;
+      add_limits[(int) mode][0] = 0;
+      add_limits[(int) mode][1] = 0;
+      
+      if (icode == CODE_FOR_nothing
+	  || ! (*insn_data[icode].operand[0].predicate) (reg, mode)
+	  || ! (*insn_data[icode].operand[1].predicate) (reg, mode)
+	  || ! (*insn_data[icode].operand[2].predicate) (const1_rtx, mode))
+	continue;
+      
+      tmp_add = GEN_FCN (icode) (reg, reg2, const1_rtx);
+      if (tmp_add != NULL_RTX
+	  && !NEXT_INSN (tmp_add)
+	  && !find_matches (tmp_add, &match))
+	have_3addr_const_add[(int) mode] = 1;
+
+      p_max = GET_MODE_BITSIZE (mode) - 1;
+      
+      if (p_max > HOST_BITS_PER_WIDE_INT - 2)
+	p_max = HOST_BITS_PER_WIDE_INT - 2;
+      
+      for (p = 1; p < p_max; p++)
+	{
+	  rtx add_const = GEN_INT (((HOST_WIDE_INT) 1 << p) - 1);
+
+	  if (! (*insn_data[icode].operand[2].predicate) (add_const, mode))
+	    break;
+
+	  tmp_add = GEN_FCN (icode) (reg, reg, add_const);
+      
+	  if (tmp_add == NULL_RTX || NEXT_INSN (tmp_add))
+	    break;
+      
+	  set = single_set (tmp_add);
+      
+	  if (! set
+	      || GET_CODE (SET_SRC (set)) != PLUS
+	      || XEXP (SET_SRC (set), 1) != add_const)
+	    break;
+	  add = tmp_add;
+	}
+      
+      add_limits[(int) mode][1] = tmp = ((HOST_WIDE_INT) 1 << (p - 1)) - 1;
+      
+      /* We need a range of known good values for the constant of the add.
+	 Thus, before checking for the power of two, check for one less first,
+	 in case the power of two is an exceptional value.  */
+      if (add
+	  && validate_change (add, &XEXP (SET_SRC (set), 1), GEN_INT (-tmp), 0))
+	{
+	  if (validate_change (add, &XEXP (SET_SRC (set), 1),
+			       GEN_INT (-tmp - 1), 0))
+	    add_limits[(int) mode][0] = -tmp - 1;
+	  else
+	    add_limits[(int) mode][0] = -tmp;
+	}
+    }
+  is_initialized = 1;
+}
+
+/* Scan the entire function for instances where multiple registers are
+   set to values that differ only by a constant.
+   Then try to reduce the number of instructions and/or registers needed
+   by exploiting auto_increment and true two-address additions.
+   NREGS and REGMOVE_DUMP_FILE are the same as in regmove_optimize.  */
+    
+static void
+optimize_related_values (int nregs)
+{
+  basic_block bb;
+  rtx insn;
+  int luid = 0;
+  int call_tally = 0;
+
+  if (dump_file)
+    fprintf (dump_file, "Starting optimize_related_values.\n");
+
+  init_add_limits ();
+  gcc_obstack_init (&related_obstack);
+  regno_related = rel_alloc (nregs * sizeof *regno_related);
+  memset ((char *) regno_related, 0, nregs * sizeof *regno_related);
+  rel_base_list = 0;
+
+  FOR_EACH_BB (bb)
+    FOR_BB_INSNS (bb, insn)
+      {
+	rtx set = NULL_RTX;
+
+	luid++;
+	
+	/* Don't do anything if this instruction is in the shadow of a
+	   live flags register.  */
+	if (GET_MODE (insn) == HImode)
+	  continue;
+
+	if (INSN_P (insn))
+	  {
+	    rtx note;
+
+	    set = single_set (insn);
+
+	    find_related_toplev (insn, luid, call_tally);
+
+	    for (note = REG_NOTES (insn); note; note = XEXP (note, 1))
+	      {
+		if (REG_NOTE_KIND (note) == REG_DEAD
+		    || (REG_NOTE_KIND (note) == REG_UNUSED
+			&& REG_P (XEXP (note, 0))))
+		  {
+		    rtx reg = XEXP (note, 0);
+		    int regno = REGNO (reg);
+		    
+		    if (REG_NOTE_KIND (note) == REG_DEAD
+			&& reg_set_p (reg, PATTERN (insn)))
+		      {
+			remove_note (insn, note);
+			REG_N_DEATHS (regno)--;
+		      }
+		    else if (regno_related[regno]
+			     && ! regno_related[regno]->invalidate_luid)
+		      {
+			regno_related[regno]->death = insn;
+			regno_related[regno]->reg_orig_calls_crossed
+			  = call_tally - regno_related[regno]->reg_set_call_tally;
+		      }
+		  }
+	      }
+	    
+	    /* Inputs to a call insn do not cross the call, therefore CALL_TALLY
+	       must be bumped *after* they have been processed.  */
+	    if (CALL_P (insn))
+	      call_tally++;
+	  }
+	    
+	/* We end current processing at the end of a basic block, or when
+	   a flags register becomes live, or when we see a return value
+	   copy.
+
+	   Otherwise, we might end up with one or more extra instructions
+	   inserted in front of the user, to set up or adjust a register. 
+	   There are cases where flag register uses could be handled smarter,
+	   but most of the time the user will be a branch anyways, so the
+	   extra effort to handle the occasional conditional instruction is
+	   probably not justified by the little possible extra gain.  */
+
+	if (insn == BB_END (bb)
+	    || GET_MODE (insn) == QImode
+	    || (set
+		&& REG_P (SET_DEST (set))
+		&& REG_FUNCTION_VALUE_P (SET_DEST (set))))
+	  {
+	    optimize_related_values_0 (rel_base_list, luid, call_tally, insn);
+	    rel_base_list = 0;
+	  }
+      }
+  obstack_free (&related_obstack, 0);
+  
+  if (dump_file)
+    fprintf (dump_file, "Finished optimize_related_values.\n");
 }
 
 static int *regno_src_regno;
@@ -983,35 +2930,36 @@
 			 "Fixed operand of insn %d.\n",
 			  INSN_UID (insn));
 
-#ifdef AUTO_INC_DEC
-	      for (p = PREV_INSN (insn); p; p = PREV_INSN (p))
+	      if (AUTO_INC_DEC)
 		{
-		  if (LABEL_P (p)
-		      || JUMP_P (p))
-		    break;
-		  if (! INSN_P (p))
-		    continue;
-		  if (reg_overlap_mentioned_p (dst, PATTERN (p)))
+		  for (p = PREV_INSN (insn); p; p = PREV_INSN (p))
 		    {
-		      if (try_auto_increment (p, insn, 0, dst, newconst, 0))
-			return 1;
-		      break;
+		      if (LABEL_P (p)
+			  || JUMP_P (p))
+			break;
+		      if (! INSN_P (p))
+			continue;
+		      if (reg_overlap_mentioned_p (dst, PATTERN (p)))
+			{
+			  if (try_auto_increment (p, insn, 0, dst, newconst, 0))
+			    return 1;
+			  break;
+			}
 		    }
-		}
-	      for (p = NEXT_INSN (insn); p; p = NEXT_INSN (p))
-		{
-		  if (LABEL_P (p)
-		      || JUMP_P (p))
-		    break;
-		  if (! INSN_P (p))
-		    continue;
-		  if (reg_overlap_mentioned_p (dst, PATTERN (p)))
+		  for (p = NEXT_INSN (insn); p; p = NEXT_INSN (p))
 		    {
-		      try_auto_increment (p, insn, 0, dst, newconst, 1);
-		      break;
+		      if (LABEL_P (p)
+			  || JUMP_P (p))
+			break;
+		      if (! INSN_P (p))
+			continue;
+		      if (reg_overlap_mentioned_p (dst, PATTERN (p)))
+			{
+			  try_auto_increment (p, insn, 0, dst, newconst, 1);
+			  break;
+			}
 		    }
 		}
-#endif
 	      return 1;
 	    }
 	}
@@ -1053,7 +3001,7 @@
 static void
 regmove_optimize (rtx f, int nregs)
 {
-  int old_max_uid = get_max_uid ();
+  int old_max_uid;
   rtx insn;
   struct match match;
   int pass;
@@ -1070,7 +3018,15 @@
      can suppress some optimizations in those zones.  */
   mark_flags_life_zones (discover_flags_reg ());
 
-  regno_src_regno = XNEWVEC (int, nregs);
+  /* See the comment in front of REL_USE_HASH_SIZE what
+     this is about.  */
+  if (AUTO_INC_DEC && flag_regmove && flag_optimize_related_values)
+    optimize_related_values (nregs);
+  /* That could have created new insns.  */
+  old_max_uid = get_max_uid ();
+
+  regno_src_regno = xmalloc (sizeof *regno_src_regno * nregs);
+
   for (i = nregs; --i >= 0; ) regno_src_regno[i] = -1;
 
   regmove_bb_head = XNEWVEC (int, old_max_uid + 1);
@@ -2014,6 +3970,8 @@
       /* Move the death note for SRC from INSN to P.  */
       if (! overlap)
 	remove_note (insn, src_note);
+      if (find_reg_note (p, REG_INC, XEXP (src_note, 0)))
+	PUT_MODE (src_note, REG_UNUSED);
       XEXP (src_note, 1) = REG_NOTES (p);
       REG_NOTES (p) = src_note;
 
@@ -2557,3 +4515,4 @@
   0                                     /* letter */
 };
 
+#include "gt-regmove.h"
diff -Naur gcc-4.2.1/gcc/reload.c gcc-4.2.1-stm/gcc/reload.c
--- gcc-4.2.1/gcc/reload.c	2007-06-20 06:47:09.000000000 +0100
+++ gcc-4.2.1-stm/gcc/reload.c	2007-08-10 13:08:58.000000000 +0100
@@ -2177,14 +2177,8 @@
 	 multiple hard register group of scalar integer registers, so that
 	 for example (reg:DI 0) and (reg:SI 1) will be considered the same
 	 register.  */
-      if (WORDS_BIG_ENDIAN && GET_MODE_SIZE (GET_MODE (x)) > UNITS_PER_WORD
-	  && SCALAR_INT_MODE_P (GET_MODE (x))
-	  && i < FIRST_PSEUDO_REGISTER)
-	i += hard_regno_nregs[i][GET_MODE (x)] - 1;
-      if (WORDS_BIG_ENDIAN && GET_MODE_SIZE (GET_MODE (y)) > UNITS_PER_WORD
-	  && SCALAR_INT_MODE_P (GET_MODE (y))
-	  && j < FIRST_PSEUDO_REGISTER)
-	j += hard_regno_nregs[j][GET_MODE (y)] - 1;
+      i = targetm.match_adjust (x, i);
+      j = targetm.match_adjust (y, j);
 
       return i == j;
     }
diff -Naur gcc-4.2.1/gcc/rtlhooks.c gcc-4.2.1-stm/gcc/rtlhooks.c
--- gcc-4.2.1/gcc/rtlhooks.c	2006-02-28 20:32:20.000000000 +0000
+++ gcc-4.2.1-stm/gcc/rtlhooks.c	2007-08-10 13:08:58.000000000 +0100
@@ -1,5 +1,6 @@
 /* Generic hooks for the RTL middle-end.
    Copyright (C) 2004, 2005 Free Software Foundation, Inc.
+   Copyright (c) 2006  STMicroelectronics.
 
 This file is part of GCC.
 
diff -Naur gcc-4.2.1/gcc/sched-deps.c gcc-4.2.1-stm/gcc/sched-deps.c
--- gcc-4.2.1/gcc/sched-deps.c	2007-04-19 12:19:39.000000000 +0100
+++ gcc-4.2.1-stm/gcc/sched-deps.c	2007-08-10 13:08:59.000000000 +0100
@@ -977,6 +977,93 @@
 	  sched_analyze_reg (deps, FIRST_STACK_REG, mode, SET, insn);
 	}
 #endif
+
+	if (regno < FIRST_PSEUDO_REGISTER)
+	  {
+	    int i = hard_regno_nregs[regno][GET_MODE (x)];
+	    while (--i >= 0)
+	      {
+		rtx sets;
+  
+		/* We have to prevent extending the lifetime of
+		   CLASS_LIKELY_SPILLED registers, no matter where they are
+		   used within a basic block.  SH -m4-nofpu  -O -fschedule-insns
+		   testcase extracted from newlib/libm/math/k_rem_pio2.c :
+		void
+		__kernel_rem_pio2 ()
+		{
+		  int jz, jk, iq[20], i, j, k;
+		  double z, f[20], fq[20], q[20];
+		
+		  for (i = 0, j = jz, z = q[1]; j > 0; i++, j--)
+		    {
+		      iq[i] = (int) (z - 1.);
+		      z = q[0] + 1.;
+		    }
+		
+		  for (k = 1; iq[jk - k] == 0; k++);
+		
+		  for (i = 1; i <= 1 + k; i++)
+		    q[i] = 1.;
+		}
+		   Here, the problem is that a (reg:DF 0 r0) result from an
+		   sfunc conflicts with an R0_REGS requirement from a store
+		   that the scheduler has placed before the copy of the result
+		   to a pseudo.  */
+		if (next_nonnote_insn (insn)
+		    && !reload_completed && !fixed_regs[regno+i]
+		    && CLASS_LIKELY_SPILLED_P (REGNO_REG_CLASS (regno + i))
+		    && (sets = deps->reg_last[regno+i].sets))
+		  {
+		    rtx set = XEXP (sets, 0);
+		    rtx curr;
+
+		    for (curr = insn; curr != set;
+			 curr = prev_nonnote_insn (curr))
+		      CANT_MOVE (curr) = 1;
+		    CANT_MOVE (set) = 1;
+		    for (curr = next_nonnote_insn (set);
+			 curr != insn && SCHED_GROUP_P (curr);)
+		      curr = next_nonnote_insn (curr);
+		    for (; curr != insn; curr = next_nonnote_insn (curr))
+		      {
+			SCHED_GROUP_P (curr) = 1;
+			fixup_sched_groups (curr);
+		      }
+		    SCHED_GROUP_P (insn) = 1;
+		  }
+		SET_REGNO_REG_SET (reg_pending_uses, regno + i);
+	      }
+	  }
+	/* ??? Reload sometimes emits USEs and CLOBBERs of pseudos that
+	   it does not reload.  Ignore these as they have served their
+	   purpose already.  */
+	else if (regno >= deps->max_reg)
+	  {
+	    gcc_assert (GET_CODE (PATTERN (insn)) == USE
+			|| GET_CODE (PATTERN (insn)) == CLOBBER);
+	  }
+	else
+	  {
+	    SET_REGNO_REG_SET (reg_pending_uses, regno);
+
+	    /* Pseudos that are REG_EQUIV to something may be replaced
+	       by that during reloading.  We need only add dependencies for
+	       the address in the REG_EQUIV note.  */
+	    if (!reload_completed && get_reg_known_equiv_p (regno))
+	      {
+		rtx t = get_reg_known_value (regno);
+		if (MEM_P (t))
+		  sched_analyze_2 (deps, XEXP (t, 0), insn);
+	      }
+
+	    /* If the register does not already cross any calls, then add this
+	       insn to the sched_before_next_call list so that it will still
+	       not cross calls after scheduling.  */
+	    if (REG_N_CALLS_CROSSED (regno) == 0)
+	      deps->sched_before_next_call
+		= alloc_INSN_LIST (insn, deps->sched_before_next_call);
+	  }
 	return;
       }
 
diff -Naur gcc-4.2.1/gcc/struct-equiv.c gcc-4.2.1-stm/gcc/struct-equiv.c
--- gcc-4.2.1/gcc/struct-equiv.c	2006-03-10 10:58:22.000000000 +0000
+++ gcc-4.2.1-stm/gcc/struct-equiv.c	2007-08-10 13:08:59.000000000 +0100
@@ -983,38 +983,47 @@
   return false;
 }
 
-/* Set up mode and register information in INFO.  Return true for success.  */
-bool
-struct_equiv_init (int mode, struct equiv_info *info)
+static bool
+struct_equiv_regs_eq_p (struct equiv_info *info)
 {
-  if ((info->x_block->flags | info->y_block->flags) & BB_DIRTY)
-    update_life_info_in_dirty_blocks (UPDATE_LIFE_GLOBAL_RM_NOTES,
-				      (PROP_DEATH_NOTES
-				       | ((mode & CLEANUP_POST_REGSTACK)
-					  ? PROP_POST_REGSTACK : 0)));
-  if (!REG_SET_EQUAL_P (info->x_block->il.rtl->global_live_at_end,
-			info->y_block->il.rtl->global_live_at_end))
-    {
 #ifdef STACK_REGS
-      unsigned rn;
-
-      if (!(mode & CLEANUP_POST_REGSTACK))
-	return false;
-      /* After reg-stack.  Remove bogus live info about stack regs.  N.B.
-	 these regs are not necessarily all dead - we swap random bogosity
-	 against constant bogosity.  However, clearing these bits at
-	 least makes the regsets comparable.  */
-      for (rn = FIRST_STACK_REG; rn <= LAST_STACK_REG; rn++)
+  if (info->mode & CLEANUP_POST_REGSTACK)
+    {
+      regset_head diff;
+      unsigned regnum;
+      bitmap_iterator rsi;
+
+      INIT_REG_SET (&diff);
+      bitmap_xor (&diff,
+		  info->x_block->il.rtl->global_live_at_end,
+		  info->y_block->il.rtl->global_live_at_end);
+      EXECUTE_IF_SET_IN_BITMAP (&diff, 0, regnum, rsi)
 	{
-	  CLEAR_REGNO_REG_SET (info->x_block->il.rtl->global_live_at_end, rn);
-	  CLEAR_REGNO_REG_SET (info->y_block->il.rtl->global_live_at_end, rn);
+	  if (regnum < FIRST_STACK_REG || regnum > LAST_STACK_REG)
+	    return false;
 	}
-      if (!REG_SET_EQUAL_P (info->x_block->il.rtl->global_live_at_end,
-			    info->y_block->il.rtl->global_live_at_end))
+      return true;
+    }
 #endif
+  return (REG_SET_EQUAL_P (info->x_block->il.rtl->global_live_at_end,
+			   info->y_block->il.rtl->global_live_at_end));
+}
+
+/* Set up mode and register information in INFO.  Return true for success.
+   Nonzero CHECK_REGS_EQ indicates that we might be called with blocks that
+   have non-matching successor sets, and thus need to check their live_at_end
+   regsets for match in the first pass.  */
+bool
+struct_equiv_init (int mode, struct equiv_info *info, bool check_regs_eq)
+{
+  info->mode = mode;
+  if (check_regs_eq && (mode & STRUCT_EQUIV_START))
+    {
+      if (!struct_equiv_regs_eq_p (info))
 	return false;
     }
-  info->mode = mode;
+  else if (mode & STRUCT_EQUIV_FINAL)
+    gcc_assert (struct_equiv_regs_eq_p (info));
   if (mode & STRUCT_EQUIV_START)
     {
       info->x_input = info->y_input = info->input_reg = NULL_RTX;
@@ -1096,8 +1105,7 @@
       x_stop = info->cur.x_start;
       y_stop = info->cur.y_start;
     }
-  if (!struct_equiv_init (mode, info))
-    gcc_unreachable ();
+  struct_equiv_init (mode, info, false);
 
   /* Skip simple jumps at the end of the blocks.  Complex jumps still
      need to be compared for equivalence, which we'll do below.  */
diff -Naur gcc-4.2.1/gcc/target-def.h gcc-4.2.1-stm/gcc/target-def.h
--- gcc-4.2.1/gcc/target-def.h	2007-03-10 00:59:15.000000000 +0000
+++ gcc-4.2.1-stm/gcc/target-def.h	2007-08-10 13:08:59.000000000 +0100
@@ -546,6 +546,10 @@
 #define TARGET_SECONDARY_RELOAD default_secondary_reload
 #endif
 
+#ifndef TARGET_MATCH_ADJUST
+#define TARGET_MATCH_ADJUST default_match_adjust
+#endif
+
 
 /* C++ specific.  */
 #ifndef TARGET_CXX_GUARD_TYPE
@@ -684,8 +688,9 @@
   TARGET_INVALID_UNARY_OP,			\
   TARGET_INVALID_BINARY_OP,			\
   TARGET_SECONDARY_RELOAD,			\
+  TARGET_MATCH_ADJUST,				\
   TARGET_CXX,					\
-  TARGET_EXTRA_LIVE_ON_ENTRY,                    \
+  TARGET_EXTRA_LIVE_ON_ENTRY,                   \
   TARGET_UNWIND_TABLES_DEFAULT,			\
   TARGET_HAVE_NAMED_SECTIONS,			\
   TARGET_HAVE_SWITCHABLE_BSS_SECTIONS,		\
diff -Naur gcc-4.2.1/gcc/target.h gcc-4.2.1-stm/gcc/target.h
--- gcc-4.2.1/gcc/target.h	2007-03-10 00:59:15.000000000 +0000
+++ gcc-4.2.1-stm/gcc/target.h	2007-08-10 13:08:58.000000000 +0100
@@ -744,6 +744,10 @@
 				      enum machine_mode,
 				      struct secondary_reload_info *);
 
+   /* Take an rtx and its regno, and return the regno for purposes of
+      checking a matching constraint.  */
+   int (*match_adjust) (rtx, int);
+
   /* Functions specific to the C++ frontend.  */
   struct cxx {
     /* Return the integer type used for guard variables.  */
diff -Naur gcc-4.2.1/gcc/targhooks.c gcc-4.2.1-stm/gcc/targhooks.c
--- gcc-4.2.1/gcc/targhooks.c	2007-03-10 00:59:15.000000000 +0000
+++ gcc-4.2.1-stm/gcc/targhooks.c	2007-08-10 13:08:58.000000000 +0100
@@ -66,6 +66,7 @@
 #include "reload.h"
 #include "optabs.h"
 #include "recog.h"
+#include "regs.h"
 
 
 void
@@ -181,6 +182,27 @@
   return long_long_integer_type_node;
 }
 
+/*  Given an rtx and its regno, return a regno value that shall be used for
+    purposes of comparison in operands_match_p.
+    Generally, we say that integer registers are subject to big-endian
+    adjustment.  This default target hook should generally work if the mode
+    of a register is a sufficient indication if this adjustment is to take
+    place; this will not work when software floating point is done in integer
+    registers.  */
+int
+default_match_adjust (rtx x, int regno)
+{
+  /* On a WORDS_BIG_ENDIAN machine, point to the last register of a
+     multiple hard register group of scalar integer registers, so that
+     for example (reg:DI 0) and (reg:SI 1) will be considered the same
+     register.  */
+  if (WORDS_BIG_ENDIAN && GET_MODE_SIZE (GET_MODE (x)) > UNITS_PER_WORD
+      && SCALAR_INT_MODE_P (GET_MODE (x))
+      && regno < FIRST_PSEUDO_REGISTER)
+    regno += hard_regno_nregs[regno][GET_MODE (x)] - 1;
+  return regno;
+}
+
 
 /* Returns the size of the cookie to use when allocating an array
    whose elements have the indicated TYPE.  Assumes that it is already
diff -Naur gcc-4.2.1/gcc/targhooks.h gcc-4.2.1-stm/gcc/targhooks.h
--- gcc-4.2.1/gcc/targhooks.h	2007-03-10 00:59:15.000000000 +0000
+++ gcc-4.2.1-stm/gcc/targhooks.h	2007-08-10 13:08:58.000000000 +0100
@@ -78,5 +78,6 @@
 						enum machine_mode,
 						secondary_reload_info *);
 extern void hook_void_bitmap (bitmap);
-
+extern int default_match_adjust (rtx, int);
 extern int default_reloc_rw_mask (void);
+
diff -Naur gcc-4.2.1/gcc/testsuite/ChangeLog.STM gcc-4.2.1-stm/gcc/testsuite/ChangeLog.STM
--- gcc-4.2.1/gcc/testsuite/ChangeLog.STM	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.2.1-stm/gcc/testsuite/ChangeLog.STM	2007-08-10 13:08:58.000000000 +0100
@@ -0,0 +1,5 @@
+2007-06-20  Christian Bruel  <christian.bruel@st.com>
+
+	* gcc.dg/attr-isr.c: Test delay slot content.
+
+	
\ No newline at end of file
diff -Naur gcc-4.2.1/gcc/testsuite/gcc.c-torture/execute/arith-rand-ll.c gcc-4.2.1-stm/gcc/testsuite/gcc.c-torture/execute/arith-rand-ll.c
--- gcc-4.2.1/gcc/testsuite/gcc.c-torture/execute/arith-rand-ll.c	2002-07-01 16:12:49.000000000 +0100
+++ gcc-4.2.1-stm/gcc/testsuite/gcc.c-torture/execute/arith-rand-ll.c	2007-08-10 13:08:58.000000000 +0100
@@ -79,7 +79,7 @@
 	  continue;
 	r1 = xx / yy;
 	r2 = xx % yy;
-	if (ABS (r2) >= (unsigned int) ABS (yy) || (signed int) (r1 * yy + r2) != xx)
+	if (ABS (r2) >= (unsigned int) ABS (yy) || (signed int) (r1 * yy + r2) != xx || ((xx < 0) != (r2 < 0) && r2))
 	  abort ();
       }
       { unsigned short xx = x, yy = y, r1, r2;
diff -Naur gcc-4.2.1/gcc/testsuite/gcc.dg/attr-isr.c gcc-4.2.1-stm/gcc/testsuite/gcc.dg/attr-isr.c
--- gcc-4.2.1/gcc/testsuite/gcc.dg/attr-isr.c	2006-01-30 15:07:43.000000000 +0000
+++ gcc-4.2.1-stm/gcc/testsuite/gcc.dg/attr-isr.c	2007-08-10 13:08:58.000000000 +0100
@@ -16,3 +16,4 @@
 /* { dg-final { scan-assembler-times "\[^f\]r\[0-9\]\[ \t\]*," 8 } } */
 /* { dg-final { scan-assembler-not "\[^f\]r1\[0-3\]" } } */
 /* { dg-final { scan-assembler-times "macl" 2} } */
+/* { dg-final { scan-assembler-not "rte.*\n.*r15\[+\],r\[0-7\]\n" } } */
diff -Naur gcc-4.2.1/gcc/testsuite/gcc.dg/tree-ssa/pr23109.c gcc-4.2.1-stm/gcc/testsuite/gcc.dg/tree-ssa/pr23109.c
--- gcc-4.2.1/gcc/testsuite/gcc.dg/tree-ssa/pr23109.c	2006-01-18 19:47:16.000000000 +0000
+++ gcc-4.2.1-stm/gcc/testsuite/gcc.dg/tree-ssa/pr23109.c	2007-08-10 13:08:58.000000000 +0100
@@ -1,5 +1,5 @@
 /* { dg-do compile } */
-/* { dg-options "-O2 -funsafe-math-optimizations -fdump-tree-recip -fdump-tree-lim" } */
+/* { dg-options "-O2 -funsafe-math-optimizations -fdump-tree-recip -fdump-tree-lim -ftrapping-math" } */
 
 double F[2] = { 0., 0. }, e = 0.;
 
diff -Naur gcc-4.2.1/gcc/testsuite/gcc.target/sh/fpchg1.c gcc-4.2.1-stm/gcc/testsuite/gcc.target/sh/fpchg1.c
--- gcc-4.2.1/gcc/testsuite/gcc.target/sh/fpchg1.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.2.1-stm/gcc/testsuite/gcc.target/sh/fpchg1.c	2007-08-10 13:08:58.000000000 +0100
@@ -0,0 +1,24 @@
+/* { dg-do run } */
+/* { dg-options "-O1 -m4-300" } */
+
+/* Check that the fpchg instruction is not moved in a delay slot if the
+   fallthru block uses the mode.  */
+
+__attribute__ ((weak))
+void barrier(void)
+{
+}
+
+float f;
+int i;
+double d;
+
+int main()
+{
+  i = 4;
+
+  barrier();
+
+  i = (f + (i && f && d));
+  return i;
+}
diff -Naur gcc-4.2.1/gcc/testsuite/gcc.target/sh/fpchg2.c gcc-4.2.1-stm/gcc/testsuite/gcc.target/sh/fpchg2.c
--- gcc-4.2.1/gcc/testsuite/gcc.target/sh/fpchg2.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.2.1-stm/gcc/testsuite/gcc.target/sh/fpchg2.c	2007-08-10 13:08:58.000000000 +0100
@@ -0,0 +1,16 @@
+/* { dg-do compile } */
+/* { dg-options "-O2 -m4-300" } */
+
+/* Make sure that fpchg is preferred over lfd.s fpscr.  */
+/* { dg-final { scan-assembler "fpchg" } } */
+/* { dg-final { scan-assembler-not "fpscr" } } */
+
+extern float c;
+
+void
+foo(int j)
+{
+  while (j--)
+    c++;
+
+}
diff -Naur gcc-4.2.1/gcc/testsuite/gcc.target/sh/sh.exp gcc-4.2.1-stm/gcc/testsuite/gcc.target/sh/sh.exp
--- gcc-4.2.1/gcc/testsuite/gcc.target/sh/sh.exp	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.2.1-stm/gcc/testsuite/gcc.target/sh/sh.exp	2007-08-10 13:08:58.000000000 +0100
@@ -0,0 +1,41 @@
+#   Copyright (C) 2007 Free Software Foundation, Inc.
+
+# This program is free software; you can redistribute it and/or modify
+# it under the terms of the GNU General Public License as published by
+# the Free Software Foundation; either version 2 of the License, or
+# (at your option) any later version.
+# 
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+# GNU General Public License for more details.
+# 
+# You should have received a copy of the GNU General Public License
+# along with this program; if not, write to the Free Software
+# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301, USA.  
+
+# GCC testsuite that uses the `gcc-dg.exp' driver, just a single option, no
+# looping over tests.
+
+# Exit immediately if this isn't a IS target.
+if ![istarget sh-*-*] then {
+  return
+}
+
+# Load support procs.
+load_lib gcc-dg.exp
+
+# If a testcase doesn't have special options, use these.
+global DEFAULT_CFLAGS
+if ![info exists DEFAULT_CFLAGS] then {
+    set DEFAULT_CFLAGS " -ansi -pedantic-errors"
+}
+
+# Initialize `dg'.
+dg-init
+
+# Main loop.
+dg-runtest [lsort [glob -nocomplain $srcdir/$subdir/*.\[cS\]]] "" $DEFAULT_CFLAGS
+
+# All done.
+dg-finish
diff -Naur gcc-4.2.1/gcc/testsuite/gfortran.dg/assumed_charlen_function_1.f90 gcc-4.2.1-stm/gcc/testsuite/gfortran.dg/assumed_charlen_function_1.f90
--- gcc-4.2.1/gcc/testsuite/gfortran.dg/assumed_charlen_function_1.f90	2006-04-21 06:10:22.000000000 +0100
+++ gcc-4.2.1-stm/gcc/testsuite/gfortran.dg/assumed_charlen_function_1.f90	2007-08-10 13:08:58.000000000 +0100
@@ -1,80 +1,80 @@
-! { dg-do compile }
-! { dg-options "-std=legacy" }
-! Tests the patch for PRs 25084, 20852, 25085 and 25086, all of
-! which involve assumed character length functions.
-! Compiled from original PR testcases, which were all contributed
-! by Joost VandeVondele  <jv244@cam.ac.uk>
-!
-! PR25084 - the error is not here but in any use of .IN.
-! It is OK to define an assumed character length function
-! in an interface but it cannot be invoked (5.1.1.5).
-
-MODULE M1
- TYPE  SET
-  INTEGER  CARD
- END  TYPE  SET
-END MODULE M1
-
-MODULE  INTEGER_SETS
- INTERFACE  OPERATOR  (.IN.)
-  FUNCTION ELEMENT(X,A) ! { dg-error "cannot be assumed character length" }
-     USE M1
-     CHARACTER(LEN=*)      :: ELEMENT
-     INTEGER, INTENT(IN)   ::  X
-     TYPE(SET), INTENT(IN) ::   A
-  END FUNCTION ELEMENT
- END  INTERFACE
-END MODULE
-
-! 5.1.1.5 of the Standard: A function name declared with an asterisk
-! char-len-param shall not be array-valued, pointer-valued, recursive
-! or pure
-! 
-! PR20852
-RECURSIVE FUNCTION TEST() ! { dg-error "cannot be recursive" }
- CHARACTER(LEN=*) :: TEST
- TEST = ""
-END FUNCTION
-
-!PR25085
-FUNCTION F1()             ! { dg-error "cannot be array-valued" }
-  CHARACTER(LEN=*), DIMENSION(10) :: F1
-  F1 = ""
-END FUNCTION F1
-
-!PR25086
-FUNCTION F2() result(f4)  ! { dg-error "cannot be pointer-valued" }
-  CHARACTER(LEN=*), POINTER  :: f4
-  f4 = ""
-END FUNCTION F2
-
-!PR?????
-pure FUNCTION F3()        ! { dg-error "cannot be pure" }
-  CHARACTER(LEN=*)  :: F3
-  F3 = ""
-END FUNCTION F3
-
-function not_OK (ch)
-  character(*) not_OK, ch ! OK in an external function
-  not_OK = ch
-end function not_OK
-
-  use m1
-
-  character(4) :: answer
-  character(*), external :: not_OK
-  integer :: i
-  type (set) :: z
-
-  interface
-    function ext (i)
-      character(*) :: ext
-      integer :: i
-    end function ext
-  end interface
-
-  answer = not_OK ("unOK") ! { dg-error "since it is not a dummy" }
-
-END
-
-! { dg-final { cleanup-modules "M1" } }
+! { dg-do compile }
+! { dg-options "-std=legacy" }
+! Tests the patch for PRs 25084, 20852, 25085 and 25086, all of
+! which involve assumed character length functions.
+! Compiled from original PR testcases, which were all contributed
+! by Joost VandeVondele  <jv244@cam.ac.uk>
+!
+! PR25084 - the error is not here but in any use of .IN.
+! It is OK to define an assumed character length function
+! in an interface but it cannot be invoked (5.1.1.5).
+
+MODULE M1
+ TYPE  SET
+  INTEGER  CARD
+ END  TYPE  SET
+END MODULE M1
+
+MODULE  INTEGER_SETS
+ INTERFACE  OPERATOR  (.IN.)
+  FUNCTION ELEMENT(X,A) ! { dg-error "cannot be assumed character length" }
+     USE M1
+     CHARACTER(LEN=*)      :: ELEMENT
+     INTEGER, INTENT(IN)   ::  X
+     TYPE(SET), INTENT(IN) ::   A
+  END FUNCTION ELEMENT
+ END  INTERFACE
+END MODULE
+
+! 5.1.1.5 of the Standard: A function name declared with an asterisk
+! char-len-param shall not be array-valued, pointer-valued, recursive
+! or pure
+! 
+! PR20852
+RECURSIVE FUNCTION TEST() ! { dg-error "cannot be recursive" }
+ CHARACTER(LEN=*) :: TEST
+ TEST = ""
+END FUNCTION
+
+!PR25085
+FUNCTION F1()             ! { dg-error "cannot be array-valued" }
+  CHARACTER(LEN=*), DIMENSION(10) :: F1
+  F1 = ""
+END FUNCTION F1
+
+!PR25086
+FUNCTION F2() result(f4)  ! { dg-error "cannot be pointer-valued" }
+  CHARACTER(LEN=*), POINTER  :: f4
+  f4 = ""
+END FUNCTION F2
+
+!PR?????
+pure FUNCTION F3()        ! { dg-error "cannot be pure" }
+  CHARACTER(LEN=*)  :: F3
+  F3 = ""
+END FUNCTION F3
+
+function not_OK (ch)
+  character(*) not_OK, ch ! OK in an external function
+  not_OK = ch
+end function not_OK
+
+  use m1
+
+  character(4) :: answer
+  character(*), external :: not_OK
+  integer :: i
+  type (set) :: z
+
+  interface
+    function ext (i)
+      character(*) :: ext
+      integer :: i
+    end function ext
+  end interface
+
+  answer = not_OK ("unOK") ! { dg-error "since it is not a dummy" }
+
+END
+
+! { dg-final { cleanup-modules "M1" } }
diff -Naur gcc-4.2.1/gcc/version.c gcc-4.2.1-stm/gcc/version.c
--- gcc-4.2.1/gcc/version.c	2005-03-16 06:04:10.000000000 +0000
+++ gcc-4.2.1-stm/gcc/version.c	2007-08-10 13:08:59.000000000 +0100
@@ -8,7 +8,7 @@
    in parentheses.  You may also wish to include a number indicating
    the revision of your modified compiler.  */
 
-#define VERSUFFIX ""
+#define VERSUFFIX " (STMicroelectronics Special) [build "__DATE__"]"
 
 /* This is the location of the online document giving instructions for
    reporting bugs.  If you distribute a modified version of GCC,
@@ -17,7 +17,7 @@
    forward us bugs reported to you, if you determine that they are
    not bugs in your modifications.)  */
 
-const char bug_report_url[] = "<URL:http://gcc.gnu.org/bugs.html>";
+const char bug_report_url[] = "<file://doc/docbug.htm> on the installation CD";
 
 /* The complete version string, assembled from several pieces.
    BASEVER, DATESTAMP, and DEVPHASE are defined by the Makefile.  */
diff -Naur gcc-4.2.1/include/ChangeLog.STM gcc-4.2.1-stm/include/ChangeLog.STM
--- gcc-4.2.1/include/ChangeLog.STM	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.2.1-stm/include/ChangeLog.STM	2007-08-10 13:08:59.000000000 +0100
@@ -0,0 +1,8 @@
+2006-05-03  Andrew Stubbs  <andrew.stubbs@st.com>
+            J"orn Rennecke <joern.rennecke@st.com>
+
+	* libiberty.h (make_relative_prefix_ignore_links): Declare.
+
+2006-03-27  Andrew Stubbs  <andrew.stubbs@st.com>
+
+	* libiberty.h: Add support for cygpath.c.
diff -Naur gcc-4.2.1/include/libiberty.h gcc-4.2.1-stm/include/libiberty.h
--- gcc-4.2.1/include/libiberty.h	2007-02-09 15:29:21.000000000 +0000
+++ gcc-4.2.1-stm/include/libiberty.h	2007-08-10 13:08:59.000000000 +0100
@@ -1,6 +1,7 @@
 /* Function declarations for libiberty.
 
    Copyright 2001, 2002, 2005 Free Software Foundation, Inc.
+   Copyright (C) 2006 STMicroelectronics
    
    Note - certain prototypes declared in this header file are for
    functions whoes implementation copyright does not belong to the
@@ -197,6 +198,13 @@
 extern char *make_relative_prefix (const char *, const char *,
                                    const char *) ATTRIBUTE_MALLOC;
 
+/* Generate a relocated path to some installation directory without
+   attempting to follow any soft links.  Allocates
+   return value using malloc.  */
+
+extern char *make_relative_prefix_ignore_links (const char *, const char *,
+						const char *) ATTRIBUTE_MALLOC;
+
 /* Choose a temporary directory to use for scratch files.  */
 
 extern char *choose_temp_base (void) ATTRIBUTE_MALLOC;
@@ -617,5 +625,28 @@
 }
 #endif
 
+#ifdef __MINGW32__
+char *cygpath (const char *path);
+void cygpath_replace (char **path);
+
+/* The following macros are just to prevent putting #ifdef MINGW everywhere.
+   You still need to if you don't want to overwite the original pointer.  */
+
+/* Reassign the pointer PATH without freeing anything.  */
+#define CYGPATH(path) do {path = cygpath (path);} while(0)
+
+/* Free memory. Intended to be used in conjunction with CYGPATH().  */
+#define CYGPATH_FREE(path) free (path)
+
+/* Reassign the pointer PATH and free the previous content.  */
+#define CYGPATH_REPLACE(path) cygpath_replace (path)
+
+#else
+/* If these were properly empty statements then there might be warnings
+   which would kill a -Werror build.  */
+#define CYGPATH(path) do {} while (0)
+#define CYGPATH_FREE(path) do {} while (0)
+#define CYGPATH_REPLACE(path) do {} while (0)
+#endif
 
 #endif /* ! defined (LIBIBERTY_H) */
diff -Naur gcc-4.2.1/libcpp/ChangeLog.STM gcc-4.2.1-stm/libcpp/ChangeLog.STM
--- gcc-4.2.1/libcpp/ChangeLog.STM	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.2.1-stm/libcpp/ChangeLog.STM	2007-08-10 13:08:59.000000000 +0100
@@ -0,0 +1,15 @@
+2006-12-28  Tom Tromey  <tromey@redhat.com>
+
+	PR preprocessor/30001:
+	* charset.c (_cpp_convert_input): Check that to.len is greater
+	than zero.
+
+2006-03-27  Andrew Stubbs  <andrew.stubbs@st.com>
+
+	* files.c (open_file, pch_open_file): Remove CYGDRIVE markers.
+	Add Cygwin pathname support for MinGW.
+
+2006-03-10  J"orn Rennecke <joern.rennecke@st.com>
+
+	* files.c (open_file, pch_open_file): Add CYGDRIVE markers.
+
diff -Naur gcc-4.2.1/libcpp/charset.c gcc-4.2.1-stm/libcpp/charset.c
--- gcc-4.2.1/libcpp/charset.c	2005-11-03 23:08:18.000000000 +0000
+++ gcc-4.2.1-stm/libcpp/charset.c	2007-08-10 13:08:59.000000000 +0100
@@ -1628,7 +1628,7 @@
      terminate with another \r, not an \n, so that we do not mistake
      the \r\n sequence for a single DOS line ending and erroneously
      issue the "No newline at end of file" diagnostic.  */
-  if (to.text[to.len - 1] == '\r')
+  if (to.len && to.text[to.len - 1] == '\r')
     to.text[to.len] = '\r';
   else
     to.text[to.len] = '\n';
diff -Naur gcc-4.2.1/libcpp/files.c gcc-4.2.1-stm/libcpp/files.c
--- gcc-4.2.1/libcpp/files.c	2006-02-18 09:25:31.000000000 +0000
+++ gcc-4.2.1-stm/libcpp/files.c	2007-08-10 13:08:59.000000000 +0100
@@ -6,6 +6,7 @@
    Adapted to ANSI C, Richard Stallman, Jan 1987
    Split out of cpplib.c, Zack Weinberg, Oct 1998
    Reimplemented, Neil Booth, Jul 2003
+   Copyright (c) 2006  STMicroelectronics.
 
 This program is free software; you can redistribute it and/or modify it
 under the terms of the GNU General Public License as published by the
@@ -206,7 +207,12 @@
       set_stdin_to_binary_mode ();
     }
   else
-    file->fd = open (file->path, O_RDONLY | O_NOCTTY | O_BINARY, 0666);
+    {
+      char *filename = file->path;
+      CYGPATH (filename);
+      file->fd = open (filename, O_RDONLY | O_NOCTTY | O_BINARY, 0666);
+      CYGPATH_FREE (filename);
+    }
 
   if (file->fd != -1)
     {
@@ -259,6 +265,16 @@
   memcpy (pchname, path, flen);
   memcpy (pchname + flen, extension, sizeof (extension));
 
+#ifdef __MINGW32__
+    {
+      char *temp = cygpath (pchname);
+      len = strlen (temp) + 1;
+      XRESIZEVEC (char, pchname, len);
+      memcpy (pchname, temp, len);
+      free (temp);
+    }
+#endif
+  
   if (stat (pchname, &st) == 0)
     {
       DIR *pchdir;
diff -Naur gcc-4.2.1/libiberty/ChangeLog.STM gcc-4.2.1-stm/libiberty/ChangeLog.STM
--- gcc-4.2.1/libiberty/ChangeLog.STM	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.2.1-stm/libiberty/ChangeLog.STM	2007-08-10 13:08:59.000000000 +0100
@@ -0,0 +1,22 @@
+2006-05-15  Andrew Stubbs  <andrew.stubbs@st.com>
+
+	* cygpath.c (cygpath): Convert pathnames consisting only of a
+	drive specifier to a valid directory (e.g 'c:' -> 'c:/').
+
+2006-05-03  Andrew Stubbs  <andrew.stubbs@st.com>
+            J"orn Rennecke <joern.rennecke@st.com>
+
+	* make-relative-prefix.c (make_relative_prefix_1): New function,
+	broken out of make_relative_prefix.  Make link resolution dependent
+	on new parameter.
+	(make_relative_prefix): Use make_relative_prefix_1.
+	(make_relative_prefix_ignore_links): New function.
+
+2006-03-27  Andrew Stubbs  <andrew.stubbs@st.com>
+
+libiberty/
+	* cygpath.c: New file.
+	* config/mh-mingw: New file.
+	* configure.ac: Add mh-mingw makefile fragment when host is MinGW.
+	* configure: Regenerate.
+	* Makefile.in: Add cygpath.[co] .
diff -Naur gcc-4.2.1/libiberty/config/mh-mingw gcc-4.2.1-stm/libiberty/config/mh-mingw
--- gcc-4.2.1/libiberty/config/mh-mingw	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.2.1-stm/libiberty/config/mh-mingw	2007-08-10 13:08:59.000000000 +0100
@@ -0,0 +1 @@
+EXTRA_OFILES=cygpath.o
diff -Naur gcc-4.2.1/libiberty/configure gcc-4.2.1-stm/libiberty/configure
--- gcc-4.2.1/libiberty/configure	2006-08-30 14:46:50.000000000 +0100
+++ gcc-4.2.1-stm/libiberty/configure	2007-08-10 13:08:59.000000000 +0100
@@ -3599,6 +3599,7 @@
   *-*-freebsd2.2.[012])	frag=mh-fbsd21 ;;
   i370-*-opened*)       frag=mh-openedition ;;
   i[34567]86-*-windows*)	frag=mh-windows ;;
+  *-*-mingw*)		frag=mh-mingw ;;
 esac
 
 if [ -n "${frag}" ]; then
diff -Naur gcc-4.2.1/libiberty/configure.ac gcc-4.2.1-stm/libiberty/configure.ac
--- gcc-4.2.1/libiberty/configure.ac	2006-08-30 14:46:50.000000000 +0100
+++ gcc-4.2.1-stm/libiberty/configure.ac	2007-08-10 13:08:59.000000000 +0100
@@ -182,6 +182,7 @@
   *-*-freebsd2.2.[[012]])	frag=mh-fbsd21 ;;
   i370-*-opened*)       frag=mh-openedition ;;
   i[[34567]]86-*-windows*)	frag=mh-windows ;;
+  *-*-mingw*)		frag=mh-mingw ;;
 esac
 
 if [[ -n "${frag}" ]]; then
diff -Naur gcc-4.2.1/libiberty/cygpath.c gcc-4.2.1-stm/libiberty/cygpath.c
--- gcc-4.2.1/libiberty/cygpath.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.2.1-stm/libiberty/cygpath.c	2007-08-10 13:08:59.000000000 +0100
@@ -0,0 +1,358 @@
+/* Basic Cygwin pathname support for MinGW.
+
+   Copyright (C) 2006 STMicroelectronics
+
+   This file is part of the libiberty library.
+
+   This program is free software; you can redistribute it and/or modify
+   it under the terms of the GNU General Public License as published by
+   the Free Software Foundation; either version 2 of the License, or
+   (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+   GNU General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, write to the Free Software
+   Foundation, Inc., 51 Franklin Street - Fifth Floor,
+   Boston, MA 02110-1301, USA.
+
+
+   This file implements a limited amount of support for Cygwin paths.
+   It is intended for use by MinGW programs that must interact with Cygwin.
+
+   It is limited to absolute paths only.  I.e. Those beginning with Cygwin
+   mounts, such as /cygdrive/...  See the comment on cygpath() below.  */
+
+#include "libiberty.h"
+#include <string.h>
+#include <ctype.h>
+#include <windows.h>
+
+
+/* These are all the possible settings for the ST_CYGPATH_MODE
+   environment variable.  */
+static enum
+{
+  mode_unset,
+  mode_off,
+  mode_normal,
+  mode_full
+} mode = mode_unset;
+
+
+/* These are the values extracted from the registry.
+   They are extracted the first time cygpath is called.  */
+static const char *cygdrive = NULL;
+static struct mount
+{
+  /* The name of the Cygwin mount point.  E.g. "/usr/bin"  */
+  char *mount;
+
+  /* The actual Windows path that the mount translates to.  */
+  char *actual;
+
+  struct mount *next;
+} *mounts = NULL;
+
+
+/* Read a string from the Windows Registry.
+   KEY should be a valid handle from RegOpenKeyEx().
+   NAME should be the name of the value within the key.
+   The value should be of type REG_SZ.
+   If the value does not exist, is of the wrong typei, or another error
+   occurs, then NULL is returned.
+   Otherwise a malloced string is returned.  */
+static char *
+read_string_from_registry (HKEY key, const char *name)
+{
+  DWORD valuetype = REG_NONE;
+  DWORD valuesize = 0;
+  char *value = NULL;
+
+  if (RegQueryValueEx (key, name, NULL, &valuetype,
+		       NULL, &valuesize) == ERROR_SUCCESS
+      && valuetype == REG_SZ)
+    {
+      value = xmalloc (valuesize);
+      if (RegQueryValueEx (key, name, NULL, &valuetype, (unsigned char *)value,
+			   &valuesize) != ERROR_SUCCESS)
+	{
+	  free (value);
+	  value = NULL;
+	}
+    }
+
+  return value;
+}
+
+
+/* Fill in the mounts list (mounts is defined statically above).
+   All subkeys (not values) of KEY that contain a REG_SZ value named 'native'
+   are added to the start of the mounts list.  */
+static void
+read_mounts (HKEY key)
+{
+  int mountsize = 15;
+  char *mount = xmalloc (mountsize);
+  DWORD size = mountsize;
+  int index = 0;
+  int retval = 0;
+
+  /* For each subkey ...  */
+  while ((retval = RegEnumKeyEx (key, index, mount, &size, 0, NULL, 0, NULL))
+	 != ERROR_NO_MORE_ITEMS)
+    {
+      struct mount *newmount;
+      HKEY subkey;
+      char *actual;
+
+      switch (retval) {
+      case ERROR_MORE_DATA:
+	/* The buffer wasn't large enough for this key name.
+	   Unlike RegQueryValueEx, RegEnumKeyEx won't tell us how big it
+	   should be, so just make it bigger and try again.
+	   Note that this code path does NOT increment index.
+       	   Most of the time we will only be dealing with short strings.  */
+	mountsize += 10;
+	mount = xrealloc (mount, mountsize);
+	break;
+
+      case ERROR_SUCCESS:
+	/* Find the actual windows path.  */
+  	if (RegOpenKeyEx (key, mount, 0, KEY_READ, &subkey) != ERROR_SUCCESS)
+	  {
+	    index++;
+	    break;
+	  }
+	actual = read_string_from_registry (subkey, "native");	
+	RegCloseKey (subkey);
+	if (actual == NULL)
+	  {
+	    index++;
+	    break;
+	  }
+
+	/* Create the new entry in the mount table.  */
+	newmount = xmalloc (sizeof (struct mount));
+	newmount->mount = xstrdup (mount);
+	newmount->actual = actual;
+	newmount->next = mounts;
+	mounts = newmount;
+	index++;
+	break;
+
+      default:
+	/* Don't infinite loop should any other return value occur.  */
+        index++;
+      }
+
+      /* The last call to RegEnumKeyEx may have clobbered size.
+         Fix it before the next call.  */
+      size = mountsize;
+    }
+
+  free (mount);
+}
+
+
+/* The top level registry reading function.
+   Open the keys, call the above functions to get the right values,
+   and clean up.  */
+static void
+read_registry (void)
+{
+  HKEY hcu_key, hlm_key;
+
+  /* Get key handles for the two places cygwin keeps its registry data.  */
+  if (RegOpenKeyEx (HKEY_CURRENT_USER,
+		    "Software\\Cygnus Solutions\\Cygwin\\mounts v2",
+		    0, KEY_READ, &hcu_key) != ERROR_SUCCESS)
+    hcu_key = NULL;
+
+  if (RegOpenKeyEx (HKEY_LOCAL_MACHINE,
+		    "SOFTWARE\\Cygnus Solutions\\Cygwin\\mounts v2",
+		    0, KEY_READ, &hlm_key) != ERROR_SUCCESS)
+    hlm_key = NULL;
+
+  /* Get the virtual mount point used for windows drives.  */
+  if (hcu_key)
+    cygdrive = read_string_from_registry (hcu_key, "cygdrive prefix");
+  if (hlm_key && cygdrive == NULL)
+    cygdrive = read_string_from_registry (hlm_key, "cygdrive prefix");
+
+  /* Read the other mount points.
+     Read hlm before hcu to ensure hcu settings get used by preference
+     by being closer on the mounts stack.  */
+  if (hlm_key)
+    read_mounts (hlm_key);
+  if (hcu_key)
+    read_mounts (hcu_key);
+
+  if (hlm_key)
+    RegCloseKey (hlm_key);
+  if (hcu_key)
+    RegCloseKey (hcu_key);
+}
+
+
+/* Given a path of unknown variety, return the same path with any
+   Cygwin mount points substituted.
+   This function always returns a malloced string which should be
+   freed when the the caller is finished with it.
+
+   The mapping is affected by the ST_CYGPATH_MODE environment variable.
+   See the fprintf messages below for full information.
+
+   It can replace /cygdrive/<letter>/..... style pathnames, even if the
+   user has used 'mount -c' to an alternative string.
+
+   It can replace (if enabled) other Cygwin mount points, such as
+   the usual '/', '/usr/bin', '/usr/lib', as well as any other user defined
+   mount points.
+
+   It does NOT attempt to convert any pathnames that look like native Windows
+   names - such as those starting with '<letter>:' or double slash (UNC).
+
+   It does NOT handle relative pathnames passing through cygwin mounts
+   (e.g. '../cygdrive/c'), or absolute paths with repeated directory
+   separators or relative elements within the mount name
+   (e.g. '/usr/./bin').
+   
+   It does NOT allow backslash \ directory separators within the actual mount
+   path (e.g. '/usr\bin').  Cygwin does not always allow them there either.  */
+char *
+cygpath (const char *path)
+{
+  char *result = NULL;
+
+  if (path == NULL)
+    return NULL;
+
+  /* If this is the first time this function has been called then read the
+     environment and registry.  */
+  if (mode == mode_unset)
+    {
+      char *env = getenv ("ST_CYGPATH_MODE");
+
+      if (env == NULL || strcmp (env, "normal") == 0)
+    	mode = mode_normal;
+      else if (strcmp (env, "full") == 0)
+	mode = mode_full;
+      else if (strcmp (env, "off") == 0)
+	mode = mode_off;
+
+      if (mode != mode_off)
+	read_registry();
+
+      if (mode == mode_unset)
+	{
+	  /* The variable was set, but not to any known value.
+	     Set up a default and print an informational message
+	     for the user.  */
+	  mode = mode_normal;
+	  fprintf (stderr, "ST_CYGPATH_MODE should be one of:\n");
+	  fprintf (stderr, " off    - Disable all path translation.\n");
+	  fprintf (stderr, " normal - Translate %s only.\n", cygdrive);
+	  fprintf (stderr, " full   - Translate all Cygwin mounts.\n");
+	}
+    }
+
+  /* First, test if this can only be a windows (non-cygwin) path.
+     This includes paths that start with a drive letter or UNC double slash.  */
+  if ((isalpha (path[0]) && path[1] == ':')
+      || ((path[0] == '\\' || path[0] == '/')
+	  && (path[1] == '\\' || path[1] == '/')))
+    result = xstrdup (path);
+
+  /* Second, handle /cygdrive/<letter>/ (or whatever) paths.  */
+  if (!result && cygdrive != NULL && (mode == mode_normal || mode == mode_full))
+    {
+      int length = strlen (cygdrive);
+      /* Note that cygwin does not allow '\\' instead of '/' in cygdrive.  */
+      if (strncmp (cygdrive, path, length) == 0
+	  && (path[length] == '/' || path[length] == '\\'
+	      || path[length] == '\0')
+	  && isalpha (path[length+1]))
+        {
+	  result = xmalloc (strlen (path) - length+1 + 1);
+	  result[0] = path[length+1];
+	  result[1] = ':';
+	  strcpy (result + 2, path + length + 2);
+	}
+    }
+
+  /* Third, handle other types of cygwin path.  */
+  if (!result && mounts != NULL && mode == mode_full)
+    {
+      int matched = 0;
+      struct mount *foundat = NULL;
+      struct mount *mount = mounts;
+      /* Find the longest matching mount point.
+	 This is important. If we just used the first matching mount point
+	 it would probably always match '/' when '/usr/bin' is right.
+	 Use the first of equal length matches - this allows current-user
+	 mounts to override 'local machine' mounts (can this happen?).
+         It is a match only if the matching part is followed by a directory
+         separator or the end of the path, except for the root mount point.  */
+      while (mount != NULL)
+	{
+	  int length = strlen (mount->mount);
+	  if (strncmp (mount->mount, path, length) == 0
+	      && matched < length
+	      && (length == 1 /* Special case for root mount point '/'.  */
+		  || path[length] == '/' || path[length] == '\\'
+		  || path[length] == '\0'))
+	    {
+	      matched = length;
+	      foundat = mount;
+	    }
+	  mount = mount->next;
+	}
+      if (matched)
+	{
+	  /* There was a match so do the substitution.
+	     If matched is 1 then it can only be the root mount point, in
+	     which case we do not want to remove the matched part as it is the 
+	     directory separator.  */
+	  if (matched == 1)
+	    matched = 0;
+	  result = xmalloc (strlen (foundat->actual) + strlen (path) + 1
+			    - matched);
+	  strcpy (result, foundat->actual);
+	  strcat (result, path + matched);
+	}
+    }
+
+  if (result)
+    {
+      /* Ensure that the return is never just a drive letter.
+	 This is not a valid directory on Windows, but code often
+	 trims trailing slashes.  */
+      int length = strlen(result);
+      if (result[length-1] == ':')
+	{
+	  result = xrealloc (result, length+2);
+	  result[length] = '/';
+	  result[length+1] = '\0';
+	}
+      return result;
+    }
+
+  /* If we get here then it must have been some other kind of path.  */
+  return xstrdup (path);
+}
+
+
+/* This is just to make inserting the conversion more convenient.
+   The CYGPATH_REPLACE is conditionally compiled so it is harder to
+   add clean up code to go with it without this.  */
+void
+cygpath_replace (char **path)
+{
+  char *result = cygpath (*path);
+  free (*path);
+  *path = result;
+}
diff -Naur gcc-4.2.1/libiberty/Makefile.in gcc-4.2.1-stm/libiberty/Makefile.in
--- gcc-4.2.1/libiberty/Makefile.in	2007-03-03 19:29:51.000000000 +0000
+++ gcc-4.2.1-stm/libiberty/Makefile.in	2007-08-10 13:08:59.000000000 +0100
@@ -2,6 +2,7 @@
 # Makefile
 #   Copyright (C) 1990, 91-99, 2000, 2001, 2002, 2003, 2004, 2005, 2006
 #   Free Software Foundation
+#   Copyright (C) 2006 STMicroelectronics
 #
 # This file is part of the libiberty library.
 # Libiberty is free software; you can redistribute it and/or
@@ -129,7 +130,7 @@
 CFILES = alloca.c argv.c asprintf.c atexit.c				\
 	basename.c bcmp.c bcopy.c bsearch.c bzero.c			\
 	calloc.c choose-temp.c clock.c concat.c cp-demangle.c		\
-	 cp-demint.c cplus-dem.c					\
+	 cp-demint.c cplus-dem.c cygpath.c				\
 	dyn-string.c							\
 	fdmatch.c ffs.c fibheap.c floatformat.c fnmatch.c		\
 	 fopen_unlocked.c						\
@@ -186,7 +187,7 @@
 # maint-missing" and "make check".
 CONFIGURED_OFILES = ./asprintf.o ./atexit.o				\
 	./basename.o ./bcmp.o ./bcopy.o ./bsearch.o ./bzero.o		\
-	./calloc.o ./clock.o ./copysign.o				\
+	./calloc.o ./clock.o ./copysign.o cygpath.o			\
 	./_doprnt.o							\
 	./ffs.o								\
 	./getcwd.o ./getpagesize.o ./gettimeofday.o			\
@@ -607,6 +608,12 @@
 	else true; fi
 	$(COMPILE.c) $(srcdir)/cplus-dem.c $(OUTPUT_OPTION)
 
+./cygpath.o: $(srcdir)/cygpath.c $(INCDIR)/ansidecl.h $(INCDIR)/libiberty.h
+	if [ x"$(PICFLAG)" != x ]; then \
+	  $(COMPILE.c) $(PICFLAG) $(srcdir)/cygpath.c -o pic/$@; \
+	else true; fi
+	$(COMPILE.c) $(srcdir)/cygpath.c $(OUTPUT_OPTION)
+
 ./dyn-string.o: $(srcdir)/dyn-string.c config.h $(INCDIR)/ansidecl.h \
 	$(INCDIR)/dyn-string.h $(INCDIR)/libiberty.h
 	if [ x"$(PICFLAG)" != x ]; then \
diff -Naur gcc-4.2.1/libiberty/make-relative-prefix.c gcc-4.2.1-stm/libiberty/make-relative-prefix.c
--- gcc-4.2.1/libiberty/make-relative-prefix.c	2005-05-24 21:48:25.000000000 +0100
+++ gcc-4.2.1-stm/libiberty/make-relative-prefix.c	2007-08-10 13:08:59.000000000 +0100
@@ -1,6 +1,7 @@
 /* Relative (relocatable) prefix support.
    Copyright (C) 1987, 1989, 1992, 1993, 1994, 1995, 1996, 1997, 1998,
    1999, 2000, 2001, 2002 Free Software Foundation, Inc.
+   Copyright (c) 2006  STMicroelectronics.
 
 This file is part of libiberty.
 
@@ -217,9 +218,9 @@
 
    If no relative prefix can be found, return NULL.  */
 
-char *
-make_relative_prefix (const char *progname,
-                      const char *bin_prefix, const char *prefix)
+static char *
+make_relative_prefix_1 (const char *progname, const char *bin_prefix,
+			const char *prefix, const int resolve_links)
 {
   char **prog_dirs, **bin_dirs, **prefix_dirs;
   int prog_num, bin_num, prefix_num;
@@ -289,9 +290,14 @@
 	}
     }
 
-  full_progname = lrealpath (progname);
-  if (full_progname == NULL)
-    return NULL;
+  if ( resolve_links )
+    {
+      full_progname = lrealpath (progname);
+      if (full_progname == NULL)
+	return NULL;
+    }
+  else
+    full_progname = strdup(progname);
 
   prog_dirs = split_directories (full_progname, &prog_num);
   bin_dirs = split_directories (bin_prefix, &bin_num);
@@ -387,3 +393,33 @@
 
   return ret;
 }
+
+
+/* Do the full job, including symlink resolution.
+   This path will find files installed in the same place as the
+   program even when a soft link has been made to the program
+   from somwhere else. */
+
+char *
+make_relative_prefix (progname, bin_prefix, prefix)
+     const char *progname;
+     const char *bin_prefix;
+     const char *prefix;
+{
+  return make_relative_prefix_1 (progname, bin_prefix, prefix, 1);
+}
+
+/* Make the relative pathname without attempting to resolve any links.
+   '..' etc may also be left in the pathname.
+   This will find the files the user meant the program to find if the
+   installation is patched together with soft links. */
+
+char *
+make_relative_prefix_ignore_links (progname, bin_prefix, prefix)
+     const char *progname;
+     const char *bin_prefix;
+     const char *prefix;
+{
+  return make_relative_prefix_1 (progname, bin_prefix, prefix, 0);
+}
+
diff -Naur gcc-4.2.1/libstdc++-v3/ChangeLog.STM gcc-4.2.1-stm/libstdc++-v3/ChangeLog.STM
--- gcc-4.2.1/libstdc++-v3/ChangeLog.STM	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.2.1-stm/libstdc++-v3/ChangeLog.STM	2007-08-10 13:08:59.000000000 +0100
@@ -0,0 +1,11 @@
+2007-03-23  Andrew Stubbs  <andrew.stubbs@st.com>
+
+	INSbl27235:
+	* include/bits/concurence.h (__glibcxx_mutex_define_initialized): Add
+	attribute init_priority to ensure the mutex iniitialised before use.
+
+2006-05-12  Antony King <anthony.king@st.com>
+            J"orn Rennecke <joern.rennecke@st.com>
+
+	* include/Makefile.am (stamp-host): Remove CCODECVT_H line.
+	* include/Makefile.in: Regenerate.
diff -Naur gcc-4.2.1/libstdc++-v3/include/bits/concurrence.h gcc-4.2.1-stm/libstdc++-v3/include/bits/concurrence.h
--- gcc-4.2.1/libstdc++-v3/include/bits/concurrence.h	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.2.1-stm/libstdc++-v3/include/bits/concurrence.h	2007-08-10 13:08:59.000000000 +0100
@@ -0,0 +1,102 @@
+// Support for concurrent programing -*- C++ -*-
+
+// Copyright (C) 2003, 2004
+// Free Software Foundation, Inc.
+//
+// Copyright (C) 2007 STMicroelectronics
+//
+// This file is part of the GNU ISO C++ Library.  This library is free
+// software; you can redistribute it and/or modify it under the
+// terms of the GNU General Public License as published by the
+// Free Software Foundation; either version 2, or (at your option)
+// any later version.
+
+// This library is distributed in the hope that it will be useful,
+// but WITHOUT ANY WARRANTY; without even the implied warranty of
+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+// GNU General Public License for more details.
+
+// You should have received a copy of the GNU General Public License along
+// with this library; see the file COPYING.  If not, write to the Free
+// Software Foundation, 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301,
+// USA.
+
+// As a special exception, you may use this file as part of a free software
+// library without restriction.  Specifically, if other files instantiate
+// templates or use macros or inline functions from this file, or you compile
+// this file and link it with other files to produce an executable, this
+// file does not by itself cause the resulting executable to be covered by
+// the GNU General Public License.  This exception does not however
+// invalidate any other reasons why the executable file might be covered by
+// the GNU General Public License.
+
+/** @file concurrence.h
+ *  This is an internal header file, included by other library headers.
+ *  You should not attempt to use it directly.
+ */
+
+#ifndef _CONCURRENCE_H
+#define _CONCURRENCE_H 1
+
+// GCC's thread abstraction layer
+#include "bits/gthr.h"
+
+#if __GTHREADS
+
+# ifdef __GTHREAD_MUTEX_INIT
+#  define __glibcxx_mutex_type __gthread_mutex_t
+#  define __glibcxx_mutex_define_initialized(NAME) \
+__gthread_mutex_t NAME = __GTHREAD_MUTEX_INIT
+#  define __glibcxx_mutex_lock(NAME) \
+__gthread_mutex_lock(&NAME)
+# else
+// Implies __GTHREAD_MUTEX_INIT_FUNCTION
+struct __glibcxx_mutex : public __gthread_mutex_t
+{
+   __glibcxx_mutex() { __GTHREAD_MUTEX_INIT_FUNCTION(this); }
+};
+
+#  define __glibcxx_mutex_type __glibcxx_mutex
+#  define __glibcxx_mutex_define_initialized(NAME) \
+__glibcxx_mutex NAME __attribute__((init_priority(30000)))
+# define __glibcxx_mutex_lock(NAME) \
+__gthread_mutex_lock(&NAME)
+# endif
+
+# define __glibcxx_mutex_unlock(NAME) __gthread_mutex_unlock(&NAME)
+
+#else
+
+# define __glibcxx_mutex_type __gthread_mutex_t
+# define __glibcxx_mutex_define_initialized(NAME) __gthread_mutex_t NAME
+# define __glibcxx_mutex_lock(NAME)
+# define __glibcxx_mutex_unlock(NAME)
+
+#endif
+
+namespace __gnu_cxx
+{
+  typedef __glibcxx_mutex_type mutex_type;
+  
+  /// @brief  Scoped lock idiom.
+  // Acquire the mutex here with a constructor call, then release with
+  // the destructor call in accordance with RAII style.
+  class lock
+  {
+    // Externally defined and initialized.
+    mutex_type& device;
+
+  public:
+    explicit lock(mutex_type& name) : device(name)
+    { __glibcxx_mutex_lock(device); }
+
+    ~lock() throw()
+    { __glibcxx_mutex_unlock(device); }
+
+  private:
+    lock(const lock&);
+    lock& operator=(const lock&);
+  };
+}
+
+#endif
diff -Naur gcc-4.2.1/libstdc++-v3/include/Makefile.am gcc-4.2.1-stm/libstdc++-v3/include/Makefile.am
--- gcc-4.2.1/libstdc++-v3/include/Makefile.am	2007-07-05 12:46:00.000000000 +0100
+++ gcc-4.2.1-stm/libstdc++-v3/include/Makefile.am	2007-08-10 13:08:59.000000000 +0100
@@ -954,6 +954,7 @@
 
 # Host includes static.
 # XXX Missing dependency info for {host_headers_extra}
+# CCODECVT_H is no longer defined; we don't want to symlink / copy the netire directory!
 stamp-host: ${host_headers} ${host_headers_noinst} stamp-${host_alias}
 	@if [ ! -f stamp-host ]; then \
 	  (cd ${host_builddir} ;\
diff -Naur gcc-4.2.1/libstdc++-v3/include/Makefile.in gcc-4.2.1-stm/libstdc++-v3/include/Makefile.in
--- gcc-4.2.1/libstdc++-v3/include/Makefile.in	2007-07-05 12:46:00.000000000 +0100
+++ gcc-4.2.1-stm/libstdc++-v3/include/Makefile.in	2007-08-10 13:08:59.000000000 +0100
@@ -1337,6 +1337,7 @@
 
 # Host includes static.
 # XXX Missing dependency info for {host_headers_extra}
+# CCODECVT_H is no longer defined; we don't want to symlink / copy the netire directory!
 stamp-host: ${host_headers} ${host_headers_noinst} stamp-${host_alias}
 	@if [ ! -f stamp-host ]; then \
 	  (cd ${host_builddir} ;\
